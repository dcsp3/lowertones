{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# The directory containing your folders with JSON files\n",
    "root_directory = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\track_information'\n",
    "\n",
    "# The path for the output CSV file\n",
    "csv_file_path = 'tracks_data.csv'\n",
    "\n",
    "# Current date for 'Date Added To DB' and 'Date Last Modified' columns in the YYYY-MM-DD format\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Template for CSV rows with the updated header titles\n",
    "csv_columns = [\n",
    "    'id', 'song_spotify_id', 'song_title', 'song_duration', 'song_album_type', \n",
    "    'song_album_id', 'song_explicit', 'song_popularity', 'song_preview_url', \n",
    "    'song_track_features_added', 'song_acousticness', 'song_danceability', 'song_energy', \n",
    "    'song_instrumentalness', 'song_liveness', 'song_loudness', 'song_speechiness', \n",
    "    'song_tempo', 'song_valence', 'song_key', 'song_time_signature', 'song_date_added_to_db', \n",
    "    'song_date_last_modified', 'recording_mbid', 'album_id'\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store rows for the CSV\n",
    "csv_rows = []\n",
    "\n",
    "def process_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for track in data.get('tracks', []):\n",
    "            # Skip processing if the track is None\n",
    "            if track is None:\n",
    "                continue\n",
    "\n",
    "            row = {\n",
    "                'id': -1,  # This will be updated later with the actual ID\n",
    "                'song_spotify_id': track['id'],\n",
    "                'song_title': track['name'],\n",
    "                'song_duration': track['duration_ms'],\n",
    "                'song_album_type': track['album']['album_type'].upper(),\n",
    "                'song_album_id': track['album']['id'],\n",
    "                'song_explicit': track['explicit'],\n",
    "                'song_popularity': track['popularity'],\n",
    "                'song_preview_url': track.get('preview_url', ''),\n",
    "                'song_track_features_added': False,\n",
    "                'song_acousticness': -1,\n",
    "                'song_danceability': -1,\n",
    "                'song_energy': -1,\n",
    "                'song_instrumentalness': -1,\n",
    "                'song_liveness': -1,\n",
    "                'song_loudness': -1,\n",
    "                'song_speechiness': -1,\n",
    "                'song_tempo': -1,\n",
    "                'song_valence': -1,\n",
    "                'song_key': -1,\n",
    "                'song_time_signature': -1,\n",
    "                'song_date_added_to_db': current_date,\n",
    "                'song_date_last_modified': current_date,\n",
    "                'recording_mbid': '0',\n",
    "                'album_id': '0'\n",
    "            }\n",
    "            csv_rows.append(row)\n",
    "\n",
    "# Iterate over each subfolder and JSON file in the directory\n",
    "for subdir, dirs, files in os.walk(root_directory):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.json'):\n",
    "            print(f\"Processing {filename}...\")\n",
    "            process_json_file(os.path.join(subdir, filename))\n",
    "\n",
    "# Write the CSV file\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=csv_columns, delimiter=';')\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Update each row with its ID before writing\n",
    "    for i, row in enumerate(csv_rows, start=3769556):\n",
    "        row['id'] = i\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"CSV file has been successfully created at {csv_file_path} with {len(csv_rows)} tracks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the root directory containing your JSON files\n",
    "root_directory = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\NEWNEW\\\\album_information'\n",
    "\n",
    "# Define the output CSV file path\n",
    "csv_file_path = 'album_data.csv'\n",
    "\n",
    "# Current date for 'Date Added To DB' and 'Date Last Modified' columns\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# CSV column headers\n",
    "csv_columns = [\n",
    "    'id', 'album_spotify_id', 'album_name', 'album_cover_art', 'album_release_date',\n",
    "    'release_date_precision', 'album_popularity', 'album_type', 'spotify_album_upc',\n",
    "    'spotify_album_ean', 'spotify_album_isrc', 'date_added_to_db', 'date_last_modified',\n",
    "    'musicbrainz_metadata_added', 'musicbrainz_id'\n",
    "]\n",
    "\n",
    "# Initialize a list to hold album data\n",
    "albums_data = []\n",
    "\n",
    "# Function to process each JSON file\n",
    "def process_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for album in data.get('albums', []):\n",
    "            # Extract the required information, with checks for nullable fields\n",
    "            album_data = {\n",
    "                'id': -1,  # Placeholder, will be updated later\n",
    "                'album_spotify_id': album['id'],\n",
    "                'album_name': album['name'],\n",
    "                'album_cover_art': album['images'][0]['url'] if album.get('images') else '',\n",
    "                'album_release_date': convert_date_format(album['release_date']),\n",
    "                'release_date_precision': album['release_date_precision'],\n",
    "                'album_popularity': album['popularity'],\n",
    "                'album_type': album['album_type'],\n",
    "                'spotify_album_upc': album['external_ids'].get('upc', '') if album.get('external_ids') else '',\n",
    "                'spotify_album_ean': album['external_ids'].get('ean', '') if album.get('external_ids') else '',\n",
    "                'spotify_album_isrc': album['external_ids'].get('isrc', '') if album.get('external_ids') else '',\n",
    "                'date_added_to_db': current_date,\n",
    "                'date_last_modified': current_date,\n",
    "                'musicbrainz_metadata_added': False,  # Placeholder\n",
    "                'musicbrainz_id': ''  # Placeholder\n",
    "            }\n",
    "            albums_data.append(album_data)\n",
    "\n",
    "\n",
    "def convert_date_format(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str, '%d/%m/%Y').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return date_str  # Returns the original string if the format is incorrect\n",
    "\n",
    "\n",
    "# Process each JSON file in the directory and subdirectories\n",
    "for subdir, dirs, files in os.walk(root_directory):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.json'):\n",
    "            process_json_file(os.path.join(subdir, filename))\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=csv_columns, delimiter=';')\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Update each row with its actual ID before writing\n",
    "    for i, album_data in enumerate(albums_data, start=538765):\n",
    "        album_data['id'] = i\n",
    "        writer.writerow(album_data)\n",
    "\n",
    "print(f\"CSV file has been successfully created at {csv_file_path} with {len(albums_data)} albums.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the dtypes for the IDs to be strings when reading the CSVs\n",
    "dtype_dict = {'id': str, 'song_album_id': str, 'album_spotify_id': str}\n",
    "tracks_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\tracks_data.csv', delimiter=';', dtype=dtype_dict)\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\album_data.csv', delimiter=';', dtype=dtype_dict)\n",
    "\n",
    "# Create a dictionary mapping from album_spotify_id to id from albums_df\n",
    "# Ensure the 'id' column in albums_df is converted to integer if it's not NaN\n",
    "album_id_map = albums_df.dropna(subset=['id']).set_index('album_spotify_id')['id'].astype(int).to_dict()\n",
    "\n",
    "# Map the song_album_id in tracks_df using the album_id_map to get the album id\n",
    "tracks_df['album_id'] = tracks_df['song_album_id'].map(album_id_map)\n",
    "\n",
    "# Convert the new album_id column to integers, NaNs will be converted to a float with a .0\n",
    "tracks_df['album_id'] = tracks_df['album_id'].fillna(-1).astype(int)\n",
    "\n",
    "# Replace -1 back to NaN if you want to keep NaN values\n",
    "tracks_df['album_id'].replace(-1, pd.NA, inplace=True)\n",
    "\n",
    "# Save the updated tracks DataFrame to a new CSV file\n",
    "tracks_df.to_csv('path_to_updated_tracks.csv', index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Read the albums and artists CSVs into DataFrames\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\PRODFILES\\\\album_table.csv', delimiter=';', dtype={'id': str, 'album_spotify_id': str})\n",
    "artists_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\PRODFILES\\\\artists_table.csv', delimiter=';', dtype={'id': str, 'artist_spotify_id': str})\n",
    "\n",
    "# Dictionary to map Spotify album ID to CSV album ID\n",
    "album_id_map = albums_df.set_index('album_spotify_id')['id'].to_dict()\n",
    "\n",
    "# Dictionary to map Spotify artist ID to CSV artist ID\n",
    "artist_id_map = artists_df.set_index('artist_spotify_id')['id'].to_dict()\n",
    "\n",
    "# Initialize a list to hold the artist-album mappings\n",
    "artist_album_mapping = []\n",
    "\n",
    "# Assuming 'path_to_json_folder' is the folder containing all the JSON subfolders\n",
    "for root, dirs, files in os.walk('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\ALBUMS'):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for album in data['albums']:\n",
    "                    album_id = album_id_map.get(album['id'])\n",
    "                    if album_id:\n",
    "                        for artist in album['artists']:\n",
    "                            artist_id = artist_id_map.get(artist['id'])\n",
    "                            if artist_id:\n",
    "                                artist_album_mapping.append({'artistID': artist_id, 'albumID': album_id})\n",
    "\n",
    "# Create a DataFrame from the artist-album mappings\n",
    "artist_album_df = pd.DataFrame(artist_album_mapping)\n",
    "\n",
    "# Remove duplicates if there are any\n",
    "artist_album_df = artist_album_df.drop_duplicates()\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "artist_album_df.to_csv('artist_album_mappings_new.csv', index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Read the albums and artists CSVs into DataFrames\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\album_data.csv', delimiter=';', dtype={'id': str, 'album_spotify_id': str})\n",
    "artists_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\artists_data_full.csv', delimiter=';', dtype={'id': str, 'artist_spotify_id': str})\n",
    "\n",
    "# Dictionary to map Spotify album ID to CSV album ID\n",
    "album_id_map = albums_df.set_index('album_spotify_id')['id'].to_dict()\n",
    "\n",
    "# Dictionary to map Spotify artist ID to CSV artist ID\n",
    "artist_id_map = artists_df.set_index('artist_spotify_id')['id'].to_dict()\n",
    "\n",
    "# Initialize a list to hold the artist-album mappings\n",
    "artist_album_mapping = []\n",
    "\n",
    "# List to hold artist Spotify IDs where CSV artist ID was not found\n",
    "missing_artist_ids = []\n",
    "\n",
    "# Assuming 'path_to_json_folder' is the folder containing all the JSON subfolders\n",
    "for root, dirs, files in os.walk('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\ALBUMS'):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for album in data['albums']:\n",
    "                    album_id = album_id_map.get(album['id'])\n",
    "                    if album_id:\n",
    "                        for artist in album['artists']:\n",
    "                            artist_spotify_id = artist['id']\n",
    "                            artist_id = artist_id_map.get(artist_spotify_id)\n",
    "                            if artist_id:\n",
    "                                artist_album_mapping.append({'artistID': artist_id, 'albumID': album_id})\n",
    "                            else:\n",
    "                                missing_artist_ids.append(artist_spotify_id)\n",
    "\n",
    "# Print the list of artist Spotify IDs where the CSV artist ID was not found\n",
    "print(\"List of artist Spotify IDs where the CSV artist ID was not found:\")\n",
    "for artist_id in missing_artist_ids:\n",
    "    print(artist_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "missing_artist_ids_df = pd.DataFrame(missing_artist_ids, columns=['missing_artist_spotify_id'])\n",
    "\n",
    "# Define the file path where you want to save the CSV\n",
    "file_path = 'missing_artist_ids.csv'  # You can specify your own path\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "missing_artist_ids_df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f'The missing artist IDs have been saved to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'input_file.csv' with the path to your input CSV file\n",
    "input_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\musicBrainzAllContributors.csv'\n",
    "# Replace 'output_file.csv' with the path where you want to save the output CSV file\n",
    "output_file_path = 'contributor_data_prelink.csv'\n",
    "\n",
    "# Step 1: Read the input CSV\n",
    "input_df = pd.read_csv(input_file_path, delimiter=',')\n",
    "\n",
    "# Step 2: Create the new DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    'ID': range(1, len(input_df) + 1),\n",
    "    'NAME': input_df['artist_credit_name'],\n",
    "    'ROLE': input_df['role'],\n",
    "    'INSTRUMENT': input_df['instrument'],\n",
    "    'MUSICBRAINZ_ID': input_df['artist_mbid'],\n",
    "    'MAINARTIST': input_df['artist_credit_name'],\n",
    "    'SONGTITLE': input_df['recording_name']\n",
    "})\n",
    "\n",
    "# Step 3: Write the resulting DataFrame to a new CSV file\n",
    "output_df.to_csv(output_file_path, sep=';', index=False)\n",
    "\n",
    "print(f\"Output CSV saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Load the CSV files\n",
    "musicbrainz_df = pd.read_csv('C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\prepopulationStuff\\\\PythonNotebooksForPrepopulation\\\\contributor_data_prelink.csv', sep=';')  # The file generated from the previous step\n",
    "spotify_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\tracks_data.csv', sep=';')  # The Spotify songs CSV\n",
    "\n",
    "# Create a list of combined song title and artist names from Spotify for matching\n",
    "spotify_combined_list = spotify_df.apply(lambda x: f\"{x['song_title']} {x['song_album_id']}\", axis=1).tolist()\n",
    "\n",
    "def find_best_match(mb_row, spotify_combined_list):\n",
    "    mb_combined = f\"{mb_row['SONGTITLE']} {mb_row['MAINARTIST']}\"\n",
    "    \n",
    "    # Using extractOne to find the best match from the list\n",
    "    best_match_info = process.extractOne(mb_combined, spotify_combined_list)\n",
    "    \n",
    "    # If there is a match found, extract it\n",
    "    if best_match_info:\n",
    "        best_match_text, best_score = best_match_info\n",
    "        # Find the index of the match in Spotify list to retrieve the full row from spotify_df\n",
    "        match_index = spotify_combined_list.index(best_match_text)\n",
    "        best_match_row = spotify_df.iloc[match_index]\n",
    "        return best_match_row, best_score\n",
    "    return None, 0\n",
    "\n",
    "# Prepare the output DataFrame\n",
    "matched_df = pd.DataFrame(columns=['CONTRIBUTOR_ID', 'SONG_TABLE_ID', 'Spotify_Song_Title', 'Spotify_Artist', 'MusicBrainz_Song_Title', 'MusicBrainz_Artist'])\n",
    "\n",
    "# Iterate over MusicBrainz entries to find matches\n",
    "for index, mb_row in musicbrainz_df.iterrows():\n",
    "    best_match_row, score = find_best_match(mb_row, spotify_combined_list)\n",
    "    if best_match_row is not None and score > 80:  # Adjust the threshold as needed\n",
    "        matched_df = matched_df.append({\n",
    "            'CONTRIBUTOR_ID': mb_row['ID'],\n",
    "            'SONG_TABLE_ID': best_match_row['id'],\n",
    "            'Spotify_Song_Title': best_match_row['song_title'],\n",
    "            'Spotify_Artist': best_match_row['song_album_id'],  # Note: Adjust if there's a more direct artist name column\n",
    "            'MusicBrainz_Song_Title': mb_row['SONGTITLE'],\n",
    "            'MusicBrainz_Artist': mb_row['MAINARTIST']\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# Output the matched DataFrame to CSV\n",
    "matched_df.to_csv('linked_songs.csv', sep=';', index=False)\n",
    "\n",
    "print(\"Linked songs CSV generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\contributors_non-normalized.csv')\n",
    "\n",
    "# Select only the 'artist_credit_name' and 'artist_credit_id' columns\n",
    "df_selected = df[['artist_credit_name', 'artist_credit_id']]\n",
    "\n",
    "# Drop duplicates based on 'artist_credit_id' to ensure each ID is unique\n",
    "df_unique = df_selected.drop_duplicates(subset=['artist_credit_id'])\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "df_unique.to_csv('unique_artist_credits.csv', index=False)\n",
    "\n",
    "print('Unique artist credits CSV file has been saved as unique_artist_credits.csv.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def parse_manual(array_str):\n",
    "    # Manually parse the string to extract elements between curly braces\n",
    "    # Remove leading and trailing braces and split by comma\n",
    "    items = array_str.strip('{}').split(',')\n",
    "    # Strip quotes and extra spaces from each item\n",
    "    items = [item.strip('\"').strip() for item in items]\n",
    "    return items\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\contributors_non-normalized.csv')\n",
    "\n",
    "# Prepare a list to collect all artist name-MBID pairs\n",
    "artist_pairs = []\n",
    "i=0\n",
    "\n",
    "# Iterate over the DataFrame rows\n",
    "for _, row in df.iterrows():\n",
    "    # Parse the 'artist_mbids' and 'individual_artist_names' fields manually\n",
    "    artist_mbids = parse_manual(row['artist_mbids'])\n",
    "    individual_artist_names = parse_manual(row['individual_artist_names'])\n",
    "    \n",
    "    # Ensure we have equal lengths of MBIDs and names before proceeding\n",
    "    if len(artist_mbids) == len(individual_artist_names):\n",
    "        # Pair each individual artist name with its corresponding MBID\n",
    "        for artist_name, artist_mbid in zip(individual_artist_names, artist_mbids):\n",
    "            artist_pairs.append((artist_name, artist_mbid))\n",
    "    else:\n",
    "        print(\"Warning: Mismatched MBIDs and artist names for a row, skipping.\")\n",
    "        print(f\"Row index: {i}\")\n",
    "        i = i+1\n",
    "\n",
    "# Convert the list of pairs into a DataFrame, ensuring uniqueness\n",
    "df_pairs = pd.DataFrame(list(set(artist_pairs)), columns=['individual_artist_name', 'artist_mbid'])\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "df_pairs.to_csv('artist_name_mbid_pairs.csv', index=False)\n",
    "\n",
    "print('Artist name-MBID pairs CSV file has been saved as artist_name_mbid_pairs.csv.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the MBID CSV\n",
    "mbid_df = pd.read_csv('C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\prepopulationStuff\\\\PythonNotebooksForPrepopulation\\\\artist_name_mbid_pairs.csv')\n",
    "\n",
    "# Load the Spotify artist CSV, remember to use the ';' delimiter\n",
    "spotify_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\artists_data.csv', delimiter=';')\n",
    "\n",
    "# Perform a direct merge based on artist names\n",
    "# Note: This assumes 'artist_name' in spotify_df exactly matches 'individual_artist_name' in mbid_df\n",
    "merged_df = pd.merge(spotify_df, mbid_df, how='left', left_on='artist_name', right_on='individual_artist_name')\n",
    "\n",
    "# Drop the 'individual_artist_name' column as it's redundant after merge\n",
    "merged_df.drop(columns=['individual_artist_name'], inplace=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('spotify_artist_with_mbid_direct_match.csv', index=False)\n",
    "\n",
    "print('Spotify artist data with direct match MBIDs has been saved as spotify_artist_with_mbid_direct_match.csv.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adjust the path to your Spotify artists CSV file\n",
    "spotify_csv_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\artists_data.csv'\n",
    "spotify_df = pd.read_csv(spotify_csv_path, delimiter=';')\n",
    "\n",
    "# Print column names to verify\n",
    "print(spotify_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\downloaded_files\\\\artists_data_full.csv'\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "\n",
    "# Function to check for empty values in columns that shouldn't have them\n",
    "def check_empty_values(df, columns):\n",
    "    for column in columns:\n",
    "        if df[column].isnull().any():\n",
    "            print(f\"Empty values found in column: {column}\")\n",
    "        else:\n",
    "            print(f\"No empty values in column: {column}\")\n",
    "\n",
    "# Function to check data types\n",
    "def check_data_types(df):\n",
    "    errors = []\n",
    "    for column, expected_type in expected_column_types.items():\n",
    "        if expected_type == 'numeric':\n",
    "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "                errors.append(f\"Column {column} is not of type {expected_type}\")\n",
    "        elif expected_type == 'string':\n",
    "            if not pd.api.types.is_string_dtype(df[column]):\n",
    "                errors.append(f\"Column {column} is not of type {expected_type}\")\n",
    "        elif expected_type == 'date':\n",
    "            try:\n",
    "                pd.to_datetime(df[column])\n",
    "            except ValueError:\n",
    "                errors.append(f\"Column {column} contains non-date values\")\n",
    "    \n",
    "    if errors:\n",
    "        for error in errors:\n",
    "            print(error)\n",
    "    else:\n",
    "        print(\"All columns match their expected data types.\")\n",
    "\n",
    "# Columns expected not to have empty values based on your schema (all of them in this case)\n",
    "mandatory_columns = [\n",
    "    'id', 'artist_spotify_id', 'artist_name', 'artist_popularity',\n",
    "    'artist_image_small', 'artist_image_medium', 'artist_image_large',\n",
    "    'artist_followers', 'date_added_to_db', 'date_last_modified'\n",
    "]\n",
    "\n",
    "# Expected data types\n",
    "expected_column_types = {\n",
    "    'id': 'numeric',\n",
    "    'artist_spotify_id': 'string',\n",
    "    'artist_name': 'string',\n",
    "    'artist_popularity': 'numeric',\n",
    "    'artist_image_small': 'string',\n",
    "    'artist_image_medium': 'string',\n",
    "    'artist_image_large': 'string',\n",
    "    'artist_followers': 'numeric',\n",
    "    'date_added_to_db': 'date',\n",
    "    'date_last_modified': 'date'\n",
    "}\n",
    "\n",
    "# Perform checks\n",
    "check_empty_values(df, mandatory_columns)\n",
    "check_data_types(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_data_full.csv', sep=';')  # Adjust the path and separator as needed\n",
    "\n",
    "# Convert the dates in the last two columns\n",
    "# Assuming the last two columns contain your dates\n",
    "df.iloc[:, -4] = pd.to_datetime(df.iloc[:, -4], dayfirst=True).dt.strftime('%Y-%m-%d')\n",
    "df.iloc[:, -3] = pd.to_datetime(df.iloc[:, -3], dayfirst=True).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Save the modified DataFrame back to a new CSV file\n",
    "df.to_csv('path_to_modified_csv.csv', index=False, sep=';')  # Adjust the path and separator as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_data_full.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "\n",
    "# Function to check for empty values in mandatory columns\n",
    "def check_empty_values(df, columns):\n",
    "    for column in columns:\n",
    "        if df[column].isnull().any():\n",
    "            print(f\"Empty values found in column: {column}\")\n",
    "        else:\n",
    "            print(f\"No empty values in column: {column}\")\n",
    "\n",
    "# Function to verify data types\n",
    "def check_data_types(df, column_types):\n",
    "    for column, expected_type in column_types.items():\n",
    "        if expected_type == 'numeric':\n",
    "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of numeric type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct numeric type.\")\n",
    "        elif expected_type == 'string':\n",
    "            if not pd.api.types.is_string_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of string type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct string type.\")\n",
    "        elif expected_type == 'date':\n",
    "            try:\n",
    "                pd.to_datetime(df[column])\n",
    "                print(f\"Column {column} is of correct date type.\")\n",
    "            except ValueError:\n",
    "                print(f\"Column {column} contains incorrect date format.\")\n",
    "\n",
    "# Mandatory columns (not nullable)\n",
    "mandatory_columns = [\n",
    "    'id', 'album_spotify_id', 'album_name', 'album_cover_art',\n",
    "    'album_release_date', 'release_date_precision', 'album_popularity',\n",
    "    'album_type', 'date_added_to_db', 'date_last_modified', 'musicbrainz_metadata_added'\n",
    "]\n",
    "\n",
    "# Columns with specific data types\n",
    "column_types = {\n",
    "    'id': 'numeric',\n",
    "    'album_spotify_id': 'string',\n",
    "    'album_name': 'string',\n",
    "    'album_cover_art': 'string',\n",
    "    'album_release_date': 'date',\n",
    "    'release_date_precision': 'string',\n",
    "    'album_popularity': 'numeric',\n",
    "    'album_type': 'string',\n",
    "    'spotify_album_upc': 'string',  # Nullable\n",
    "    'spotify_album_ean': 'string',  # Nullable\n",
    "    'spotify_album_isrc': 'string',  # Nullable\n",
    "    'date_added_to_db': 'date',\n",
    "    'date_last_modified': 'date',\n",
    "    'musicbrainz_metadata_added': 'boolean',  # Assuming true/false representation\n",
    "    'musicbrainz_id': 'string'  # Nullable\n",
    "}\n",
    "\n",
    "# Check for empty values in mandatory columns\n",
    "check_empty_values(df, mandatory_columns)\n",
    "\n",
    "# Check if data types are correct\n",
    "check_data_types(df, column_types)\n",
    "31045;59\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_data_full.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "\n",
    "# Function to check for empty values in mandatory columns\n",
    "def check_empty_values(df, columns):\n",
    "    for column in columns:\n",
    "        if df[column].isnull().any():\n",
    "            print(f\"Empty values found in column: {column}\")\n",
    "        else:\n",
    "            print(f\"No empty values in column: {column}\")\n",
    "\n",
    "# Function to verify data types and print incorrect date formats\n",
    "def check_data_types_and_dates(df, column_types):\n",
    "    for column, expected_type in column_types.items():\n",
    "        if expected_type == 'numeric':\n",
    "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of numeric type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct numeric type.\")\n",
    "        elif expected_type == 'string':\n",
    "            if not pd.api.types.is_string_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of string type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct string type.\")\n",
    "        elif expected_type == 'date':\n",
    "            incorrect_format = df[column].apply(lambda x: check_date_format(x))\n",
    "            if incorrect_format.any():\n",
    "                print(f\"Rows with incorrect date format in column '{column}':\")\n",
    "                print(df[incorrect_format])\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct date type.\")\n",
    "\n",
    "# Helper function to check date format\n",
    "def check_date_format(date_string):\n",
    "    try:\n",
    "        pd.to_datetime(date_string, errors='raise')\n",
    "        return False  # Date format is correct\n",
    "    except ValueError:\n",
    "        return True  # Date format is incorrect\n",
    "\n",
    "# Mandatory columns (not nullable)\n",
    "mandatory_columns = [\n",
    "    'id', 'album_spotify_id', 'album_name', 'album_cover_art',\n",
    "    'album_release_date', 'release_date_precision', 'album_popularity',\n",
    "    'album_type', 'date_added_to_db', 'date_last_modified', 'musicbrainz_metadata_added'\n",
    "]\n",
    "\n",
    "# Columns with specific data types\n",
    "column_types = {\n",
    "    'id': 'numeric',\n",
    "    'album_spotify_id': 'string',\n",
    "    'album_name': 'string',\n",
    "    'album_cover_art': 'string',\n",
    "    'album_release_date': 'date',\n",
    "    'release_date_precision': 'string',\n",
    "    'album_popularity': 'numeric',\n",
    "    'album_type': 'string',\n",
    "    'spotify_album_upc': 'string',  # Nullable\n",
    "    'spotify_album_ean': 'string',  # Nullable\n",
    "    'spotify_album_isrc': 'string',  # Nullable\n",
    "    'date_added_to_db': 'date',\n",
    "    'date_last_modified': 'date',\n",
    "    'musicbrainz_metadata_added': 'boolean',  # Assuming true/false representation\n",
    "    'musicbrainz_id': 'string'  # Nullable\n",
    "}\n",
    "\n",
    "# Check for empty values in mandatory columns\n",
    "check_empty_values(df, mandatory_columns)\n",
    "\n",
    "# Check if data types are correct and for incorrect date formats\n",
    "check_data_types_and_dates(df, column_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\tracks_data_full.csv'  # Update this to the path of your CSV file\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')  # Ensure delimiter matches your CSV format\n",
    "\n",
    "# Function to check for empty values in mandatory columns\n",
    "def check_empty_values(df, columns):\n",
    "    for column in columns:\n",
    "        if df[column].isnull().any():\n",
    "            print(f\"Empty values found in column: {column}\")\n",
    "        else:\n",
    "            print(f\"No empty values in column: {column}\")\n",
    "\n",
    "# Function to verify data types\n",
    "def check_data_types(df, column_types):\n",
    "    for column, expected_type in column_types.items():\n",
    "        if expected_type == 'numeric':\n",
    "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of numeric type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct numeric type.\")\n",
    "        elif expected_type == 'string':\n",
    "            if not pd.api.types.is_string_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of string type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct string type.\")\n",
    "        elif expected_type == 'date':\n",
    "            try:\n",
    "                pd.to_datetime(df[column])\n",
    "                print(f\"Column {column} is of correct date type.\")\n",
    "            except ValueError:\n",
    "                print(f\"Column {column} contains incorrect date format.\")\n",
    "        elif expected_type == 'boolean':\n",
    "            # Checking if the column is boolean; Assuming boolean is represented as True/False or 1/0\n",
    "            if not pd.api.types.is_bool_dtype(df[column]) and not all(df[column].dropna().isin([0, 1, 'True', 'False', True, False])):\n",
    "                print(f\"Column {column} is not of boolean type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct boolean type.\")\n",
    "\n",
    "# Mandatory columns (not nullable)\n",
    "mandatory_columns = [\n",
    "    'id', 'song_spotify_id', 'song_title', 'song_duration', \n",
    "    'song_album_type', 'song_album_id', 'song_explicit', \n",
    "    'song_popularity', 'song_track_features_added', 'song_date_added_to_db', \n",
    "    'song_date_last_modified'\n",
    "]\n",
    "\n",
    "# Columns with specific data types\n",
    "column_types = {\n",
    "    'id': 'numeric',\n",
    "    'song_spotify_id': 'string',\n",
    "    'song_title': 'string',\n",
    "    'song_duration': 'numeric',\n",
    "    'song_album_type': 'string',\n",
    "    'song_album_id': 'string',\n",
    "    'song_explicit': 'boolean',\n",
    "    'song_popularity': 'numeric',\n",
    "    'song_preview_url': 'string',  # Nullable\n",
    "    'song_track_features_added': 'boolean',\n",
    "    # Assuming 'floatType' corresponds to 'numeric' in Python/Pandas\n",
    "    'song_acousticness': 'numeric',  # Nullable\n",
    "    'song_danceability': 'numeric',  # Nullable\n",
    "    'song_energy': 'numeric',  # Nullable\n",
    "    'song_instrumentalness': 'numeric',  # Nullable\n",
    "    'song_liveness': 'numeric',  # Nullable\n",
    "    'song_loudness': 'numeric',  # Nullable\n",
    "    'song_speechiness': 'numeric',  # Nullable\n",
    "    'song_tempo': 'numeric',  # Nullable\n",
    "    'song_valence': 'numeric',  # Nullable\n",
    "    'song_key': 'numeric',  # Nullable\n",
    "    'song_time_signature': 'numeric',  # Nullable\n",
    "    'song_date_added_to_db': 'date',\n",
    "    'song_date_last_modified': 'date',\n",
    "    'album_id': 'numeric'  # Nullable\n",
    "}\n",
    "\n",
    "# Check for empty values in mandatory columns\n",
    "check_empty_values(df, mandatory_columns)\n",
    "\n",
    "# Check if data types are correct\n",
    "check_data_types(df, column_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adjust these settings as needed\n",
    "pd.set_option('display.max_rows', None)  # This will allow all rows to be displayed\n",
    "pd.set_option('display.max_columns', None)  # This will allow all columns to be displayed\n",
    "pd.set_option('display.width', 1000)  # Adjust the width to accommodate the number of columns\n",
    "pd.set_option('display.max_colwidth', None)  # This ensures that the content of each column is fully displayed\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\tracks_data_full.csv'  # Update this to the path of your CSV file\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "\n",
    "# Function to print rows with empty values in a specific column\n",
    "def print_rows_with_empty_values_in_column(df, column_name):\n",
    "    empty_rows = df[df[column_name].isnull() | (df[column_name] == '')]\n",
    "    if not empty_rows.empty:\n",
    "        print(f\"Rows with empty values in column '{column_name}':\")\n",
    "        # print the song_spotfiy_id for each row with empty values\n",
    "        print(empty_rows['song_spotify_id'])\n",
    "    else:\n",
    "        print(f\"No empty values found in column '{column_name}'.\")\n",
    "\n",
    "# Print all rows where the song_title is empty\n",
    "print_rows_with_empty_values_in_column(df, 'song_title')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load the CSV files\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_data_full.csv', delimiter=';', dtype={'id': str, 'album_spotify_id': str})\n",
    "artists_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\artists_data_full.csv', delimiter=';', dtype={'id': str, 'artist_spotify_id': str})\n",
    "\n",
    "# Create dictionaries for ID mappings\n",
    "album_id_map = albums_df.set_index('album_spotify_id')['id'].to_dict()\n",
    "artist_id_map = artists_df.set_index('artist_spotify_id')['id'].to_dict()\n",
    "\n",
    "artist_album_mapping = []\n",
    "\n",
    "# Assuming the JSON structure is as shown in your example\n",
    "for root, dirs, files in os.walk('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\ALBUMS'):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for album in data['albums']:\n",
    "                    album_id = album_id_map.get(album['id'])\n",
    "                    if not album_id:\n",
    "                        print(f\"Album ID {album['id']} not found in album CSV.\")\n",
    "                        continue\n",
    "                    for artist in album['artists']:\n",
    "                        artist_id = artist_id_map.get(artist['id'])\n",
    "                        if not artist_id:\n",
    "                            print(f\"Artist ID {artist['id']} not found in artist CSV.\")\n",
    "                            continue\n",
    "                        artist_album_mapping.append({'artistID': artist_id, 'albumID': album_id})\n",
    "\n",
    "# Create DataFrame and remove duplicates\n",
    "artist_album_df = pd.DataFrame(artist_album_mapping).drop_duplicates()\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "artist_album_df.to_csv('artist_album_mappings_new.csv', index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load the CSV files\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_table.csv', delimiter=';', dtype={'id': str, 'album_spotify_id': str})\n",
    "artists_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\artists_data_full.csv', delimiter=';', dtype={'id': str, 'artist_spotify_id': str})\n",
    "\n",
    "# Create dictionaries for ID mappings\n",
    "album_id_map = albums_df.set_index('album_spotify_id')['id'].to_dict()\n",
    "artist_id_map = artists_df.set_index('artist_spotify_id')['id'].to_dict()\n",
    "\n",
    "artist_album_mapping = []\n",
    "missing_artists = {}  # Store missing artist IDs and the album IDs they're attributed to\n",
    "\n",
    "# Assuming the JSON structure is as shown in your example\n",
    "for root, dirs, files in os.walk('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\ALBUMS'):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for album in data['albums']:\n",
    "                    album_id = album_id_map.get(album['id'])\n",
    "                    if not album_id:\n",
    "                        print(f\"Album ID {album['id']} not found in album CSV.\")\n",
    "                        continue\n",
    "                    for artist in album['artists']:\n",
    "                        artist_id = artist_id_map.get(artist['id'])\n",
    "                        if not artist_id:\n",
    "                            # Record the missing artist ID along with the current album ID\n",
    "                            missing_artists.setdefault(artist['id'], []).append(album['id'])\n",
    "                            print(f\"Artist ID {artist['id']} not found in artist CSV, attributed to Album ID {album['id']}.\")\n",
    "                            continue\n",
    "                        artist_album_mapping.append({'artistID': artist_id, 'albumID': album_id})\n",
    "\n",
    "# Display missing artist IDs and the albums they're attributed to\n",
    "for artist_id, album_ids in missing_artists.items():\n",
    "    print(f\"Artist ID {artist_id} (not found) is attributed to Album IDs: {', '.join(album_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\WORKING\\\\recordings_artist_list.csv', delimiter=',', dtype={'id': str, 'artist_spotify_id': str})\n",
    "df2 = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\artists_table.csv', delimiter=';', dtype={'id': str, 'artist_spotify_id': str})\n",
    "\n",
    "\n",
    "# Convert the musicbrainz_id column into a set for fast lookup\n",
    "musicbrainz_ids = set(df2['musicbrainz_id'])\n",
    "\n",
    "# Define a function to check if any artist_mbids match the musicbrainz_ids\n",
    "def matches_musicbrainz_ids(artist_mbids_str):\n",
    "    # Parse the string of artist_mbids into a list\n",
    "    artist_mbids = artist_mbids_str.strip('{}').split(',')\n",
    "    # Check if any of the artist_mbids is in the set of musicbrainz_ids\n",
    "    return any(mbid.strip() in musicbrainz_ids for mbid in artist_mbids)\n",
    "\n",
    "# Apply the function to filter df1\n",
    "df1['artist_mbids_match'] = df1['artist_mbids'].apply(matches_musicbrainz_ids)\n",
    "filtered_df1 = df1[df1['artist_mbids_match']]\n",
    "\n",
    "# Drop the helper column after filtering\n",
    "filtered_df1 = filtered_df1.drop(columns=['artist_mbids_match'])\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "output_csv_path = 'recordings_artist_list_onlyMatchedArtists.csv'\n",
    "filtered_df1.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV files into DataFrames\n",
    "csv_path_1 = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\WORKING\\\\recordings_artist_list_onlyMatchedArtists.csv'\n",
    "csv_path_2 = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\WORKING\\\\contributors_normalized.csv'\n",
    "\n",
    "df1 = pd.read_csv(csv_path_1)\n",
    "df2 = pd.read_csv(csv_path_2)\n",
    "\n",
    "# Merge the DataFrames on 'recording_mbid'\n",
    "# Ensure that the key for merging ('recording_mbid') exists in both DataFrames exactly as named.\n",
    "merged_df = pd.merge(df1, df2, on='recording_mbid', how='inner')\n",
    "\n",
    "# Select only the required columns\n",
    "final_df = merged_df[['recording_mbid', 'recording_name_x', 'artist_names', 'artist_mbids', 'artist_mbid', 'individual_artist_name', 'role', 'instrument']]\n",
    "\n",
    "# Optionally, rename 'recording_name_x' to 'recording_name' to correct the column name after merging\n",
    "final_df.rename(columns={'recording_name_x': 'recording_name'}, inplace=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "output_csv_path = 'contributors_normalized_onlyMatchedArtists.csv'\n",
    "final_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Combined data saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "contributors_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\WORKING\\\\contributors_normalized_onlyMatchedArtists.csv')\n",
    "artists_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\\\PRODFILES\\\\artists_table.csv', sep=';')\n",
    "artist_album_mapping_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\PRODFILES\\\\artist_album_mapping.csv', sep=';')\n",
    "track_data_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\tracks_data_full.csv', sep=';')\n",
    "\n",
    "# Step 1: Merge artists with their albums\n",
    "artists_with_albums = pd.merge(artists_df, artist_album_mapping_df, left_on='id', right_on='artistID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Merge the above with track data on albumID\n",
    "artist_tracks = pd.merge(artists_with_albums, track_data_df, left_on='albumID', right_on='album_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributors_dict = contributors_df.groupby('artist_mbid')['recording_mbid'].apply(set).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_dict = {}\n",
    "for _, row in artist_tracks.iterrows():\n",
    "    mbid = row['musicbrainz_id']\n",
    "    if mbid not in track_dict:\n",
    "        track_dict[mbid] = []\n",
    "    track_dict[mbid].append({\n",
    "        'song_title': row['song_title'],\n",
    "        'song_spotify_id': row['song_spotify_id']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map recording_mbid to song_spotify_id based on musicbrainz_id and song title\n",
    "def find_recording_mbid():\n",
    "    results = []\n",
    "    for mbid, recordings in contributors_dict.items():\n",
    "        if mbid in track_dict:\n",
    "            for record in track_dict[mbid]:\n",
    "                for recording_mbid in recordings:\n",
    "                    # Check if contributors recording_name matches track_dict song_title\n",
    "                    contributor_row = contributors_df[contributors_df['recording_mbid'] == recording_mbid]\n",
    "                    if not contributor_row.empty and contributor_row['recording_name'].iloc[0] == record['song_title']:\n",
    "                        results.append({\n",
    "                            'song_spotify_id': record['song_spotify_id'],\n",
    "                            'recording_mbid': recording_mbid\n",
    "                        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = find_recording_mbid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_process_csv(file_path):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Prepare a container for the expanded DataFrame\n",
    "    expanded_rows = []\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Manually parse the artist names and MBIDs by trimming the braces and splitting by comma\n",
    "        artist_names = row['artist_names'].strip('{}').replace('\"', '').split(',')\n",
    "        artist_mbids = row['artist_mbids'].strip('{}').split(',')\n",
    "\n",
    "        # Create a new row for each artist\n",
    "        for artist_name, artist_mbid in zip(artist_names, artist_mbids):\n",
    "            new_row = row.copy()\n",
    "            new_row['artist_names'] = f'{{\"{artist_name}\"}}'\n",
    "            new_row['artist_mbids'] = f'{{\"{artist_mbid}\"}}'\n",
    "            expanded_rows.append(new_row)\n",
    "\n",
    "    # Create a new DataFrame with the expanded rows\n",
    "    expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "    return expanded_df\n",
    "\n",
    "\n",
    "# Example file path\n",
    "file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\WORKING\\\\contributors_normalized_onlyMatchedArtists.csv'  # Update this with the actual file path\n",
    "\n",
    "# Process the CSV file\n",
    "processed_df = load_and_process_csv(file_path)\n",
    "\n",
    "# Save the processed DataFrame to a new CSV file\n",
    "processed_df.to_csv('processed_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "contributors_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\WORKING\\\\contributors_normalized_onlyMatchedArtists_artistlinebyline.csv')\n",
    "artists_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\\\PRODFILES\\\\artists_table.csv', sep=';')\n",
    "artist_album_mapping_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\PRODFILES\\\\artist_album_mappings.csv', sep=';')\n",
    "track_data_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\PRODFILES\\\\tracks_data_actually_full.csv', sep=';')\n",
    "\n",
    "artists_with_albums = pd.merge(artists_df, artist_album_mapping_df, left_on='id', right_on='artistID', how='inner')\n",
    "artist_tracks = pd.merge(artists_with_albums, track_data_df, left_on='albumID', right_on='album_id', how='inner')\n",
    "#contributors_dict = contributors_df.groupby('artist_mbid')['recording_mbid'].apply(set).to_dict()\n",
    "\n",
    "track_dict = {}\n",
    "for _, row in artist_tracks.iterrows():\n",
    "    mbid = row['musicbrainz_id']\n",
    "    if mbid not in track_dict:\n",
    "        track_dict[mbid] = []\n",
    "    track_dict[mbid].append({\n",
    "        'song_title': row['song_title'],\n",
    "        'song_spotify_id': row['song_spotify_id']\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributors_dict = {}\n",
    "recording_main_artist_added = set()  # A set to track which recording and main artist combinations have been added\n",
    "\n",
    "for _, row in contributors_df.iterrows():\n",
    "    # Clean each mbid thoroughly\n",
    "    mbid = row['artist_mbid'].replace('\"', '').strip()\n",
    "    recording_mbid = row['recording_mbid']\n",
    "    main_artist_mbids = [x.replace('\"', '').strip() for x in row['artist_mbids'].strip('{}').split(',')]\n",
    "\n",
    "    # Handle individual contributor\n",
    "    if mbid not in contributors_dict:\n",
    "        contributors_dict[mbid] = []\n",
    "    contributors_dict[mbid].append({\n",
    "        'recording_mbid': recording_mbid,\n",
    "        'recording_name': row['recording_name'],\n",
    "        'role': row['role']\n",
    "    })\n",
    "\n",
    "    # Handle main artist(s)\n",
    "    for main_mbid in main_artist_mbids:\n",
    "        if (main_mbid, recording_mbid) not in recording_main_artist_added:\n",
    "            if main_mbid not in contributors_dict:\n",
    "                contributors_dict[main_mbid] = []\n",
    "            contributors_dict[main_mbid].append({\n",
    "                'recording_mbid': recording_mbid,\n",
    "                'recording_name': row['recording_name'],\n",
    "                'role': 'main_artist'\n",
    "            })\n",
    "            recording_main_artist_added.add((main_mbid, recording_mbid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributors_dict = {}\n",
    "for _, row in contributors_df.iterrows():\n",
    "    mbid = row['artist_mbid']\n",
    "    if mbid not in contributors_dict:\n",
    "        contributors_dict[mbid] = []\n",
    "    contributors_dict[mbid].append({\n",
    "        'recording_mbid': row['recording_mbid'],\n",
    "        'recording_name': row['recording_name']\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING CONTIRBUTORS_DICT\n",
    "contributors_df = pd.read_csv(\n",
    "    'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\WORKING\\\\contributors_normalized_onlyMatchedArtists_artistlinebyline.csv',\n",
    "    escapechar='\\\\',  # Add escape character if needed\n",
    "    quotechar='\"',    # Set quote character to handle quoted strings properly\n",
    "    dtype=str         # Treat all data as strings\n",
    ")\n",
    "contributors_dict = {}\n",
    "for _, row in contributors_df.iterrows():\n",
    "    mbid = row['artist_mbid']\n",
    "    if mbid not in contributors_dict:\n",
    "        contributors_dict[mbid] = []\n",
    "    contributors_dict[mbid].append({\n",
    "        'recording_mbid': row['recording_mbid'],\n",
    "        'recording_name': row['recording_name'],\n",
    "        'role': row['role'],  # Assuming you want to also capture the role\n",
    "        'individual_artist_name': row['individual_artist_name']  # Capturing individual artist name\n",
    "    })\n",
    "\n",
    "import csv\n",
    "\n",
    "# File path for the CSV output\n",
    "output_file = 'output_contributors_TEST.csv'\n",
    "\n",
    "# Writing to a CSV file\n",
    "with open(output_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header row\n",
    "    writer.writerow(['Artist MBID', 'Recording MBID', 'Recording Name'])\n",
    "    \n",
    "    # Write data rows\n",
    "    for artist_mbid, recordings in contributors_dict.items():\n",
    "        for recording in recordings:\n",
    "            writer.writerow([artist_mbid, recording['recording_mbid'], recording['recording_name']])\n",
    "\n",
    "print(f\"Data successfully written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# File path for the CSV output\n",
    "output_file = 'output_contributors_with_mainartists_quotes_stripped.csv'\n",
    "\n",
    "# Writing to a CSV file\n",
    "with open(output_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header row\n",
    "    writer.writerow(['Artist MBID', 'Recording MBID', 'Recording Name'])\n",
    "    \n",
    "    # Write data rows\n",
    "    for artist_mbid, recordings in contributors_dict.items():\n",
    "        for recording in recordings:\n",
    "            writer.writerow([artist_mbid, recording['recording_mbid'], recording['recording_name']])\n",
    "\n",
    "print(f\"Data successfully written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# File path for the CSV output\n",
    "output_file = 'track_dict_1.csv'\n",
    "\n",
    "# Open a file to write\n",
    "with open(output_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file, quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    # Write the header row\n",
    "    writer.writerow(['Artist MBID', 'Song Title', 'Song Spotify ID'])\n",
    "    \n",
    "    # Write data rows\n",
    "    for mbid, tracks in track_dict.items():\n",
    "        for track in tracks:\n",
    "            writer.writerow([mbid, track['song_title'], track['song_spotify_id']])\n",
    "\n",
    "print(f\"Data successfully written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbid_to_spotify_id = {}\n",
    "\n",
    "# Iterate over the smaller dictionary to reduce the number of lookups\n",
    "for mbid, tracks in track_dict.items():\n",
    "    # Check if the same mbid is in contributors_dict\n",
    "    if mbid in contributors_dict:\n",
    "        recordings = contributors_dict[mbid]\n",
    "        \n",
    "        # Create a quick lookup for song titles to Spotify IDs from track_dict\n",
    "        song_title_to_spotify_id = {track['song_title']: track['song_spotify_id'] for track in tracks}\n",
    "        \n",
    "        # Iterate over each recording for the current mbid\n",
    "        for recording in recordings:\n",
    "            # If recording name matches a song title, map the MBIDs\n",
    "            if recording['recording_name'] in song_title_to_spotify_id:\n",
    "                spotify_id = song_title_to_spotify_id[recording['recording_name']]\n",
    "                # Add to the result dictionary\n",
    "                mbid_to_spotify_id[recording['recording_mbid']] = spotify_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result dictionary for recording MBID to Spotify ID and Artist MBID mapping\n",
    "mbid_to_spotify_id_and_artist = {}\n",
    "\n",
    "# Iterate over the smaller dictionary to reduce the number of lookups\n",
    "for mbid, tracks in track_dict.items():\n",
    "    # Check if the same mbid is in contributors_dict\n",
    "    if mbid in contributors_dict:\n",
    "        recordings = contributors_dict[mbid]\n",
    "        \n",
    "        # Create a quick lookup for song titles to Spotify IDs from track_dict\n",
    "        song_title_to_spotify_id = {track['song_title']: track['song_spotify_id'] for track in tracks}\n",
    "        \n",
    "        # Iterate over each recording for the current mbid\n",
    "        for recording in recordings:\n",
    "            # If recording name matches a song title, map the MBIDs\n",
    "            if recording['recording_name'] in song_title_to_spotify_id:\n",
    "                spotify_id = song_title_to_spotify_id[recording['recording_name']]\n",
    "                # Add to the result dictionary\n",
    "                # Now also storing the artist MBID for each match\n",
    "                mbid_to_spotify_id_and_artist[recording['recording_mbid']] = {'spotify_id': spotify_id, 'artist_mbid': mbid}\n",
    "\n",
    "# mbid_to_spotify_id_and_artist now contains the mapping from recording MBIDs to Spotify IDs along with Artist MBIDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for the CSV output\n",
    "output_file = 'output_mbid_spotify_mapping_with_main_artists_withquotesstripped.csv'\n",
    "\n",
    "# Writing to a CSV file\n",
    "with open(output_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header row\n",
    "    writer.writerow(['Recording MBID', 'Spotify ID', 'Artist MBID'])\n",
    "    \n",
    "    # Write data rows\n",
    "    for recording_mbid, info in mbid_to_spotify_id_and_artist.items():\n",
    "        writer.writerow([recording_mbid, info['spotify_id'], info['artist_mbid']])\n",
    "\n",
    "print(f\"Data successfully written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_tracks(mbid, track_details):\n",
    "    results = []\n",
    "    if mbid in contributors_dict:\n",
    "        for record in track_details:\n",
    "            for recording_mbid in contributors_dict[mbid]:\n",
    "                contributor_row = contributors_df[contributors_df['recording_mbid'] == recording_mbid]\n",
    "                if not contributor_row.empty and contributor_row['recording_name'].iloc[0] == record['song_title']:\n",
    "                    results.append({\n",
    "                        'song_spotify_id': record['song_spotify_id'],\n",
    "                        'recording_mbid': recording_mbid\n",
    "                    })\n",
    "    return results\n",
    "from multiprocessing import Pool, cpu_count\n",
    "# Using multiprocessing.Pool to parallelize the processing\n",
    "if __name__ == '__main__':  # Required for multiprocessing safety on Windows\n",
    "    with Pool(processes=cpu_count()) as pool:  # Using all available CPU cores\n",
    "        results = pool.map(process_tracks, list(track_dict.keys()))\n",
    "\n",
    "# Flatten the list of results\n",
    "flattened_results = [item for sublist in results for item in sublist]\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "final_df = pd.DataFrame(flattened_results)\n",
    "final_df.to_csv('final_tracks_with_mapped_mbid.csv', sep=';', index=False)\n",
    "\n",
    "print(\"Mapping complete and saved to final_tracks_with_mapped_mbid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the number of CPU cores\n",
    "cpu_cores = os.cpu_count()\n",
    "print(f\"Number of CPU cores: {cpu_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tracks_with_mbid = artist_tracks.dropna(subset=['mapped_recording_mbid'])\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "final_tracks_with_mbid.to_csv('final_tracks_with_mapped_mbid.csv', sep=';', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where no recording_mbid was mapped (if necessary)\n",
    "# final_tracks_with_mbid = artist_tracks.dropna(subset=['mapped_recording_mbid'])\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "final_tracks_with_mbid.to_csv('final_tracks_with_mapped_mbid.csv', sep=';', index=False)\n",
    "\n",
    "print(\"Mapping complete and saved to final_tracks_with_mapped_mbid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\scraping_final.csv'\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "# Remove rows where 'album_id' is empty\n",
    "df_filtered = df[df['album_id'].notna()]\n",
    "\n",
    "# Save the filtered DataFrame back to a new CSV file\n",
    "output_file_path = 'scraping_final_new.csv'  # Replace with your desired output file path\n",
    "df_filtered.to_csv(output_file_path, index=False, sep=';')\n",
    "\n",
    "print(f\"Filtered file saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have already read the CSV into a DataFrame named df\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\scraping_final_new.csv', delimiter=';')\n",
    "\n",
    "# Function to truncate strings longer than 254 characters\n",
    "def truncate_string(s):\n",
    "    return s[:254] if isinstance(s, str) and len(s) > 254 else s\n",
    "\n",
    "# Apply the truncation function to the 'song_title' column\n",
    "df['song_title'] = df['song_title'].apply(truncate_string)\n",
    "\n",
    "# Save the DataFrame back to CSV\n",
    "df.to_csv('scraping_final_new_new.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Parameters\n",
    "input_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\PRODFILES\\\\NONSPLIT\\\\tracks_full.csv'  # Replace with the path to your large CSV\n",
    "output_file_base_path = 'tracks'  # Replace with your desired output base path\n",
    "max_rows_per_file = 200000  # Maximum rows per subfile\n",
    "delimiter = ';'  # Define the delimiter used in your CSV file\n",
    "\n",
    "chunk_number = 1\n",
    "for chunk in pd.read_csv(input_file_path, delimiter=delimiter, chunksize=max_rows_per_file):\n",
    "    output_file_path = f\"{output_file_base_path}_part_{chunk_number}.csv\"\n",
    "    chunk.to_csv(output_file_path, index=False, sep=delimiter)\n",
    "    chunk_number += 1\n",
    "\n",
    "print(f\"Splitting complete. {chunk_number} files were created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'path_to_your_csv_file.csv' with the path to your CSV file\n",
    "input_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\related_artists.csv'\n",
    "# Replace 'path_to_your_output_csv_file.csv' with the desired path for the output CSV file\n",
    "output_file_path = 'related_artists.csv'\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(input_file_path, delimiter=';')\n",
    "\n",
    "# Add the 'ID' column starting with 1 and incrementing by 1 for each row\n",
    "df.insert(0, 'ID', range(1, 1 + len(df)))\n",
    "\n",
    "# Save the DataFrame with the new 'ID' column back to a new CSV file\n",
    "df.to_csv(output_file_path, index=False, sep=';')\n",
    "\n",
    "# Print out a message to confirm completion\n",
    "print(f\"New CSV file with 'ID' column saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the text file containing the IDs (assuming it's saved as 'ids.txt' in JSON list format)\n",
    "with open('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\artist_albums\\\\FULL_ALBUM_LIST.txt', 'r') as file:\n",
    "    id_list = eval(file.read())\n",
    "\n",
    "# Load the CSV file (assuming it's named 'data.csv' and uses ';' as the delimiter)\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\PRODFILES\\\\album_table.csv', delimiter=';')\n",
    "\n",
    "# Extract the column of IDs from the CSV file\n",
    "csv_ids_dict = {id_: True for id_ in df['album_spotify_id'].tolist()}\n",
    "\n",
    "# Find IDs that are in the text file list but not in the CSV column\n",
    "missing_ids = [id_ for id_ in id_list if id_ not in csv_ids_dict]\n",
    "\n",
    "with open('missing_ids.txt', 'w') as output_file:\n",
    "    for id_ in missing_ids:\n",
    "        output_file.write(id_ + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file containing songs (assuming it's named 'songs.csv' and uses ';' as the delimiter)\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\tracks_data_full.csv', delimiter=';')\n",
    "\n",
    "# Convert the song Spotify IDs from the CSV into a dictionary for faster lookup\n",
    "song_ids_dict = {id_: True for id_ in df['song_spotify_id'].tolist()}\n",
    "\n",
    "# Assuming the list of song IDs is stored in a text file named 'song_ids.txt', one ID per line\n",
    "with open('unique_track_ids.txt', 'r') as file:\n",
    "    # Read the file and split by lines to create a list of IDs\n",
    "    song_list = file.read().splitlines()\n",
    "\n",
    "# Find song IDs that are not in the CSV file using dictionary\n",
    "unique_song_ids = [song_id for song_id in song_list if song_id not in song_ids_dict]\n",
    "\n",
    "# Save the list of unique song IDs to a new text file\n",
    "with open('unique_song_ids.txt', 'w') as output_file:\n",
    "    for song_id in unique_song_ids:\n",
    "        output_file.write(song_id + '\\n')\n",
    "\n",
    "print(\"Unique song IDs have been saved to 'unique_song_ids.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\PRODFILES\\\\album_table.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "\n",
    "# Function to convert date format\n",
    "def convert_date_format(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str, '%d/%m/%Y').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return date_str  # Returns the original string if the format is incorrect\n",
    "\n",
    "# Apply the conversion function to the album_release_date column\n",
    "df['album_release_date'] = df['album_release_date'].apply(convert_date_format)\n",
    "\n",
    "# Specify the path where you want to save the updated CSV\n",
    "output_csv_file_path = 'updated_album_data.csv'\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "df.to_csv(output_csv_file_path, index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'merged_output_file.csv'\n",
    "df = pd.read_csv(file_path, sep=';')\n",
    "\n",
    "# Check if 'album_id' column exists and then convert it\n",
    "if 'album_id' in df.columns:\n",
    "    df['album_id'] = df['album_id'].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "\n",
    "# Save the modified DataFrame back to a new CSV file\n",
    "output_file_path = 'modified_file.csv'\n",
    "df.to_csv(output_file_path, index=False, sep=';')\n",
    "\n",
    "print(f\"CSV file has been processed and saved as {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "file_path_songs = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\PRODFILES\\\\tracks_data_actually_full.csv'\n",
    "file_path_recordings = 'output_mbid_spotify_mapping_with_main_artists_withquotesstripped.csv'\n",
    "\n",
    "df_songs = pd.read_csv(file_path_songs, sep=';')\n",
    "df_recordings = pd.read_csv(file_path_recordings, sep=',')\n",
    "\n",
    "# Check and process necessary columns\n",
    "if 'Recording MBID' in df_recordings.columns and 'Spotify ID' in df_recordings.columns:\n",
    "    # Rename columns for clarity and to prevent conflicts\n",
    "    df_recordings.rename(columns={'Recording MBID': 'recording_mbid', 'Spotify ID': 'song_spotify_id'}, inplace=True)\n",
    "    \n",
    "    # Convert to string if not already (important for matching)\n",
    "    df_songs['song_spotify_id'] = df_songs['song_spotify_id'].astype(str)\n",
    "    df_recordings['song_spotify_id'] = df_recordings['song_spotify_id'].astype(str)\n",
    "    \n",
    "    # Merge the DataFrames based on 'song_spotify_id'\n",
    "    df_merged = pd.merge(df_songs, df_recordings[['song_spotify_id', 'recording_mbid']], on='song_spotify_id', how='left')\n",
    "else:\n",
    "    print(\"Required columns are missing in the recordings data.\")\n",
    "\n",
    "# Save the modified DataFrame back to a new CSV file\n",
    "output_file_path = 'merged_output_file.csv'\n",
    "df_merged.to_csv(output_file_path, index=False, sep=';')\n",
    "\n",
    "print(f\"CSV file has been processed and saved as {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "file_path_songs = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\PRODFILES\\\\tracks_full.csv'\n",
    "file_path_recordings = 'output_mbid_spotify_mapping_with_main_artists_withquotesstripped.csv'\n",
    "\n",
    "df_songs = pd.read_csv(file_path_songs, sep=';')\n",
    "df_recordings = pd.read_csv(file_path_recordings, sep=',')\n",
    "\n",
    "# Rename columns for clarity and to prevent conflicts\n",
    "df_recordings.rename(columns={'Recording MBID': 'new_recording_mbid', 'Spotify ID': 'song_spotify_id'}, inplace=True)\n",
    "\n",
    "# Convert to string if not already (important for matching)\n",
    "df_songs['song_spotify_id'] = df_songs['song_spotify_id'].astype(str)\n",
    "df_recordings['song_spotify_id'] = df_recordings['song_spotify_id'].astype(str)\n",
    "\n",
    "# Merge the DataFrames based on 'song_spotify_id'\n",
    "df_merged = pd.merge(df_songs, df_recordings[['song_spotify_id', 'new_recording_mbid']], on='song_spotify_id', how='left')\n",
    "\n",
    "# Update 'recording_mbid' in the original DataFrame where applicable\n",
    "df_merged['recording_mbid'] = df_merged['new_recording_mbid'].where(pd.notnull(df_merged['new_recording_mbid']), df_merged['recording_mbid'])\n",
    "\n",
    "# Drop the temporary 'new_recording_mbid' column\n",
    "df_merged.drop(columns=['new_recording_mbid'], inplace=True)\n",
    "\n",
    "# Save the modified DataFrame back to a new CSV file\n",
    "output_file_path = 'updated_songs_file.csv'\n",
    "df_merged.to_csv(output_file_path, index=False, sep=';')\n",
    "\n",
    "print(f\"CSV file has been processed and saved as {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "path_csv_1 = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\PRODFILES\\\\tracks_full.csv'  # Update with the path to your first CSV\n",
    "path_csv_2 = 'output_mbid_spotify_mapping_with_main_artists_withquotesstripped.csv'  # Update with the path to your second CSV\n",
    "\n",
    "df1 = pd.read_csv(path_csv_1, sep=';')\n",
    "df2 = pd.read_csv(path_csv_2, sep=',')\n",
    "\n",
    "# Rename columns in df2 for clarity\n",
    "df2.rename(columns={'Recording MBID': 'recording_mbid', 'Spotify ID': 'spotify_id'}, inplace=True)\n",
    "\n",
    "# Merge df1 and df2 based on the Spotify IDs to update recording_mbid_x\n",
    "df1 = df1.merge(df2[['spotify_id', 'recording_mbid']], left_on='song_spotify_id', right_on='spotify_id', how='left')\n",
    "\n",
    "# Update the recording_mbid_x with the value from df2\n",
    "df1['recording_mbid_x'] = df1['recording_mbid']\n",
    "\n",
    "# Drop the temporary columns and the recording_mbid_y column\n",
    "df1.drop(['spotify_id', 'recording_mbid', 'recording_mbid_y'], axis=1, inplace=True)\n",
    "\n",
    "# Save the modified DataFrame back to a new CSV file\n",
    "output_file_path = 'updated_file.csv'\n",
    "df1.to_csv(output_file_path, index=False, sep=';')\n",
    "\n",
    "print(f\"CSV file has been updated and saved as {output_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "\n",
    "# Load your CSV files\n",
    "df1 = pd.read_csv('output_mbid_spotify_mapping_with_main_artists_withquotesstripped.csv')\n",
    "df2 = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\WORKING\\\\contributors_non-normalized.csv')\n",
    "\n",
    "def convert_to_set(mbids_str):\n",
    "    # Add quotes around the UUIDs to make them valid for literal evaluation\n",
    "    mbids_str_quoted = re.sub(r'([a-f0-9-]{36})', r\"'\\1'\", mbids_str)\n",
    "    try:\n",
    "        # Convert the string to a set\n",
    "        return set(ast.literal_eval(mbids_str_quoted))\n",
    "    except:\n",
    "        # Return an empty set in case of any error\n",
    "        return set()\n",
    "\n",
    "\n",
    "# Apply the conversion function to the 'artist_mbids' column\n",
    "df2['artist_mbids'] = df2['artist_mbids'].apply(convert_to_set)\n",
    "\n",
    "# Explode 'artist_mbids' into separate rows for each MBID\n",
    "df2 = df2.explode('artist_mbids')\n",
    "\n",
    "# Remove duplicates and count unique artist MBIDs for each recording MBID\n",
    "artist_count = df2.groupby('recording_mbid')['artist_mbids'].nunique().reset_index()\n",
    "artist_count.columns = ['Recording MBID', 'Unique Artist Count']\n",
    "\n",
    "# Merge this count back to the first DataFrame\n",
    "df1 = df1.merge(artist_count, on='Recording MBID', how='left')\n",
    "\n",
    "# Select the row with the highest count of unique artist MBIDs for each Spotify ID\n",
    "result = df1.loc[df1.groupby('Spotify ID')['Unique Artist Count'].idxmax()]\n",
    "\n",
    "# Save or display the result\n",
    "result.to_csv('filtered_output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "df_filtered_output = pd.read_csv('filtered_output.csv')  # This is the first CSV from the previous task\n",
    "df_songs = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\PRODFILES\\\\tracks_full.csv', delimiter=';')  # Make sure to specify the delimiter\n",
    "\n",
    "# Merge the two DataFrames on the matching Spotify ID fields\n",
    "merged_df = pd.merge(df_songs, df_filtered_output, left_on='song_spotify_id', right_on='Spotify ID', how='left')\n",
    "\n",
    "# Update the 'recording_mbid_x' column with the 'Recording MBID' from the first CSV\n",
    "merged_df['recording_mbid_x'] = merged_df['Recording MBID']\n",
    "\n",
    "# Drop the 'recording_mbid_y' column and any other unwanted columns\n",
    "merged_df.drop(columns=['recording_mbid_y', 'Recording MBID', 'Spotify ID', 'Artist MBID', 'Unique Artist Count'], inplace=True)\n",
    "\n",
    "# Save or display the resulting DataFrame\n",
    "merged_df.to_csv('final_song_data.csv', index=False)\n",
    "merged_df.head()  # Display the first few rows to check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('final_song_data.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first CSV file\n",
    "first_csv = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\filtered_output.csv')\n",
    "# Load the second CSV file\n",
    "second_csv = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\WORKING\\\\contributors_normalized.csv')\n",
    "\n",
    "# Ensure that the 'Recording MBID' column in the first CSV and 'recording_mbid' in the second CSV are strings\n",
    "first_csv['Recording MBID'] = first_csv['Recording MBID'].astype(str)\n",
    "second_csv['recording_mbid'] = second_csv['recording_mbid'].astype(str)\n",
    "\n",
    "# Filter the second CSV to only include rows where the 'recording_mbid' exists in the 'Recording MBID' column of the first CSV\n",
    "filtered_second_csv = second_csv[second_csv['recording_mbid'].isin(first_csv['Recording MBID'])]\n",
    "\n",
    "# Save the filtered data to a new CSV file (optional)\n",
    "filtered_second_csv.to_csv('contributors_normalized_filteredToOnlyMatched.csv', index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\contributors_normalized_filteredToOnlyMatched.csv', delimiter=';')\n",
    "\n",
    "# Add a new column 'ID' before 'recording_mbid' with sequential IDs starting from 1\n",
    "df.insert(0, 'ID', range(1, 1 + len(df)))\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df.to_csv('modified_output.csv', sep=';', index=False)\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first CSV file\n",
    "df1 = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\contributors_normalized_filteredToOnlyMatched.csv', delimiter=';')\n",
    "\n",
    "# Load the second CSV file\n",
    "df2 = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\PRODFILES\\\\tracks_full.csv', delimiter=';')\n",
    "\n",
    "# Ensure that 'recording_mbid' columns in both dataframes are of type string for accurate comparison\n",
    "df1['recording_mbid'] = df1['recording_mbid'].astype(str)\n",
    "df2['recording_mbid'] = df2['recording_mbid'].astype(str)\n",
    "\n",
    "# Create a mapping DataFrame based on matching 'recording_mbid' in both DataFrames\n",
    "# 'id' from df1 is 'CONTRIBUTOR_ID' and 'id' from df2 is 'SONG_TABLE_ID'\n",
    "mapping_df = df1.merge(df2, on='recording_mbid', suffixes=('_df1', '_df2'))\n",
    "\n",
    "# Select only the needed columns for the final output and rename them\n",
    "final_df = mapping_df[['id_df1', 'id_df2']].rename(columns={'id_df1': 'CONTRIBUTOR_ID', 'id_df2': 'SONG_TABLE_ID'})\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "final_df.to_csv('mapping_output.csv', sep=';', index=False)\n",
    "\n",
    "# Display the result\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\contributors_normalized_filteredToOnlyMatched.csv', delimiter=';')\n",
    "\n",
    "# Drop the specified columns\n",
    "df_dropped = df.drop(columns=['recording_mbid', 'recording_name', 'artist_credit_name', 'artist_credit_id'])\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df_dropped.to_csv('reduced_output.csv', sep=';', index=False)\n",
    "\n",
    "# Display the reduced DataFrame\n",
    "print(df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "artists_albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\PRODFILES\\\\artist_album_mapping.csv', delimiter=';')\n",
    "artists_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\PRODFILES\\\\artist_data.csv', delimiter=';')\n",
    "albums1_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\PRODFILES\\\\album_part_1.csv', delimiter=';')\n",
    "albums2_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\PRODFILES\\\\album_part_2.csv', delimiter=';')\n",
    "\n",
    "combined_albums_df = pd.concat([albums1_df, albums2_df], ignore_index=True)\n",
    "\n",
    "# Function to check if all IDs in the main DataFrame exist in another DataFrame\n",
    "def check_ids(main_df, check_df, main_column, check_column, name):\n",
    "    missing_ids = main_df[~main_df[main_column].isin(check_df[check_column])]\n",
    "    if missing_ids.empty:\n",
    "        print(f\"All {main_column} in {name} are present.\")\n",
    "    else:\n",
    "        print(f\"Missing {main_column} in {name}:\")\n",
    "        print(missing_ids)\n",
    "\n",
    "# Check if all artistIDs from artists_albums.csv are in artists.csv\n",
    "check_ids(artists_albums_df, artists_df, 'artistID', 'id', 'artists.csv')\n",
    "\n",
    "check_ids(artists_albums_df, combined_albums_df, 'albumID', 'id', 'combined albums.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\PRODFILES\\\\NONSPLIT\\\\tracks_full.csv', delimiter=';')\n",
    "\n",
    "# Remove duplicate rows\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df_cleaned.to_csv('tracks_full_NODUPES.csv', index=False, sep=';')\n",
    "\n",
    "# If you want to overwrite the original file, you can use:\n",
    "# df_cleaned.to_csv('path/to/your_file.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                artist_mbid              spotify_id\n",
      "0      2f7bc75e-4ffe-4400-a200-69cc9adaac23  3Wkf3Zv5wMaIsz2MwkVOKF\n",
      "1      870ab832-904b-4b49-ab72-7e6b0e0da65f  4xcYVPssil6vbG6tq3W43S\n",
      "2      2e0f7014-ce5e-4ce1-a3d9-f52c249482dc  5bmpsjaT6cyVlVFX8vYnBm\n",
      "3      a972372a-6d87-4b21-8f5c-5ba6e2d640b0  07tB6knv7c0JsSbkfV5iZd\n",
      "4      831fe0ce-c4b3-4044-bbf6-8db6b9ac2070  1SlJJwJtVLy0X1RjfDTmVm\n",
      "...                                     ...                     ...\n",
      "63708  8bc56b79-d760-431f-a1d3-a9f71318680a  1Bxew4Eedjg5DLlAyQZK9c\n",
      "63709  ff7b28cc-34d2-4182-a5f3-b06336e14c23  1XaPi3wwjPdMMsSw58bgCO\n",
      "63710  c47028fa-0086-426d-a7b8-ed20eba3ed61  0WmzT6tMLhdST5BfYagbha\n",
      "63711  86cb3243-62c1-4b0c-aeb9-bc8502bba200  7MRWNE1dN3o6bPMLPH0c3h\n",
      "63712  60a21ce6-01ef-49dd-8566-6351599609f4  6HkzZenXCsrdGcqMgWJECN\n",
      "\n",
      "[63713 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\working\\\\matched_artists.csv')\n",
    "\n",
    "# Assuming columns are known and fixed as \"Recording MBID\", \"Spotify ID\", \"Artist MBID\", \"Unique Artist Count\"\n",
    "df_reduced = df.drop(columns=[df.columns[0], df.columns[2]])\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df_reduced.to_csv('modified_output.csv', index=False)\n",
    "\n",
    "# Display the reduced DataFrame\n",
    "print(df_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "S2TENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
