{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# The directory containing your folders with JSON files\n",
    "root_directory = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\track_information'\n",
    "\n",
    "# The path for the output CSV file\n",
    "csv_file_path = 'tracks_data.csv'\n",
    "\n",
    "# Current date for 'Date Added To DB' and 'Date Last Modified' columns in the YYYY-MM-DD format\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Template for CSV rows with the updated header titles\n",
    "csv_columns = [\n",
    "    'id', 'song_spotify_id', 'song_title', 'song_duration', 'song_album_type', \n",
    "    'song_album_id', 'song_explicit', 'song_popularity', 'song_preview_url', \n",
    "    'song_track_features_added', 'song_acousticness', 'song_danceability', 'song_energy', \n",
    "    'song_instrumentalness', 'song_liveness', 'song_loudness', 'song_speechiness', \n",
    "    'song_tempo', 'song_valence', 'song_key', 'song_time_signature', 'song_date_added_to_db', \n",
    "    'song_date_last_modified'\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store rows for the CSV\n",
    "csv_rows = []\n",
    "\n",
    "def process_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for track in data.get('tracks', []):\n",
    "            # Skip processing if the track is None\n",
    "            if track is None:\n",
    "                continue\n",
    "\n",
    "            row = {\n",
    "                'id': -1,  # This will be updated later with the actual ID\n",
    "                'song_spotify_id': track['id'],\n",
    "                'song_title': track['name'],\n",
    "                'song_duration': track['duration_ms'],\n",
    "                'song_album_type': track['album']['album_type'].upper(),\n",
    "                'song_album_id': track['album']['id'],\n",
    "                'song_explicit': track['explicit'],\n",
    "                'song_popularity': track['popularity'],\n",
    "                'song_preview_url': track.get('preview_url', ''),\n",
    "                'song_track_features_added': False,\n",
    "                'song_acousticness': -1,\n",
    "                'song_danceability': -1,\n",
    "                'song_energy': -1,\n",
    "                'song_instrumentalness': -1,\n",
    "                'song_liveness': -1,\n",
    "                'song_loudness': -1,\n",
    "                'song_speechiness': -1,\n",
    "                'song_tempo': -1,\n",
    "                'song_valence': -1,\n",
    "                'song_key': -1,\n",
    "                'song_time_signature': -1,\n",
    "                'song_date_added_to_db': current_date,\n",
    "                'song_date_last_modified': current_date\n",
    "            }\n",
    "            csv_rows.append(row)\n",
    "\n",
    "# Iterate over each subfolder and JSON file in the directory\n",
    "for subdir, dirs, files in os.walk(root_directory):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.json'):\n",
    "            print(f\"Processing {filename}...\")\n",
    "            process_json_file(os.path.join(subdir, filename))\n",
    "\n",
    "# Write the CSV file\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=csv_columns, delimiter=';')\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Update each row with its ID before writing\n",
    "    for i, row in enumerate(csv_rows, start=1):\n",
    "        row['id'] = i\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"CSV file has been successfully created at {csv_file_path} with {len(csv_rows)} tracks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the root directory containing your JSON files\n",
    "root_directory = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\ALBUMS'\n",
    "\n",
    "# Define the output CSV file path\n",
    "csv_file_path = 'album_data.csv'\n",
    "\n",
    "# Current date for 'Date Added To DB' and 'Date Last Modified' columns\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# CSV column headers\n",
    "csv_columns = [\n",
    "    'id', 'album_spotify_id', 'album_name', 'album_cover_art', 'album_release_date',\n",
    "    'release_date_precision', 'album_popularity', 'album_type', 'spotify_album_upc',\n",
    "    'spotify_album_ean', 'spotify_album_isrc', 'date_added_to_db', 'date_last_modified',\n",
    "    'musicbrainz_metadata_added', 'musicbrainz_id'\n",
    "]\n",
    "\n",
    "# Initialize a list to hold album data\n",
    "albums_data = []\n",
    "\n",
    "# Function to process each JSON file\n",
    "def process_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for album in data.get('albums', []):\n",
    "            # Extract the required information, with checks for nullable fields\n",
    "            album_data = {\n",
    "                'id': -1,  # Placeholder, will be updated later\n",
    "                'album_spotify_id': album['id'],\n",
    "                'album_name': album['name'],\n",
    "                'album_cover_art': album['images'][0]['url'] if album.get('images') else '',\n",
    "                'album_release_date': album['release_date'],\n",
    "                'release_date_precision': album['release_date_precision'],\n",
    "                'album_popularity': album['popularity'],\n",
    "                'album_type': album['album_type'],\n",
    "                'spotify_album_upc': album['external_ids'].get('upc', '') if album.get('external_ids') else '',\n",
    "                'spotify_album_ean': album['external_ids'].get('ean', '') if album.get('external_ids') else '',\n",
    "                'spotify_album_isrc': album['external_ids'].get('isrc', '') if album.get('external_ids') else '',\n",
    "                'date_added_to_db': current_date,\n",
    "                'date_last_modified': current_date,\n",
    "                'musicbrainz_metadata_added': False,  # Placeholder\n",
    "                'musicbrainz_id': ''  # Placeholder\n",
    "            }\n",
    "            albums_data.append(album_data)\n",
    "\n",
    "# Process each JSON file in the directory and subdirectories\n",
    "for subdir, dirs, files in os.walk(root_directory):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.json'):\n",
    "            process_json_file(os.path.join(subdir, filename))\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=csv_columns, delimiter=';')\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Update each row with its actual ID before writing\n",
    "    for i, album_data in enumerate(albums_data, start=1):\n",
    "        album_data['id'] = i\n",
    "        writer.writerow(album_data)\n",
    "\n",
    "print(f\"CSV file has been successfully created at {csv_file_path} with {len(albums_data)} albums.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the dtypes for the IDs to be strings when reading the CSVs\n",
    "dtype_dict = {'id': str, 'song_album_id': str, 'album_spotify_id': str}\n",
    "tracks_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\tracks_data.csv', delimiter=';', dtype=dtype_dict)\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\album_data.csv', delimiter=';', dtype=dtype_dict)\n",
    "\n",
    "# Create a dictionary mapping from album_spotify_id to id from albums_df\n",
    "# Ensure the 'id' column in albums_df is converted to integer if it's not NaN\n",
    "album_id_map = albums_df.dropna(subset=['id']).set_index('album_spotify_id')['id'].astype(int).to_dict()\n",
    "\n",
    "# Map the song_album_id in tracks_df using the album_id_map to get the album id\n",
    "tracks_df['album_id'] = tracks_df['song_album_id'].map(album_id_map)\n",
    "\n",
    "# Convert the new album_id column to integers, NaNs will be converted to a float with a .0\n",
    "tracks_df['album_id'] = tracks_df['album_id'].fillna(-1).astype(int)\n",
    "\n",
    "# Replace -1 back to NaN if you want to keep NaN values\n",
    "tracks_df['album_id'].replace(-1, pd.NA, inplace=True)\n",
    "\n",
    "# Save the updated tracks DataFrame to a new CSV file\n",
    "tracks_df.to_csv('path_to_updated_tracks.csv', index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Read the albums and artists CSVs into DataFrames\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_data_full.csv', delimiter=';', dtype={'id': str, 'album_spotify_id': str})\n",
    "artists_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\artists_data_full.csv', delimiter=';', dtype={'id': str, 'artist_spotify_id': str})\n",
    "\n",
    "# Dictionary to map Spotify album ID to CSV album ID\n",
    "album_id_map = albums_df.set_index('album_spotify_id')['id'].to_dict()\n",
    "\n",
    "# Dictionary to map Spotify artist ID to CSV artist ID\n",
    "artist_id_map = artists_df.set_index('artist_spotify_id')['id'].to_dict()\n",
    "\n",
    "# Initialize a list to hold the artist-album mappings\n",
    "artist_album_mapping = []\n",
    "\n",
    "# Assuming 'path_to_json_folder' is the folder containing all the JSON subfolders\n",
    "for root, dirs, files in os.walk('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\ALBUMS'):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for album in data['albums']:\n",
    "                    album_id = album_id_map.get(album['id'])\n",
    "                    if album_id:\n",
    "                        for artist in album['artists']:\n",
    "                            artist_id = artist_id_map.get(artist['id'])\n",
    "                            if artist_id:\n",
    "                                artist_album_mapping.append({'artistID': artist_id, 'albumID': album_id})\n",
    "\n",
    "# Create a DataFrame from the artist-album mappings\n",
    "artist_album_df = pd.DataFrame(artist_album_mapping)\n",
    "\n",
    "# Remove duplicates if there are any\n",
    "artist_album_df = artist_album_df.drop_duplicates()\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "artist_album_df.to_csv('artist_album_mappings_new.csv', index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Read the albums and artists CSVs into DataFrames\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\album_data.csv', delimiter=';', dtype={'id': str, 'album_spotify_id': str})\n",
    "artists_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\artists_data_full.csv', delimiter=';', dtype={'id': str, 'artist_spotify_id': str})\n",
    "\n",
    "# Dictionary to map Spotify album ID to CSV album ID\n",
    "album_id_map = albums_df.set_index('album_spotify_id')['id'].to_dict()\n",
    "\n",
    "# Dictionary to map Spotify artist ID to CSV artist ID\n",
    "artist_id_map = artists_df.set_index('artist_spotify_id')['id'].to_dict()\n",
    "\n",
    "# Initialize a list to hold the artist-album mappings\n",
    "artist_album_mapping = []\n",
    "\n",
    "# List to hold artist Spotify IDs where CSV artist ID was not found\n",
    "missing_artist_ids = []\n",
    "\n",
    "# Assuming 'path_to_json_folder' is the folder containing all the JSON subfolders\n",
    "for root, dirs, files in os.walk('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\ALBUMS'):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for album in data['albums']:\n",
    "                    album_id = album_id_map.get(album['id'])\n",
    "                    if album_id:\n",
    "                        for artist in album['artists']:\n",
    "                            artist_spotify_id = artist['id']\n",
    "                            artist_id = artist_id_map.get(artist_spotify_id)\n",
    "                            if artist_id:\n",
    "                                artist_album_mapping.append({'artistID': artist_id, 'albumID': album_id})\n",
    "                            else:\n",
    "                                missing_artist_ids.append(artist_spotify_id)\n",
    "\n",
    "# Print the list of artist Spotify IDs where the CSV artist ID was not found\n",
    "print(\"List of artist Spotify IDs where the CSV artist ID was not found:\")\n",
    "for artist_id in missing_artist_ids:\n",
    "    print(artist_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "missing_artist_ids_df = pd.DataFrame(missing_artist_ids, columns=['missing_artist_spotify_id'])\n",
    "\n",
    "# Define the file path where you want to save the CSV\n",
    "file_path = 'missing_artist_ids.csv'  # You can specify your own path\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "missing_artist_ids_df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f'The missing artist IDs have been saved to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'input_file.csv' with the path to your input CSV file\n",
    "input_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\musicBrainzAllContributors.csv'\n",
    "# Replace 'output_file.csv' with the path where you want to save the output CSV file\n",
    "output_file_path = 'contributor_data_prelink.csv'\n",
    "\n",
    "# Step 1: Read the input CSV\n",
    "input_df = pd.read_csv(input_file_path, delimiter=',')\n",
    "\n",
    "# Step 2: Create the new DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    'ID': range(1, len(input_df) + 1),\n",
    "    'NAME': input_df['artist_credit_name'],\n",
    "    'ROLE': input_df['role'],\n",
    "    'INSTRUMENT': input_df['instrument'],\n",
    "    'MUSICBRAINZ_ID': input_df['artist_mbid'],\n",
    "    'MAINARTIST': input_df['artist_credit_name'],\n",
    "    'SONGTITLE': input_df['recording_name']\n",
    "})\n",
    "\n",
    "# Step 3: Write the resulting DataFrame to a new CSV file\n",
    "output_df.to_csv(output_file_path, sep=';', index=False)\n",
    "\n",
    "print(f\"Output CSV saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Load the CSV files\n",
    "musicbrainz_df = pd.read_csv('C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\prepopulationStuff\\\\PythonNotebooksForPrepopulation\\\\contributor_data_prelink.csv', sep=';')  # The file generated from the previous step\n",
    "spotify_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\tracks_data.csv', sep=';')  # The Spotify songs CSV\n",
    "\n",
    "# Create a list of combined song title and artist names from Spotify for matching\n",
    "spotify_combined_list = spotify_df.apply(lambda x: f\"{x['song_title']} {x['song_album_id']}\", axis=1).tolist()\n",
    "\n",
    "def find_best_match(mb_row, spotify_combined_list):\n",
    "    mb_combined = f\"{mb_row['SONGTITLE']} {mb_row['MAINARTIST']}\"\n",
    "    \n",
    "    # Using extractOne to find the best match from the list\n",
    "    best_match_info = process.extractOne(mb_combined, spotify_combined_list)\n",
    "    \n",
    "    # If there is a match found, extract it\n",
    "    if best_match_info:\n",
    "        best_match_text, best_score = best_match_info\n",
    "        # Find the index of the match in Spotify list to retrieve the full row from spotify_df\n",
    "        match_index = spotify_combined_list.index(best_match_text)\n",
    "        best_match_row = spotify_df.iloc[match_index]\n",
    "        return best_match_row, best_score\n",
    "    return None, 0\n",
    "\n",
    "# Prepare the output DataFrame\n",
    "matched_df = pd.DataFrame(columns=['CONTRIBUTOR_ID', 'SONG_TABLE_ID', 'Spotify_Song_Title', 'Spotify_Artist', 'MusicBrainz_Song_Title', 'MusicBrainz_Artist'])\n",
    "\n",
    "# Iterate over MusicBrainz entries to find matches\n",
    "for index, mb_row in musicbrainz_df.iterrows():\n",
    "    best_match_row, score = find_best_match(mb_row, spotify_combined_list)\n",
    "    if best_match_row is not None and score > 80:  # Adjust the threshold as needed\n",
    "        matched_df = matched_df.append({\n",
    "            'CONTRIBUTOR_ID': mb_row['ID'],\n",
    "            'SONG_TABLE_ID': best_match_row['id'],\n",
    "            'Spotify_Song_Title': best_match_row['song_title'],\n",
    "            'Spotify_Artist': best_match_row['song_album_id'],  # Note: Adjust if there's a more direct artist name column\n",
    "            'MusicBrainz_Song_Title': mb_row['SONGTITLE'],\n",
    "            'MusicBrainz_Artist': mb_row['MAINARTIST']\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# Output the matched DataFrame to CSV\n",
    "matched_df.to_csv('linked_songs.csv', sep=';', index=False)\n",
    "\n",
    "print(\"Linked songs CSV generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\contributors_non-normalized.csv')\n",
    "\n",
    "# Select only the 'artist_credit_name' and 'artist_credit_id' columns\n",
    "df_selected = df[['artist_credit_name', 'artist_credit_id']]\n",
    "\n",
    "# Drop duplicates based on 'artist_credit_id' to ensure each ID is unique\n",
    "df_unique = df_selected.drop_duplicates(subset=['artist_credit_id'])\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "df_unique.to_csv('unique_artist_credits.csv', index=False)\n",
    "\n",
    "print('Unique artist credits CSV file has been saved as unique_artist_credits.csv.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def parse_manual(array_str):\n",
    "    # Manually parse the string to extract elements between curly braces\n",
    "    # Remove leading and trailing braces and split by comma\n",
    "    items = array_str.strip('{}').split(',')\n",
    "    # Strip quotes and extra spaces from each item\n",
    "    items = [item.strip('\"').strip() for item in items]\n",
    "    return items\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\contributors_non-normalized.csv')\n",
    "\n",
    "# Prepare a list to collect all artist name-MBID pairs\n",
    "artist_pairs = []\n",
    "i=0\n",
    "\n",
    "# Iterate over the DataFrame rows\n",
    "for _, row in df.iterrows():\n",
    "    # Parse the 'artist_mbids' and 'individual_artist_names' fields manually\n",
    "    artist_mbids = parse_manual(row['artist_mbids'])\n",
    "    individual_artist_names = parse_manual(row['individual_artist_names'])\n",
    "    \n",
    "    # Ensure we have equal lengths of MBIDs and names before proceeding\n",
    "    if len(artist_mbids) == len(individual_artist_names):\n",
    "        # Pair each individual artist name with its corresponding MBID\n",
    "        for artist_name, artist_mbid in zip(individual_artist_names, artist_mbids):\n",
    "            artist_pairs.append((artist_name, artist_mbid))\n",
    "    else:\n",
    "        print(\"Warning: Mismatched MBIDs and artist names for a row, skipping.\")\n",
    "        print(f\"Row index: {i}\")\n",
    "        i = i+1\n",
    "\n",
    "# Convert the list of pairs into a DataFrame, ensuring uniqueness\n",
    "df_pairs = pd.DataFrame(list(set(artist_pairs)), columns=['individual_artist_name', 'artist_mbid'])\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "df_pairs.to_csv('artist_name_mbid_pairs.csv', index=False)\n",
    "\n",
    "print('Artist name-MBID pairs CSV file has been saved as artist_name_mbid_pairs.csv.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the MBID CSV\n",
    "mbid_df = pd.read_csv('C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\prepopulationStuff\\\\PythonNotebooksForPrepopulation\\\\artist_name_mbid_pairs.csv')\n",
    "\n",
    "# Load the Spotify artist CSV, remember to use the ';' delimiter\n",
    "spotify_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\artists_data.csv', delimiter=';')\n",
    "\n",
    "# Perform a direct merge based on artist names\n",
    "# Note: This assumes 'artist_name' in spotify_df exactly matches 'individual_artist_name' in mbid_df\n",
    "merged_df = pd.merge(spotify_df, mbid_df, how='left', left_on='artist_name', right_on='individual_artist_name')\n",
    "\n",
    "# Drop the 'individual_artist_name' column as it's redundant after merge\n",
    "merged_df.drop(columns=['individual_artist_name'], inplace=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('spotify_artist_with_mbid_direct_match.csv', index=False)\n",
    "\n",
    "print('Spotify artist data with direct match MBIDs has been saved as spotify_artist_with_mbid_direct_match.csv.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adjust the path to your Spotify artists CSV file\n",
    "spotify_csv_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\artists_data.csv'\n",
    "spotify_df = pd.read_csv(spotify_csv_path, delimiter=';')\n",
    "\n",
    "# Print column names to verify\n",
    "print(spotify_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\src\\\\main\\\\resources\\\\config\\\\liquibase\\\\fake-data\\\\artists_table_new_notworking.csv'\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "\n",
    "# Function to check for empty values in columns that shouldn't have them\n",
    "def check_empty_values(df, columns):\n",
    "    for column in columns:\n",
    "        if df[column].isnull().any():\n",
    "            print(f\"Empty values found in column: {column}\")\n",
    "        else:\n",
    "            print(f\"No empty values in column: {column}\")\n",
    "\n",
    "# Function to check data types\n",
    "def check_data_types(df):\n",
    "    errors = []\n",
    "    for column, expected_type in expected_column_types.items():\n",
    "        if expected_type == 'numeric':\n",
    "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "                errors.append(f\"Column {column} is not of type {expected_type}\")\n",
    "        elif expected_type == 'string':\n",
    "            if not pd.api.types.is_string_dtype(df[column]):\n",
    "                errors.append(f\"Column {column} is not of type {expected_type}\")\n",
    "        elif expected_type == 'date':\n",
    "            try:\n",
    "                pd.to_datetime(df[column])\n",
    "            except ValueError:\n",
    "                errors.append(f\"Column {column} contains non-date values\")\n",
    "    \n",
    "    if errors:\n",
    "        for error in errors:\n",
    "            print(error)\n",
    "    else:\n",
    "        print(\"All columns match their expected data types.\")\n",
    "\n",
    "# Columns expected not to have empty values based on your schema (all of them in this case)\n",
    "mandatory_columns = [\n",
    "    'id', 'artist_spotify_id', 'artist_name', 'artist_popularity',\n",
    "    'artist_image_small', 'artist_image_medium', 'artist_image_large',\n",
    "    'artist_followers', 'date_added_to_db', 'date_last_modified'\n",
    "]\n",
    "\n",
    "# Expected data types\n",
    "expected_column_types = {\n",
    "    'id': 'numeric',\n",
    "    'artist_spotify_id': 'string',\n",
    "    'artist_name': 'string',\n",
    "    'artist_popularity': 'numeric',\n",
    "    'artist_image_small': 'string',\n",
    "    'artist_image_medium': 'string',\n",
    "    'artist_image_large': 'string',\n",
    "    'artist_followers': 'numeric',\n",
    "    'date_added_to_db': 'date',\n",
    "    'date_last_modified': 'date'\n",
    "}\n",
    "\n",
    "# Perform checks\n",
    "check_empty_values(df, mandatory_columns)\n",
    "check_data_types(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_data_full.csv', sep=';')  # Adjust the path and separator as needed\n",
    "\n",
    "# Convert the dates in the last two columns\n",
    "# Assuming the last two columns contain your dates\n",
    "df.iloc[:, -4] = pd.to_datetime(df.iloc[:, -4], dayfirst=True).dt.strftime('%Y-%m-%d')\n",
    "df.iloc[:, -3] = pd.to_datetime(df.iloc[:, -3], dayfirst=True).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Save the modified DataFrame back to a new CSV file\n",
    "df.to_csv('path_to_modified_csv.csv', index=False, sep=';')  # Adjust the path and separator as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_data_full.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "\n",
    "# Function to check for empty values in mandatory columns\n",
    "def check_empty_values(df, columns):\n",
    "    for column in columns:\n",
    "        if df[column].isnull().any():\n",
    "            print(f\"Empty values found in column: {column}\")\n",
    "        else:\n",
    "            print(f\"No empty values in column: {column}\")\n",
    "\n",
    "# Function to verify data types\n",
    "def check_data_types(df, column_types):\n",
    "    for column, expected_type in column_types.items():\n",
    "        if expected_type == 'numeric':\n",
    "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of numeric type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct numeric type.\")\n",
    "        elif expected_type == 'string':\n",
    "            if not pd.api.types.is_string_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of string type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct string type.\")\n",
    "        elif expected_type == 'date':\n",
    "            try:\n",
    "                pd.to_datetime(df[column])\n",
    "                print(f\"Column {column} is of correct date type.\")\n",
    "            except ValueError:\n",
    "                print(f\"Column {column} contains incorrect date format.\")\n",
    "\n",
    "# Mandatory columns (not nullable)\n",
    "mandatory_columns = [\n",
    "    'id', 'album_spotify_id', 'album_name', 'album_cover_art',\n",
    "    'album_release_date', 'release_date_precision', 'album_popularity',\n",
    "    'album_type', 'date_added_to_db', 'date_last_modified', 'musicbrainz_metadata_added'\n",
    "]\n",
    "\n",
    "# Columns with specific data types\n",
    "column_types = {\n",
    "    'id': 'numeric',\n",
    "    'album_spotify_id': 'string',\n",
    "    'album_name': 'string',\n",
    "    'album_cover_art': 'string',\n",
    "    'album_release_date': 'date',\n",
    "    'release_date_precision': 'string',\n",
    "    'album_popularity': 'numeric',\n",
    "    'album_type': 'string',\n",
    "    'spotify_album_upc': 'string',  # Nullable\n",
    "    'spotify_album_ean': 'string',  # Nullable\n",
    "    'spotify_album_isrc': 'string',  # Nullable\n",
    "    'date_added_to_db': 'date',\n",
    "    'date_last_modified': 'date',\n",
    "    'musicbrainz_metadata_added': 'boolean',  # Assuming true/false representation\n",
    "    'musicbrainz_id': 'string'  # Nullable\n",
    "}\n",
    "\n",
    "# Check for empty values in mandatory columns\n",
    "check_empty_values(df, mandatory_columns)\n",
    "\n",
    "# Check if data types are correct\n",
    "check_data_types(df, column_types)\n",
    "31045;59\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\src\\\\main\\\\resources\\\\downloaded_files\\\\album_table.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "\n",
    "# Function to check for empty values in mandatory columns\n",
    "def check_empty_values(df, columns):\n",
    "    for column in columns:\n",
    "        if df[column].isnull().any():\n",
    "            print(f\"Empty values found in column: {column}\")\n",
    "        else:\n",
    "            print(f\"No empty values in column: {column}\")\n",
    "\n",
    "# Function to verify data types and print incorrect date formats\n",
    "def check_data_types_and_dates(df, column_types):\n",
    "    for column, expected_type in column_types.items():\n",
    "        if expected_type == 'numeric':\n",
    "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of numeric type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct numeric type.\")\n",
    "        elif expected_type == 'string':\n",
    "            if not pd.api.types.is_string_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of string type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct string type.\")\n",
    "        elif expected_type == 'date':\n",
    "            incorrect_format = df[column].apply(lambda x: check_date_format(x))\n",
    "            if incorrect_format.any():\n",
    "                print(f\"Rows with incorrect date format in column '{column}':\")\n",
    "                print(df[incorrect_format])\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct date type.\")\n",
    "\n",
    "# Helper function to check date format\n",
    "def check_date_format(date_string):\n",
    "    try:\n",
    "        pd.to_datetime(date_string, errors='raise')\n",
    "        return False  # Date format is correct\n",
    "    except ValueError:\n",
    "        return True  # Date format is incorrect\n",
    "\n",
    "# Mandatory columns (not nullable)\n",
    "mandatory_columns = [\n",
    "    'id', 'album_spotify_id', 'album_name', 'album_cover_art',\n",
    "    'album_release_date', 'release_date_precision', 'album_popularity',\n",
    "    'album_type', 'date_added_to_db', 'date_last_modified', 'musicbrainz_metadata_added'\n",
    "]\n",
    "\n",
    "# Columns with specific data types\n",
    "column_types = {\n",
    "    'id': 'numeric',\n",
    "    'album_spotify_id': 'string',\n",
    "    'album_name': 'string',\n",
    "    'album_cover_art': 'string',\n",
    "    'album_release_date': 'date',\n",
    "    'release_date_precision': 'string',\n",
    "    'album_popularity': 'numeric',\n",
    "    'album_type': 'string',\n",
    "    'spotify_album_upc': 'string',  # Nullable\n",
    "    'spotify_album_ean': 'string',  # Nullable\n",
    "    'spotify_album_isrc': 'string',  # Nullable\n",
    "    'date_added_to_db': 'date',\n",
    "    'date_last_modified': 'date',\n",
    "    'musicbrainz_metadata_added': 'boolean',  # Assuming true/false representation\n",
    "    'musicbrainz_id': 'string'  # Nullable\n",
    "}\n",
    "\n",
    "# Check for empty values in mandatory columns\n",
    "check_empty_values(df, mandatory_columns)\n",
    "\n",
    "# Check if data types are correct and for incorrect date formats\n",
    "check_data_types_and_dates(df, column_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\tracks_data_full.csv'  # Update this to the path of your CSV file\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')  # Ensure delimiter matches your CSV format\n",
    "\n",
    "# Function to check for empty values in mandatory columns\n",
    "def check_empty_values(df, columns):\n",
    "    for column in columns:\n",
    "        if df[column].isnull().any():\n",
    "            print(f\"Empty values found in column: {column}\")\n",
    "        else:\n",
    "            print(f\"No empty values in column: {column}\")\n",
    "\n",
    "# Function to verify data types\n",
    "def check_data_types(df, column_types):\n",
    "    for column, expected_type in column_types.items():\n",
    "        if expected_type == 'numeric':\n",
    "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of numeric type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct numeric type.\")\n",
    "        elif expected_type == 'string':\n",
    "            if not pd.api.types.is_string_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of string type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct string type.\")\n",
    "        elif expected_type == 'date':\n",
    "            try:\n",
    "                pd.to_datetime(df[column])\n",
    "                print(f\"Column {column} is of correct date type.\")\n",
    "            except ValueError:\n",
    "                print(f\"Column {column} contains incorrect date format.\")\n",
    "        elif expected_type == 'boolean':\n",
    "            # Checking if the column is boolean; Assuming boolean is represented as True/False or 1/0\n",
    "            if not pd.api.types.is_bool_dtype(df[column]) and not all(df[column].dropna().isin([0, 1, 'True', 'False', True, False])):\n",
    "                print(f\"Column {column} is not of boolean type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct boolean type.\")\n",
    "\n",
    "# Mandatory columns (not nullable)\n",
    "mandatory_columns = [\n",
    "    'id', 'song_spotify_id', 'song_title', 'song_duration', \n",
    "    'song_album_type', 'song_album_id', 'song_explicit', \n",
    "    'song_popularity', 'song_track_features_added', 'song_date_added_to_db', \n",
    "    'song_date_last_modified'\n",
    "]\n",
    "\n",
    "# Columns with specific data types\n",
    "column_types = {\n",
    "    'id': 'numeric',\n",
    "    'song_spotify_id': 'string',\n",
    "    'song_title': 'string',\n",
    "    'song_duration': 'numeric',\n",
    "    'song_album_type': 'string',\n",
    "    'song_album_id': 'string',\n",
    "    'song_explicit': 'boolean',\n",
    "    'song_popularity': 'numeric',\n",
    "    'song_preview_url': 'string',  # Nullable\n",
    "    'song_track_features_added': 'boolean',\n",
    "    # Assuming 'floatType' corresponds to 'numeric' in Python/Pandas\n",
    "    'song_acousticness': 'numeric',  # Nullable\n",
    "    'song_danceability': 'numeric',  # Nullable\n",
    "    'song_energy': 'numeric',  # Nullable\n",
    "    'song_instrumentalness': 'numeric',  # Nullable\n",
    "    'song_liveness': 'numeric',  # Nullable\n",
    "    'song_loudness': 'numeric',  # Nullable\n",
    "    'song_speechiness': 'numeric',  # Nullable\n",
    "    'song_tempo': 'numeric',  # Nullable\n",
    "    'song_valence': 'numeric',  # Nullable\n",
    "    'song_key': 'numeric',  # Nullable\n",
    "    'song_time_signature': 'numeric',  # Nullable\n",
    "    'song_date_added_to_db': 'date',\n",
    "    'song_date_last_modified': 'date',\n",
    "    'album_id': 'numeric'  # Nullable\n",
    "}\n",
    "\n",
    "# Check for empty values in mandatory columns\n",
    "check_empty_values(df, mandatory_columns)\n",
    "\n",
    "# Check if data types are correct\n",
    "check_data_types(df, column_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adjust these settings as needed\n",
    "pd.set_option('display.max_rows', None)  # This will allow all rows to be displayed\n",
    "pd.set_option('display.max_columns', None)  # This will allow all columns to be displayed\n",
    "pd.set_option('display.width', 1000)  # Adjust the width to accommodate the number of columns\n",
    "pd.set_option('display.max_colwidth', None)  # This ensures that the content of each column is fully displayed\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\tracks_data_full.csv'  # Update this to the path of your CSV file\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "\n",
    "# Function to print rows with empty values in a specific column\n",
    "def print_rows_with_empty_values_in_column(df, column_name):\n",
    "    empty_rows = df[df[column_name].isnull() | (df[column_name] == '')]\n",
    "    if not empty_rows.empty:\n",
    "        print(f\"Rows with empty values in column '{column_name}':\")\n",
    "        # print the song_spotfiy_id for each row with empty values\n",
    "        print(empty_rows['song_spotify_id'])\n",
    "    else:\n",
    "        print(f\"No empty values found in column '{column_name}'.\")\n",
    "\n",
    "# Print all rows where the song_title is empty\n",
    "print_rows_with_empty_values_in_column(df, 'song_title')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load the CSV files\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\src\\\\main\\\\resources\\\\downloaded_files\\\\album_table.csv', delimiter=';', dtype={'id': str, 'album_spotify_id': str})\n",
    "artists_df = pd.read_csv('C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\src\\\\main\\\\resources\\\\downloaded_files\\\\artists_table.csv', delimiter=';', dtype={'id': str, 'artist_spotify_id': str})\n",
    "\n",
    "# Create dictionaries for ID mappings\n",
    "album_id_map = albums_df.set_index('album_spotify_id')['id'].to_dict()\n",
    "artist_id_map = artists_df.set_index('artist_spotify_id')['id'].to_dict()\n",
    "\n",
    "artist_album_mapping = []\n",
    "\n",
    "# Assuming the JSON structure is as shown in your example\n",
    "for root, dirs, files in os.walk('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\ALBUMS'):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for album in data['albums']:\n",
    "                    album_id = album_id_map.get(album['id'])\n",
    "                    if not album_id:\n",
    "                        print(f\"Album ID {album['id']} not found in album CSV.\")\n",
    "                        continue\n",
    "                    for artist in album['artists']:\n",
    "                        artist_id = artist_id_map.get(artist['id'])\n",
    "                        if not artist_id:\n",
    "                            print(f\"Artist ID {artist['id']} not found in artist CSV.\")\n",
    "                            continue\n",
    "                        artist_album_mapping.append({'artistID': artist_id, 'albumID': album_id})\n",
    "\n",
    "# Create DataFrame and remove duplicates\n",
    "artist_album_df = pd.DataFrame(artist_album_mapping).drop_duplicates()\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "artist_album_df.to_csv('artist_album_mappings_new.csv', index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load the CSV files\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\src\\\\main\\\\resources\\\\downloaded_files\\\\album_table.csv', delimiter=';', dtype={'id': str, 'album_spotify_id': str})\n",
    "artists_df = pd.read_csv('C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\src\\\\main\\\\resources\\\\downloaded_files\\\\artists_table.csv', delimiter=';', dtype={'id': str, 'artist_spotify_id': str})\n",
    "\n",
    "# Create dictionaries for ID mappings\n",
    "album_id_map = albums_df.set_index('album_spotify_id')['id'].to_dict()\n",
    "artist_id_map = artists_df.set_index('artist_spotify_id')['id'].to_dict()\n",
    "\n",
    "artist_album_mapping = []\n",
    "missing_artists = {}  # Store missing artist IDs and the album IDs they're attributed to\n",
    "\n",
    "# Assuming the JSON structure is as shown in your example\n",
    "for root, dirs, files in os.walk('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\ALBUMS'):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for album in data['albums']:\n",
    "                    album_id = album_id_map.get(album['id'])\n",
    "                    if not album_id:\n",
    "                        print(f\"Album ID {album['id']} not found in album CSV.\")\n",
    "                        continue\n",
    "                    for artist in album['artists']:\n",
    "                        artist_id = artist_id_map.get(artist['id'])\n",
    "                        if not artist_id:\n",
    "                            # Record the missing artist ID along with the current album ID\n",
    "                            missing_artists.setdefault(artist['id'], []).append(album['id'])\n",
    "                            print(f\"Artist ID {artist['id']} not found in artist CSV, attributed to Album ID {album['id']}.\")\n",
    "                            continue\n",
    "                        artist_album_mapping.append({'artistID': artist_id, 'albumID': album_id})\n",
    "\n",
    "# Display missing artist IDs and the albums they're attributed to\n",
    "for artist_id, album_ids in missing_artists.items():\n",
    "    print(f\"Artist ID {artist_id} (not found) is attributed to Album IDs: {', '.join(album_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the tracks CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\tracks_data_full.csv'\n",
    "df = pd.read_csv(csv_file_path, delimiter=';', dtype=str)\n",
    "\n",
    "# Add a new column `recording_mbid` with empty values (or any default value you prefer)\n",
    "# The column is added at the specific position before `album_id`\n",
    "df.insert(df.columns.get_loc(\"album_id\"), \"recording_mbid\", \"\")\n",
    "\n",
    "# List of song_spotify_id to delete\n",
    "ids_to_delete = [\n",
    "    \"75UvH3TDraHQon52VexYrd\",\n",
    "    \"72QOdqmnjwpx4fJcUhjjLp\",\n",
    "    \"1dF3LpNoeDV25vFEucwqxL\",\n",
    "    \"74T4vgexWpBwybtGx0KbM7\",\n",
    "    \"3JBcNHwpW8WcJMnXLXyrgK\",\n",
    "    \"3LUCdcXCYko2PsbMUCOLZm\",\n",
    "    \"26VIoWSeLRLh8yI3PuWQ12\",\n",
    "    \"2yUpba8YCjUhsvqUszidRA\",\n",
    "    \"2EDuAbx8M1pK0xumWpCPV0\",\n",
    "    \"4CBsZXLknCBL2Yx1oHtEd1\",\n",
    "    \"4FKYbGJ8ZWwqY6BzRGunp3\",\n",
    "    \"74wJH625TY4DsGIUlEU8N2\",\n",
    "    \"5OVSK2zk2PWiWUo1AKNPHb\",\n",
    "    \"4ZsZvWhziGDL4RIslCnTSg\",\n",
    "    \"7xv6Eu2FvAYnhnVZ9oNDy9\",\n",
    "    \"4uxqGutcmbvpdxqploz0nP\",\n",
    "    \"0c2OsDg3NUnbB81Zcdv3hO\",\n",
    "    \"3CWtEob3yRj8kxvmtERogB\",\n",
    "    \"2Lk7JJX3c6guZ7Zz1kXZee\",\n",
    "    \"1Ak80nlobVpr3j1eVhDin2\",\n",
    "    \"2PiZCczBPG6B2ImYpou6Af\",\n",
    "    \"2OvCbJTgPJYh4gndgTOVQu\",\n",
    "    \"7bszJWvC2o9Ux72fWipYJL\",\n",
    "    \"7jNOZY1aOlm3moWVG8WoKi\",\n",
    "    \"5b9nPvneKlpS62phraSwYV\",\n",
    "    \"7zYQM45B7J9NKKxG1if4Wz\",\n",
    "    \"4L264W9L6K78M8QnBXPz03\",\n",
    "    \"456EsNmqInGDaX0qxgfBnX\",\n",
    "    \"3gEhgYimmqodF6BrvlKtar\",\n",
    "    \"1qqxdGz5o6Kfh1ZyvRTZvi\",\n",
    "    \"62LpvA7MNOuDdhIW6Kh9AI\",\n",
    "    \"78Wt8QOMTBHfvn1E57b5Xu\",\n",
    "    \"1TmmI3VPZjMwsqV7kMskwx\",\n",
    "    \"0iW4yp45NhZHmuw4d9uNF6\",\n",
    "    \"47PqaO65CrVnMwH7CfxJvb\",\n",
    "    \"6uepvILocapt6Fg40WJS99\",\n",
    "    \"5MIxwk6y73xTDqGj4hBbkz\",\n",
    "    \"7ahK0JIHV3n7SWja1Fmi4b\",\n",
    "    \"5PdyHsV2Av4ih0jnsguG1m\",\n",
    "    \"0k5jqDnVF8i4GdnDqabodl\",\n",
    "    \"25DzjQxLNatPVUzo0rC7AW\",\n",
    "    \"1zcR9YAcOzZ5WmBBQpZpnY\",\n",
    "    \"2GWruQDCJeeNSaOlV8mgvv\",\n",
    "    \"0xUMokwO3lo0YkdAfQySjT\",\n",
    "    \"3wcDi6kWtAetFydVN4Bbrx\",\n",
    "    \"7l67g6KzQsFBDPSKvyPsAy\",\n",
    "    \"0dsTq154TF7mCMg18lpI58\",\n",
    "    \"5gSiQViEHnsmGuQ5Tibvs5\",\n",
    "    \"6zMpg17rh0XftHphBJJmmd\",\n",
    "    \"2njlNRtphrdtdHREmrEqR3\",\n",
    "    \"3NXYmpj0lp5427XbKbUcfC\",\n",
    "    \"6JbXjioNWKx6XjBcPleJ72\",\n",
    "    \"2JPJpFF275xAB8H95o8ueG\",\n",
    "    \"2QigB5AHMSzX6X82ijLosf\",\n",
    "    \"67TrkSnOD9Nui8m95Qe5q2\",\n",
    "    \"2wT3CYlhPHfj4eh2X06TbX\",\n",
    "    \"2V2TFejf9JlzXVpHIr6jSt\",\n",
    "    \"2trNJPmMIgDeowTdZRRP3F\",\n",
    "    \"12Xx9dJ5oHW0MwyzYOtur7\",\n",
    "    \"4M6XTI8Uk7hMAs2UdEkwRl\",\n",
    "    \"33J6EyMfqdhamFdTuAoJX7\",\n",
    "    \"1UL8YBCLVVnDzmZeoV5ttL\",\n",
    "    \"3Zi0Mr2fWJsf41fTvQlH9U\",\n",
    "    \"3AdrYdf4H4z0AT93tGmKEf\",\n",
    "    \"0tpdtfsYH4GL2iYWP6XujW\",\n",
    "    \"5EbuaL6OikuBKntP1rOeY1\",\n",
    "    \"54OE3k8dzodj9Dkvzq6hOD\",\n",
    "    \"5wQHfM4ImDZAfb20sZED1Y\",\n",
    "    \"5IW9oM2x5mNfReN28qNDAo\",\n",
    "    \"2xhMF0vhmuI2utvwpbLoLU\",\n",
    "    \"64cRwHXDTKmcwcBgG8jKcP\",\n",
    "    \"0JCfef9cI9aHjUuV542gAp\",\n",
    "    \"0jF41wuIItYGsK0xb72h6e\",\n",
    "    \"3b30Jt6Jv0AVOGy7knAyEd\",\n",
    "    \"4NFtHPM5C74PE4me5EAzjf\",\n",
    "    \"2fp46neVxgQdC9WYfskdU8\",\n",
    "    \"47mweMIEiVG1VTURjooVdU\",\n",
    "    \"0mGGQ9BKBsGVZURwtC1ri6\",\n",
    "    \"4RLGpC5pvisS0py1wJ8OF4\",\n",
    "    \"2b42gCsSo7Kwg1MgOTXlv8\",\n",
    "    \"3HihOJceOThzpsbAFCLq9p\",\n",
    "    \"4fguT5jLuyS6aIszCjkoUW\",\n",
    "    \"2XlGPrt6URuTsuwXCqQoXp\",\n",
    "    \"3qgvtszdPCoKBj02aflFjM\",\n",
    "    \"5nGitkFcddduRwEFCQAmaY\",\n",
    "    \"0a99u1uV6YsgoZJ0ArL8YG\",\n",
    "    \"17fEyUuqKTEHgh6eGijMNd\",\n",
    "    \"5YRyBSPSXi1kha3p4jnW30\",\n",
    "    \"67HIfSjDNBiu4L4KHe4Iwu\",\n",
    "    \"5kax80HlqMnyxhxxzvrHIg\",\n",
    "    \"3bkrlUkcpVUaFfoF8aUsMG\",\n",
    "    \"7H12MyuGjh83lr30LZDemx\",\n",
    "    \"7Gv17UJTle9X61IPSAx60k\",\n",
    "    \"3ymrE7n8eJiXEOdFJ2lplT\",\n",
    "    \"7pt1yDbKBo3M0cwVeVSqYp\",\n",
    "    \"7EgwllELTl4RMOITMuhIa3\",\n",
    "    \"65qooAhuNUPKG3AYNQx3hQ\",\n",
    "    \"3iIXnGdigJTNZRkUbNVZVQ\",\n",
    "    \"72HJ0dXhrCp2h1tUCUbwP6\",\n",
    "    \"1Oo1O7C2yLwKx7YF7EjaJY\",\n",
    "    \"45Q19MuCEgo3ebzcmmQJeA\",\n",
    "    \"5lwCBdknMtWs4OKlYEdb5w\",\n",
    "    \"1Gnmu7Li3aJ1IAh3K5V2Fl\",\n",
    "    \"60224pftm64rJrayNN2MTA\",\n",
    "    \"4siKkiCT4VP6InhBOHV4vP\",\n",
    "    \"4hWiD8q9m4gs9QlqydcEv7\",\n",
    "    \"5yyG2O9ABwlEuBunYWmWT0\",\n",
    "    \"4NyRcqUXDCRxzWqZmZxyWl\",\n",
    "    \"6xIkjw9xcd8s60dLUtAxM6\",\n",
    "    \"0fhZjHAfWzfwfkMW9Zdpga\",\n",
    "    \"4doiaWc4GbuOpIT9GeVwub\",\n",
    "    \"4lp6ZxoXb6BU3emooKHcNe\",\n",
    "    \"0U7ruXdI2rjw14XB0yabUc\",\n",
    "    \"2xJuT0ZJz0WoIYGMKXG01t\",\n",
    "    \"4k86QF2GSQh7wGVb2CWzSL\",\n",
    "    \"0RskFK29CBwj1GpkpqaYEG\",\n",
    "    \"1dxUsz7reRhvjAfhja9w0d\",\n",
    "    \"2vN2OF6zeAIYmGBjId0g5r\",\n",
    "    \"4bZ6YhwSfSb3kLtR8F0R5P\",\n",
    "    \"4KKMeWvjZqvXRnPpE83GrC\"\n",
    "]\n",
    "# Delete rows with the specified song_spotify_id\n",
    "df = df[~df['song_spotify_id'].isin(ids_to_delete)]\n",
    "\n",
    "# Save the updated DataFrame back to CSV\n",
    "df.to_csv('updated_tracks_data_full.csv', index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the tracks and album CSVs into DataFrames\n",
    "tracks_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\tracks_data_full.csv', delimiter=';', dtype=str)\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_data_full.csv', delimiter=';', dtype=str)\n",
    "\n",
    "# Create a mapping dictionary from the album CSV: Spotify ID -> album_id\n",
    "spotify_id_to_album_id = dict(zip(albums_df['album_spotify_id'], albums_df['id']))\n",
    "\n",
    "# Use the map to update the album_id in the tracks DataFrame\n",
    "# The `song_album_id` column contains the Spotify IDs that will be used for mapping\n",
    "tracks_df['album_id'] = tracks_df['song_album_id'].map(spotify_id_to_album_id)\n",
    "\n",
    "# Save the updated DataFrame back to CSV\n",
    "tracks_df.to_csv('updated_tracks_data_full.csv', index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the album CSV into a DataFrame\n",
    "album_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_data_full.csv', delimiter=';', dtype=str)\n",
    "\n",
    "# Drop the 'URL' column\n",
    "album_df = album_df.drop(columns=['URL'])\n",
    "\n",
    "# Save the updated DataFrame back to CSV\n",
    "album_df.to_csv('updated_album_data_full.csv', index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\src\\\\main\\\\resources\\\\config\\\\liquibase\\\\fake-data\\\\artists_table_new_notworking.csv', sep=';')\n",
    "\n",
    "# Define a function to convert date format from DD/MM/YYYY to DD-MM-YYYY\n",
    "def convert_date_format(date_str):\n",
    "    return pd.to_datetime(date_str, dayfirst=True).strftime('%d-%m-%Y')\n",
    "\n",
    "# Apply the function to the 'date_added_to_db' and 'date_last_modified' columns\n",
    "df['date_added_to_db'] = df['date_added_to_db'].apply(convert_date_format)\n",
    "df['date_last_modified'] = df['date_last_modified'].apply(convert_date_format)\n",
    "\n",
    "# Convert artist_followers from float to int by first filling NaN values to avoid errors\n",
    "df['artist_followers'] = df['artist_followers'].fillna(0).astype(int)\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file\n",
    "df.to_csv('modified_file.csv', sep=';', index=False)\n",
    "\n",
    "# Display the head of the modified DataFrame as a sanity check\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def convert_to_iso_format(date_str):\n",
    "    \"\"\"\n",
    "    Convert dates from DD-MM-YYYY to YYYY-MM-DD format.\n",
    "    \"\"\"\n",
    "    return pd.to_datetime(date_str, dayfirst=True).strftime('%Y-%m-%d')\n",
    "\n",
    "# Load your CSV file\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\src\\\\main\\\\resources\\\\config\\\\liquibase\\\\fake-data\\\\artists_table_new_notworking.csv', sep=';')\n",
    "\n",
    "\n",
    "# Convert 'date_added_to_db' and 'date_last_modified' columns to the ISO 8601 date format (YYYY-MM-DD)\n",
    "df['date_added_to_db'] = df['date_added_to_db'].apply(convert_to_iso_format)\n",
    "df['date_last_modified'] = df['date_last_modified'].apply(convert_to_iso_format)\n",
    "\n",
    "# Convert 'artist_followers' from float to int, handling NaN values by converting them to 0\n",
    "df['artist_followers'] = df['artist_followers'].fillna(0).astype(int)\n",
    "\n",
    "# Save the modified DataFrame back to a new CSV file\n",
    "modified_csv_file_path = 'modified_file.csv'  # You can change this to your desired output file path\n",
    "df.to_csv(modified_csv_file_path, sep=';', index=False)\n",
    "\n",
    "print(\"File has been modified and saved as:\", modified_csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\src\\\\main\\\\resources\\\\downloaded_files\\\\album_table.csv'\n",
    "# Path for the output CSV file with rows having null album_release_date\n",
    "output_csv_file_path = 'null_release_dates.csv'\n",
    "\n",
    "def check_null_release_dates(csv_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path, sep=';')\n",
    "\n",
    "    # Filter rows where album_release_date is null or empty\n",
    "    null_release_dates = df[df['album_release_date'].isnull() | (df['album_release_date'] == '')]\n",
    "\n",
    "    # Check if there are any such rows\n",
    "    if not null_release_dates.empty:\n",
    "        print(\"Rows with null or empty album_release_date:\")\n",
    "        print(null_release_dates)\n",
    "\n",
    "        # Optional: Save these rows to a new CSV file\n",
    "        null_release_dates.to_csv(output_csv_file_path, sep=';', index=False)\n",
    "        print(f\"Saved rows with null or empty album_release_date to {output_csv_file_path}\")\n",
    "    else:\n",
    "        print(\"No rows with null or empty album_release_date found.\")\n",
    "\n",
    "# Call the function with your CSV file path\n",
    "check_null_release_dates(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\src\\\\main\\\\resources\\\\downloaded_files\\\\album_table.csv'\n",
    "# Path for the output CSV file with adjusted release dates\n",
    "output_csv_file_path = 'adjusted_release_dates.csv'\n",
    "\n",
    "def adjust_release_date(row):\n",
    "    # Check the release_date_precision and adjust album_release_date accordingly\n",
    "    if row['release_date_precision'] == 'year':\n",
    "        # For year precision, add \"-01-01\"\n",
    "        return f\"{row['album_release_date']}-01-01\"\n",
    "    elif row['release_date_precision'] == 'month':\n",
    "        # For month precision, add \"-01\"\n",
    "        return f\"{row['album_release_date']}-01\"\n",
    "    else:\n",
    "        # If it's already precise, or another value, leave it as is\n",
    "        return row['album_release_date']\n",
    "\n",
    "def process_csv_file(csv_path, output_path):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_path, sep=';')\n",
    "    \n",
    "    # Apply the adjustment function to each row\n",
    "    df['album_release_date'] = df.apply(adjust_release_date, axis=1)\n",
    "    \n",
    "    # Save the modified DataFrame to a new CSV file\n",
    "    df.to_csv(output_path, sep=';', index=False)\n",
    "    \n",
    "    print(f\"Processed file saved to: {output_path}\")\n",
    "\n",
    "# Call the function with your CSV file path\n",
    "process_csv_file(csv_file_path, output_csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Path to the large CSV file\n",
    "input_csv_path = 'C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\src\\\\main\\\\resources\\\\downloaded_files\\\\tracks_data_full_removed.csv'\n",
    "\n",
    "# Output directory for the smaller CSV files\n",
    "output_dir = 'split_csv_files'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Separator character used in the CSV file\n",
    "separator = ';'\n",
    "\n",
    "# Load the large CSV file\n",
    "df = pd.read_csv(input_csv_path, sep=separator)\n",
    "\n",
    "# Number of rows in each split file\n",
    "rows_per_file = 400000\n",
    "\n",
    "# Total number of rows in the DataFrame\n",
    "total_rows = len(df)\n",
    "\n",
    "# Calculate the number of files needed\n",
    "num_files = math.ceil(total_rows / rows_per_file)\n",
    "\n",
    "# Split and save the DataFrame into smaller CSV files\n",
    "for i in range(num_files):\n",
    "    start_row = i * rows_per_file\n",
    "    end_row = min((i + 1) * rows_per_file, total_rows)\n",
    "    df_subset = df.iloc[start_row:end_row]\n",
    "    output_file_path = os.path.join(output_dir, f'split_file_{i + 1}.csv')\n",
    "    df_subset.to_csv(output_file_path, sep=separator, index=False)\n",
    "\n",
    "print(f\"Split completed. {num_files} files were created in '{output_dir}' directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files into Pandas DataFrames\n",
    "songs_df = pd.read_csv('C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\src\\\\main\\\\resources\\\\downloaded_files\\\\tracks_data_full_removed.csv', sep=';')\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\src\\\\main\\\\resources\\\\downloaded_files\\\\album_table.csv', sep=';')\n",
    "\n",
    "# Extract the album_id columns from both DataFrames\n",
    "song_album_ids = songs_df['album_id'].unique()\n",
    "album_ids = albums_df['id'].unique()\n",
    "\n",
    "# Find album_ids in the song CSV that are not in the album CSV\n",
    "missing_album_ids = set(song_album_ids) - set(album_ids)\n",
    "\n",
    "# Initialize an empty list to hold the IDs of songs with missing album_ids\n",
    "missing_songs = []\n",
    "\n",
    "# Check each song to see if its album_id is missing in the album CSV\n",
    "for index, row in songs_df.iterrows():\n",
    "    if row['album_id'] in missing_album_ids:\n",
    "        missing_songs.append(row['id'])\n",
    "\n",
    "if missing_songs:\n",
    "    print(\"The following song IDs reference album_ids that are missing in the albums CSV:\")\n",
    "    print(missing_songs)\n",
    "else:\n",
    "    print(\"All songs in the songs CSV have corresponding album_ids in the albums CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the original tracks CSV file\n",
    "input_csv_path = 'C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\src\\\\main\\\\resources\\\\downloaded_files\\\\tracks_data_full.csv'\n",
    "# Path for the output CSV file without the specified songs\n",
    "output_csv_path = 'filtered_tracks.csv'\n",
    "\n",
    "# List of song IDs to remove\n",
    "song_ids_to_remove = [\n",
    "    62034, 88913, 112688, 300187, 316102, 329999, 367946, 375943, 386729, 401892,\n",
    "    435595, 436765, 447794, 447852, 608310, 711292, 790732, 883084, 920213, 1018358,\n",
    "    1114764, 1127693, 1142259, 1168089, 1187599, 1265380, 1320394, 1402381, 1413819,\n",
    "    1512351, 1563201, 1632407, 1681835, 1730948, 1770887, 1791508, 1808512, 1851036,\n",
    "    1889088, 1941659, 1949441, 1982776, 2034044, 2109511, 2111128, 2147642, 2194426,\n",
    "    2201476, 2211371, 2320285, 2327313, 2361529, 2363631, 2406470, 2437122, 2505740,\n",
    "    2513991, 2624535, 2638046, 2772894, 2777596, 2789005, 2818861, 2819844, 2883260,\n",
    "    2907509, 2917805, 2954339, 3170871, 3329632, 3371496, 3460982, 3496147, 3625644,\n",
    "    3628044, 3650231, 3658670, 3673094, 3701507\n",
    "]\n",
    "\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(input_csv_path, sep=';')\n",
    "\n",
    "# Filter the DataFrame to exclude rows with the specified song IDs\n",
    "filtered_df = df[~df['id'].isin(song_ids_to_remove)]\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "filtered_df.to_csv(output_csv_path, sep=';', index=False)\n",
    "\n",
    "print(f\"Filtered CSV saved to '{output_csv_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\WORKING\\\\spotify_musicbrainz_matches_with_fuzzy_album_only.csv'\n",
    "# Path for the output CSV file\n",
    "output_csv_path = 'filtered_no_duplicates.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Sort the DataFrame by spotify_id and match_score in descending order\n",
    "df_sorted = df.sort_values(by=['spotify_id', 'match_score'], ascending=[True, False])\n",
    "\n",
    "# Drop duplicates, keeping the first occurrence (which has the highest match_score due to the sort)\n",
    "df_filtered = df_sorted.drop_duplicates(subset='spotify_id', keep='first')\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "df_filtered.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Filtered CSV without duplicates saved to '{output_csv_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to your CSV files\n",
    "csv_file_path_1 = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\WORKING\\\\spotify_musicbrainz_matches_filtered_no_duplicates.csv'  # Update this to the path of your first CSV\n",
    "csv_file_path_2 = 'C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\src\\\\main\\\\resources\\\\downloaded_files\\\\artists_table.csv'  # Update this to the path of your second CSV\n",
    "\n",
    "# Output path for the merged CSV file\n",
    "output_csv_path = 'merged_csv_with_musicbrainz_id.csv'\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "df1 = pd.read_csv(csv_file_path_1)\n",
    "df2 = pd.read_csv(csv_file_path_2, sep=';')\n",
    "\n",
    "# We only need the 'spotify_id' and 'musicbrainz_id' columns from the first DataFrame\n",
    "df1_relevant = df1[['spotify_id', 'musicbrainz_id']]\n",
    "\n",
    "# Rename columns in df1_relevant for consistency with df2\n",
    "df1_relevant.rename(columns={'spotify_id': 'artist_spotify_id'}, inplace=True)\n",
    "\n",
    "# Merge df2 with df1_relevant to add the musicbrainz_id based on artist_spotify_id\n",
    "# Perform a left join to keep all rows from df2 and only add musicbrainz_id where matches are found\n",
    "merged_df = pd.merge(df2, df1_relevant, on='artist_spotify_id', how='left')\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv(output_csv_path, sep=';', index=False)\n",
    "\n",
    "print(f\"Merged CSV saved to '{output_csv_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\src\\\\main\\\\resources\\\\downloaded_files\\\\artists_table.csv'  # Update this to the path of your CSV file\n",
    "df = pd.read_csv(csv_file_path, sep=';')\n",
    "\n",
    "# Merge the musicbrainz_id_x and musicbrainz_id_y columns\n",
    "# Assuming we prioritize non-empty values in musicbrainz_id_x\n",
    "df['musicbrainz_id'] = df['musicbrainz_id_x'].where(df['musicbrainz_id_x'].notna(), df['musicbrainz_id_y'])\n",
    "\n",
    "# Drop the original musicbrainz_id_x and musicbrainz_id_y columns\n",
    "df.drop(['musicbrainz_id_x', 'musicbrainz_id_y'], axis=1, inplace=True)\n",
    "\n",
    "# Save the updated DataFrame back to a CSV file\n",
    "output_csv_path = 'updated_csv_file.csv'  # Update this to your desired output file path\n",
    "df.to_csv(output_csv_path, sep=';', index=False)\n",
    "\n",
    "print(f\"Updated CSV saved to '{output_csv_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\src\\\\main\\\\resources\\\\downloaded_files\\\\artists_table.csv'  # Update this to the path of your CSV file\n",
    "df = pd.read_csv(csv_file_path, sep=';')\n",
    "\n",
    "# Find duplicates based on the 'id' column\n",
    "duplicates = df[df.duplicated('id', keep=False)]  # keep=False marks all duplicates as True\n",
    "\n",
    "# Check if there are any duplicates\n",
    "if not duplicates.empty:\n",
    "    print(\"Duplicate entries based on 'id':\")\n",
    "    print(duplicates)\n",
    "    # Optional: Save the duplicates to a new CSV file for further investigation\n",
    "    duplicates.to_csv('duplicates_in_csv.csv', sep=';', index=False)\n",
    "    print(\"Duplicates saved to 'duplicates_in_csv.csv'.\")\n",
    "else:\n",
    "    print(\"No duplicate 'id' entries found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbids_to_remove = [\"ee58c59f-8e7f-4430-b8ca-236c4d3745ae\",\n",
    "\"614ee114-7d22-4d35-bfa4-6b9ee8292766\",\n",
    "\"18bb9c95-5cd1-4aaf-8069-1a216ac7dde4\",\n",
    "\"569fc5bf-bcb6-4bc5-9a00-bd0d258e34f1\",\n",
    "\"55501cc6-c291-4456-a86a-1ef61019ac95\",\n",
    "\"e2459c47-a682-405f-bd20-0105b9a7f50a\",\n",
    "\"a40b46f7-7b40-4a28-9dec-a1e3cb440a8b\",\n",
    "\"379fb100-063e-4472-b7c2-5bb9eda435ed\",\n",
    "\"80cadd99-f560-41e3-babd-16292bbd248a\",\n",
    "\"9953ebf6-1836-45ef-bcca-bee2e32e29c6\",\n",
    "\"67a71fbe-1a41-4c6d-93ab-09c000ab4aa7\",\n",
    "\"e2b34653-c813-4c07-93ad-674bc58e4696\",\n",
    "\"b97a49d7-e8b9-406d-9e79-2720004e2806\",\n",
    "\"11cb567e-ecac-4226-8944-f862440e8bca\",\n",
    "\"408b2a33-d229-46a5-a361-0667bc616e34\",\n",
    "\"807dcde7-5deb-4c18-940b-ca3b0dc0e469\",\n",
    "\"ed2ac1e9-d51d-4eff-a2c2-85e81abd6360\",\n",
    "\"66a3f091-c5a7-4ead-8fd8-1adf4ddccd78\",\n",
    "\"7b6c5ade-d762-4456-804b-8321f93a8307\",\n",
    "\"ac940ef6-5d4b-4da3-8c3a-245954c299b9\",\n",
    "\"21fc9320-c4dd-4b92-9963-927527b4c59e\",\n",
    "\"467a0aba-7e76-4080-9643-b88f82452d66\",\n",
    "\"5fdae5d2-8aff-4671-8cf8-3b216f63265f\",\n",
    "\"7cbdfc61-30a9-4f17-b2fc-dcae3757adba\",\n",
    "\"e5750671-59c8-447b-8511-fd29d85596b6\",\n",
    "\"a9443494-9d85-4ff5-b53f-da6d912de210\",\n",
    "\"6d1d17dc-256b-4ebf-8cad-db2abb60b1e9\",\n",
    "\"85e260f5-a1b4-4bb1-bb8a-7d0c8db0588b\",\n",
    "\"1dce6682-6db5-495a-afbf-c8566d3fd084\",\n",
    "\"9f0a5371-5a2b-49bc-9083-0c93d4571a76\",\n",
    "\"714f3d1a-6e6a-46a2-beda-5bcbc37e9a13\",\n",
    "\"bf28e2d7-ba14-4578-afb9-e14f92153bad\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\src\\\\main\\\\resources\\\\downloaded_files\\\\artists_table.csv'  # Update with the path to your CSV file\n",
    "df = pd.read_csv(csv_file_path, sep=';')\n",
    "\n",
    "# Filter the DataFrame to remove rows with MBIDs in the mbids_to_remove list\n",
    "filtered_df = df[~df['musicbrainz_id'].isin(mbids_to_remove)]\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "output_csv_path = 'cleaned_csv_file.csv'  # Update with your desired output path\n",
    "filtered_df.to_csv(output_csv_path, sep=';', index=False)\n",
    "\n",
    "print(\"Filtered CSV saved to:\", output_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "S2TENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
