{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# The directory containing your folders with JSON files\n",
    "root_directory = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\track_information'\n",
    "\n",
    "# The path for the output CSV file\n",
    "csv_file_path = 'tracks_data.csv'\n",
    "\n",
    "# Current date for 'Date Added To DB' and 'Date Last Modified' columns in the YYYY-MM-DD format\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Template for CSV rows with the updated header titles\n",
    "csv_columns = [\n",
    "    'id', 'song_spotify_id', 'song_title', 'song_duration', 'song_album_type', \n",
    "    'song_album_id', 'song_explicit', 'song_popularity', 'song_preview_url', \n",
    "    'song_track_features_added', 'song_acousticness', 'song_danceability', 'song_energy', \n",
    "    'song_instrumentalness', 'song_liveness', 'song_loudness', 'song_speechiness', \n",
    "    'song_tempo', 'song_valence', 'song_key', 'song_time_signature', 'song_date_added_to_db', \n",
    "    'song_date_last_modified'\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store rows for the CSV\n",
    "csv_rows = []\n",
    "\n",
    "def process_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for track in data.get('tracks', []):\n",
    "            # Skip processing if the track is None\n",
    "            if track is None:\n",
    "                continue\n",
    "\n",
    "            row = {\n",
    "                'id': -1,  # This will be updated later with the actual ID\n",
    "                'song_spotify_id': track['id'],\n",
    "                'song_title': track['name'],\n",
    "                'song_duration': track['duration_ms'],\n",
    "                'song_album_type': track['album']['album_type'].upper(),\n",
    "                'song_album_id': track['album']['id'],\n",
    "                'song_explicit': track['explicit'],\n",
    "                'song_popularity': track['popularity'],\n",
    "                'song_preview_url': track.get('preview_url', ''),\n",
    "                'song_track_features_added': False,\n",
    "                'song_acousticness': -1,\n",
    "                'song_danceability': -1,\n",
    "                'song_energy': -1,\n",
    "                'song_instrumentalness': -1,\n",
    "                'song_liveness': -1,\n",
    "                'song_loudness': -1,\n",
    "                'song_speechiness': -1,\n",
    "                'song_tempo': -1,\n",
    "                'song_valence': -1,\n",
    "                'song_key': -1,\n",
    "                'song_time_signature': -1,\n",
    "                'song_date_added_to_db': current_date,\n",
    "                'song_date_last_modified': current_date\n",
    "            }\n",
    "            csv_rows.append(row)\n",
    "\n",
    "# Iterate over each subfolder and JSON file in the directory\n",
    "for subdir, dirs, files in os.walk(root_directory):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.json'):\n",
    "            print(f\"Processing {filename}...\")\n",
    "            process_json_file(os.path.join(subdir, filename))\n",
    "\n",
    "# Write the CSV file\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=csv_columns, delimiter=';')\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Update each row with its ID before writing\n",
    "    for i, row in enumerate(csv_rows, start=1):\n",
    "        row['id'] = i\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"CSV file has been successfully created at {csv_file_path} with {len(csv_rows)} tracks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the root directory containing your JSON files\n",
    "root_directory = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\ALBUMS'\n",
    "\n",
    "# Define the output CSV file path\n",
    "csv_file_path = 'album_data.csv'\n",
    "\n",
    "# Current date for 'Date Added To DB' and 'Date Last Modified' columns\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# CSV column headers\n",
    "csv_columns = [\n",
    "    'id', 'album_spotify_id', 'album_name', 'album_cover_art', 'album_release_date',\n",
    "    'release_date_precision', 'album_popularity', 'album_type', 'spotify_album_upc',\n",
    "    'spotify_album_ean', 'spotify_album_isrc', 'date_added_to_db', 'date_last_modified',\n",
    "    'musicbrainz_metadata_added', 'musicbrainz_id'\n",
    "]\n",
    "\n",
    "# Initialize a list to hold album data\n",
    "albums_data = []\n",
    "\n",
    "# Function to process each JSON file\n",
    "def process_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for album in data.get('albums', []):\n",
    "            # Extract the required information, with checks for nullable fields\n",
    "            album_data = {\n",
    "                'id': -1,  # Placeholder, will be updated later\n",
    "                'album_spotify_id': album['id'],\n",
    "                'album_name': album['name'],\n",
    "                'album_cover_art': album['images'][0]['url'] if album.get('images') else '',\n",
    "                'album_release_date': album['release_date'],\n",
    "                'release_date_precision': album['release_date_precision'],\n",
    "                'album_popularity': album['popularity'],\n",
    "                'album_type': album['album_type'],\n",
    "                'spotify_album_upc': album['external_ids'].get('upc', '') if album.get('external_ids') else '',\n",
    "                'spotify_album_ean': album['external_ids'].get('ean', '') if album.get('external_ids') else '',\n",
    "                'spotify_album_isrc': album['external_ids'].get('isrc', '') if album.get('external_ids') else '',\n",
    "                'date_added_to_db': current_date,\n",
    "                'date_last_modified': current_date,\n",
    "                'musicbrainz_metadata_added': False,  # Placeholder\n",
    "                'musicbrainz_id': ''  # Placeholder\n",
    "            }\n",
    "            albums_data.append(album_data)\n",
    "\n",
    "# Process each JSON file in the directory and subdirectories\n",
    "for subdir, dirs, files in os.walk(root_directory):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.json'):\n",
    "            process_json_file(os.path.join(subdir, filename))\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=csv_columns, delimiter=';')\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Update each row with its actual ID before writing\n",
    "    for i, album_data in enumerate(albums_data, start=1):\n",
    "        album_data['id'] = i\n",
    "        writer.writerow(album_data)\n",
    "\n",
    "print(f\"CSV file has been successfully created at {csv_file_path} with {len(albums_data)} albums.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the dtypes for the IDs to be strings when reading the CSVs\n",
    "dtype_dict = {'id': str, 'song_album_id': str, 'album_spotify_id': str}\n",
    "tracks_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\tracks_data.csv', delimiter=';', dtype=dtype_dict)\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\album_data.csv', delimiter=';', dtype=dtype_dict)\n",
    "\n",
    "# Create a dictionary mapping from album_spotify_id to id from albums_df\n",
    "# Ensure the 'id' column in albums_df is converted to integer if it's not NaN\n",
    "album_id_map = albums_df.dropna(subset=['id']).set_index('album_spotify_id')['id'].astype(int).to_dict()\n",
    "\n",
    "# Map the song_album_id in tracks_df using the album_id_map to get the album id\n",
    "tracks_df['album_id'] = tracks_df['song_album_id'].map(album_id_map)\n",
    "\n",
    "# Convert the new album_id column to integers, NaNs will be converted to a float with a .0\n",
    "tracks_df['album_id'] = tracks_df['album_id'].fillna(-1).astype(int)\n",
    "\n",
    "# Replace -1 back to NaN if you want to keep NaN values\n",
    "tracks_df['album_id'].replace(-1, pd.NA, inplace=True)\n",
    "\n",
    "# Save the updated tracks DataFrame to a new CSV file\n",
    "tracks_df.to_csv('path_to_updated_tracks.csv', index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Read the albums and artists CSVs into DataFrames\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_data_full.csv', delimiter=';', dtype={'id': str, 'album_spotify_id': str})\n",
    "artists_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\artists_data_full.csv', delimiter=';', dtype={'id': str, 'artist_spotify_id': str})\n",
    "\n",
    "# Dictionary to map Spotify album ID to CSV album ID\n",
    "album_id_map = albums_df.set_index('album_spotify_id')['id'].to_dict()\n",
    "\n",
    "# Dictionary to map Spotify artist ID to CSV artist ID\n",
    "artist_id_map = artists_df.set_index('artist_spotify_id')['id'].to_dict()\n",
    "\n",
    "# Initialize a list to hold the artist-album mappings\n",
    "artist_album_mapping = []\n",
    "\n",
    "# Assuming 'path_to_json_folder' is the folder containing all the JSON subfolders\n",
    "for root, dirs, files in os.walk('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\ALBUMS'):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for album in data['albums']:\n",
    "                    album_id = album_id_map.get(album['id'])\n",
    "                    if album_id:\n",
    "                        for artist in album['artists']:\n",
    "                            artist_id = artist_id_map.get(artist['id'])\n",
    "                            if artist_id:\n",
    "                                artist_album_mapping.append({'artistID': artist_id, 'albumID': album_id})\n",
    "\n",
    "# Create a DataFrame from the artist-album mappings\n",
    "artist_album_df = pd.DataFrame(artist_album_mapping)\n",
    "\n",
    "# Remove duplicates if there are any\n",
    "artist_album_df = artist_album_df.drop_duplicates()\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "artist_album_df.to_csv('artist_album_mappings_new.csv', index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Read the albums and artists CSVs into DataFrames\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\album_data.csv', delimiter=';', dtype={'id': str, 'album_spotify_id': str})\n",
    "artists_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\artists_data_full.csv', delimiter=';', dtype={'id': str, 'artist_spotify_id': str})\n",
    "\n",
    "# Dictionary to map Spotify album ID to CSV album ID\n",
    "album_id_map = albums_df.set_index('album_spotify_id')['id'].to_dict()\n",
    "\n",
    "# Dictionary to map Spotify artist ID to CSV artist ID\n",
    "artist_id_map = artists_df.set_index('artist_spotify_id')['id'].to_dict()\n",
    "\n",
    "# Initialize a list to hold the artist-album mappings\n",
    "artist_album_mapping = []\n",
    "\n",
    "# List to hold artist Spotify IDs where CSV artist ID was not found\n",
    "missing_artist_ids = []\n",
    "\n",
    "# Assuming 'path_to_json_folder' is the folder containing all the JSON subfolders\n",
    "for root, dirs, files in os.walk('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\ALBUMS'):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for album in data['albums']:\n",
    "                    album_id = album_id_map.get(album['id'])\n",
    "                    if album_id:\n",
    "                        for artist in album['artists']:\n",
    "                            artist_spotify_id = artist['id']\n",
    "                            artist_id = artist_id_map.get(artist_spotify_id)\n",
    "                            if artist_id:\n",
    "                                artist_album_mapping.append({'artistID': artist_id, 'albumID': album_id})\n",
    "                            else:\n",
    "                                missing_artist_ids.append(artist_spotify_id)\n",
    "\n",
    "# Print the list of artist Spotify IDs where the CSV artist ID was not found\n",
    "print(\"List of artist Spotify IDs where the CSV artist ID was not found:\")\n",
    "for artist_id in missing_artist_ids:\n",
    "    print(artist_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "missing_artist_ids_df = pd.DataFrame(missing_artist_ids, columns=['missing_artist_spotify_id'])\n",
    "\n",
    "# Define the file path where you want to save the CSV\n",
    "file_path = 'missing_artist_ids.csv'  # You can specify your own path\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "missing_artist_ids_df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f'The missing artist IDs have been saved to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'input_file.csv' with the path to your input CSV file\n",
    "input_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\musicBrainzAllContributors.csv'\n",
    "# Replace 'output_file.csv' with the path where you want to save the output CSV file\n",
    "output_file_path = 'contributor_data_prelink.csv'\n",
    "\n",
    "# Step 1: Read the input CSV\n",
    "input_df = pd.read_csv(input_file_path, delimiter=',')\n",
    "\n",
    "# Step 2: Create the new DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    'ID': range(1, len(input_df) + 1),\n",
    "    'NAME': input_df['artist_credit_name'],\n",
    "    'ROLE': input_df['role'],\n",
    "    'INSTRUMENT': input_df['instrument'],\n",
    "    'MUSICBRAINZ_ID': input_df['artist_mbid'],\n",
    "    'MAINARTIST': input_df['artist_credit_name'],\n",
    "    'SONGTITLE': input_df['recording_name']\n",
    "})\n",
    "\n",
    "# Step 3: Write the resulting DataFrame to a new CSV file\n",
    "output_df.to_csv(output_file_path, sep=';', index=False)\n",
    "\n",
    "print(f\"Output CSV saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Load the CSV files\n",
    "musicbrainz_df = pd.read_csv('C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\prepopulationStuff\\\\PythonNotebooksForPrepopulation\\\\contributor_data_prelink.csv', sep=';')  # The file generated from the previous step\n",
    "spotify_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\tracks_data.csv', sep=';')  # The Spotify songs CSV\n",
    "\n",
    "# Create a list of combined song title and artist names from Spotify for matching\n",
    "spotify_combined_list = spotify_df.apply(lambda x: f\"{x['song_title']} {x['song_album_id']}\", axis=1).tolist()\n",
    "\n",
    "def find_best_match(mb_row, spotify_combined_list):\n",
    "    mb_combined = f\"{mb_row['SONGTITLE']} {mb_row['MAINARTIST']}\"\n",
    "    \n",
    "    # Using extractOne to find the best match from the list\n",
    "    best_match_info = process.extractOne(mb_combined, spotify_combined_list)\n",
    "    \n",
    "    # If there is a match found, extract it\n",
    "    if best_match_info:\n",
    "        best_match_text, best_score = best_match_info\n",
    "        # Find the index of the match in Spotify list to retrieve the full row from spotify_df\n",
    "        match_index = spotify_combined_list.index(best_match_text)\n",
    "        best_match_row = spotify_df.iloc[match_index]\n",
    "        return best_match_row, best_score\n",
    "    return None, 0\n",
    "\n",
    "# Prepare the output DataFrame\n",
    "matched_df = pd.DataFrame(columns=['CONTRIBUTOR_ID', 'SONG_TABLE_ID', 'Spotify_Song_Title', 'Spotify_Artist', 'MusicBrainz_Song_Title', 'MusicBrainz_Artist'])\n",
    "\n",
    "# Iterate over MusicBrainz entries to find matches\n",
    "for index, mb_row in musicbrainz_df.iterrows():\n",
    "    best_match_row, score = find_best_match(mb_row, spotify_combined_list)\n",
    "    if best_match_row is not None and score > 80:  # Adjust the threshold as needed\n",
    "        matched_df = matched_df.append({\n",
    "            'CONTRIBUTOR_ID': mb_row['ID'],\n",
    "            'SONG_TABLE_ID': best_match_row['id'],\n",
    "            'Spotify_Song_Title': best_match_row['song_title'],\n",
    "            'Spotify_Artist': best_match_row['song_album_id'],  # Note: Adjust if there's a more direct artist name column\n",
    "            'MusicBrainz_Song_Title': mb_row['SONGTITLE'],\n",
    "            'MusicBrainz_Artist': mb_row['MAINARTIST']\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# Output the matched DataFrame to CSV\n",
    "matched_df.to_csv('linked_songs.csv', sep=';', index=False)\n",
    "\n",
    "print(\"Linked songs CSV generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\contributors_non-normalized.csv')\n",
    "\n",
    "# Select only the 'artist_credit_name' and 'artist_credit_id' columns\n",
    "df_selected = df[['artist_credit_name', 'artist_credit_id']]\n",
    "\n",
    "# Drop duplicates based on 'artist_credit_id' to ensure each ID is unique\n",
    "df_unique = df_selected.drop_duplicates(subset=['artist_credit_id'])\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "df_unique.to_csv('unique_artist_credits.csv', index=False)\n",
    "\n",
    "print('Unique artist credits CSV file has been saved as unique_artist_credits.csv.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def parse_manual(array_str):\n",
    "    # Manually parse the string to extract elements between curly braces\n",
    "    # Remove leading and trailing braces and split by comma\n",
    "    items = array_str.strip('{}').split(',')\n",
    "    # Strip quotes and extra spaces from each item\n",
    "    items = [item.strip('\"').strip() for item in items]\n",
    "    return items\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\contributors_non-normalized.csv')\n",
    "\n",
    "# Prepare a list to collect all artist name-MBID pairs\n",
    "artist_pairs = []\n",
    "i=0\n",
    "\n",
    "# Iterate over the DataFrame rows\n",
    "for _, row in df.iterrows():\n",
    "    # Parse the 'artist_mbids' and 'individual_artist_names' fields manually\n",
    "    artist_mbids = parse_manual(row['artist_mbids'])\n",
    "    individual_artist_names = parse_manual(row['individual_artist_names'])\n",
    "    \n",
    "    # Ensure we have equal lengths of MBIDs and names before proceeding\n",
    "    if len(artist_mbids) == len(individual_artist_names):\n",
    "        # Pair each individual artist name with its corresponding MBID\n",
    "        for artist_name, artist_mbid in zip(individual_artist_names, artist_mbids):\n",
    "            artist_pairs.append((artist_name, artist_mbid))\n",
    "    else:\n",
    "        print(\"Warning: Mismatched MBIDs and artist names for a row, skipping.\")\n",
    "        print(f\"Row index: {i}\")\n",
    "        i = i+1\n",
    "\n",
    "# Convert the list of pairs into a DataFrame, ensuring uniqueness\n",
    "df_pairs = pd.DataFrame(list(set(artist_pairs)), columns=['individual_artist_name', 'artist_mbid'])\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "df_pairs.to_csv('artist_name_mbid_pairs.csv', index=False)\n",
    "\n",
    "print('Artist name-MBID pairs CSV file has been saved as artist_name_mbid_pairs.csv.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the MBID CSV\n",
    "mbid_df = pd.read_csv('C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\prepopulationStuff\\\\PythonNotebooksForPrepopulation\\\\artist_name_mbid_pairs.csv')\n",
    "\n",
    "# Load the Spotify artist CSV, remember to use the ';' delimiter\n",
    "spotify_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\artists_data.csv', delimiter=';')\n",
    "\n",
    "# Perform a direct merge based on artist names\n",
    "# Note: This assumes 'artist_name' in spotify_df exactly matches 'individual_artist_name' in mbid_df\n",
    "merged_df = pd.merge(spotify_df, mbid_df, how='left', left_on='artist_name', right_on='individual_artist_name')\n",
    "\n",
    "# Drop the 'individual_artist_name' column as it's redundant after merge\n",
    "merged_df.drop(columns=['individual_artist_name'], inplace=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('spotify_artist_with_mbid_direct_match.csv', index=False)\n",
    "\n",
    "print('Spotify artist data with direct match MBIDs has been saved as spotify_artist_with_mbid_direct_match.csv.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adjust the path to your Spotify artists CSV file\n",
    "spotify_csv_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\artists_data.csv'\n",
    "spotify_df = pd.read_csv(spotify_csv_path, delimiter=';')\n",
    "\n",
    "# Print column names to verify\n",
    "print(spotify_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\downloaded_files\\\\artists_data_full.csv'\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "\n",
    "# Function to check for empty values in columns that shouldn't have them\n",
    "def check_empty_values(df, columns):\n",
    "    for column in columns:\n",
    "        if df[column].isnull().any():\n",
    "            print(f\"Empty values found in column: {column}\")\n",
    "        else:\n",
    "            print(f\"No empty values in column: {column}\")\n",
    "\n",
    "# Function to check data types\n",
    "def check_data_types(df):\n",
    "    errors = []\n",
    "    for column, expected_type in expected_column_types.items():\n",
    "        if expected_type == 'numeric':\n",
    "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "                errors.append(f\"Column {column} is not of type {expected_type}\")\n",
    "        elif expected_type == 'string':\n",
    "            if not pd.api.types.is_string_dtype(df[column]):\n",
    "                errors.append(f\"Column {column} is not of type {expected_type}\")\n",
    "        elif expected_type == 'date':\n",
    "            try:\n",
    "                pd.to_datetime(df[column])\n",
    "            except ValueError:\n",
    "                errors.append(f\"Column {column} contains non-date values\")\n",
    "    \n",
    "    if errors:\n",
    "        for error in errors:\n",
    "            print(error)\n",
    "    else:\n",
    "        print(\"All columns match their expected data types.\")\n",
    "\n",
    "# Columns expected not to have empty values based on your schema (all of them in this case)\n",
    "mandatory_columns = [\n",
    "    'id', 'artist_spotify_id', 'artist_name', 'artist_popularity',\n",
    "    'artist_image_small', 'artist_image_medium', 'artist_image_large',\n",
    "    'artist_followers', 'date_added_to_db', 'date_last_modified'\n",
    "]\n",
    "\n",
    "# Expected data types\n",
    "expected_column_types = {\n",
    "    'id': 'numeric',\n",
    "    'artist_spotify_id': 'string',\n",
    "    'artist_name': 'string',\n",
    "    'artist_popularity': 'numeric',\n",
    "    'artist_image_small': 'string',\n",
    "    'artist_image_medium': 'string',\n",
    "    'artist_image_large': 'string',\n",
    "    'artist_followers': 'numeric',\n",
    "    'date_added_to_db': 'date',\n",
    "    'date_last_modified': 'date'\n",
    "}\n",
    "\n",
    "# Perform checks\n",
    "check_empty_values(df, mandatory_columns)\n",
    "check_data_types(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_data_full.csv', sep=';')  # Adjust the path and separator as needed\n",
    "\n",
    "# Convert the dates in the last two columns\n",
    "# Assuming the last two columns contain your dates\n",
    "df.iloc[:, -4] = pd.to_datetime(df.iloc[:, -4], dayfirst=True).dt.strftime('%Y-%m-%d')\n",
    "df.iloc[:, -3] = pd.to_datetime(df.iloc[:, -3], dayfirst=True).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Save the modified DataFrame back to a new CSV file\n",
    "df.to_csv('path_to_modified_csv.csv', index=False, sep=';')  # Adjust the path and separator as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_data_full.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "\n",
    "# Function to check for empty values in mandatory columns\n",
    "def check_empty_values(df, columns):\n",
    "    for column in columns:\n",
    "        if df[column].isnull().any():\n",
    "            print(f\"Empty values found in column: {column}\")\n",
    "        else:\n",
    "            print(f\"No empty values in column: {column}\")\n",
    "\n",
    "# Function to verify data types\n",
    "def check_data_types(df, column_types):\n",
    "    for column, expected_type in column_types.items():\n",
    "        if expected_type == 'numeric':\n",
    "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of numeric type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct numeric type.\")\n",
    "        elif expected_type == 'string':\n",
    "            if not pd.api.types.is_string_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of string type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct string type.\")\n",
    "        elif expected_type == 'date':\n",
    "            try:\n",
    "                pd.to_datetime(df[column])\n",
    "                print(f\"Column {column} is of correct date type.\")\n",
    "            except ValueError:\n",
    "                print(f\"Column {column} contains incorrect date format.\")\n",
    "\n",
    "# Mandatory columns (not nullable)\n",
    "mandatory_columns = [\n",
    "    'id', 'album_spotify_id', 'album_name', 'album_cover_art',\n",
    "    'album_release_date', 'release_date_precision', 'album_popularity',\n",
    "    'album_type', 'date_added_to_db', 'date_last_modified', 'musicbrainz_metadata_added'\n",
    "]\n",
    "\n",
    "# Columns with specific data types\n",
    "column_types = {\n",
    "    'id': 'numeric',\n",
    "    'album_spotify_id': 'string',\n",
    "    'album_name': 'string',\n",
    "    'album_cover_art': 'string',\n",
    "    'album_release_date': 'date',\n",
    "    'release_date_precision': 'string',\n",
    "    'album_popularity': 'numeric',\n",
    "    'album_type': 'string',\n",
    "    'spotify_album_upc': 'string',  # Nullable\n",
    "    'spotify_album_ean': 'string',  # Nullable\n",
    "    'spotify_album_isrc': 'string',  # Nullable\n",
    "    'date_added_to_db': 'date',\n",
    "    'date_last_modified': 'date',\n",
    "    'musicbrainz_metadata_added': 'boolean',  # Assuming true/false representation\n",
    "    'musicbrainz_id': 'string'  # Nullable\n",
    "}\n",
    "\n",
    "# Check for empty values in mandatory columns\n",
    "check_empty_values(df, mandatory_columns)\n",
    "\n",
    "# Check if data types are correct\n",
    "check_data_types(df, column_types)\n",
    "31045;59\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_data_full.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "\n",
    "# Function to check for empty values in mandatory columns\n",
    "def check_empty_values(df, columns):\n",
    "    for column in columns:\n",
    "        if df[column].isnull().any():\n",
    "            print(f\"Empty values found in column: {column}\")\n",
    "        else:\n",
    "            print(f\"No empty values in column: {column}\")\n",
    "\n",
    "# Function to verify data types and print incorrect date formats\n",
    "def check_data_types_and_dates(df, column_types):\n",
    "    for column, expected_type in column_types.items():\n",
    "        if expected_type == 'numeric':\n",
    "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of numeric type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct numeric type.\")\n",
    "        elif expected_type == 'string':\n",
    "            if not pd.api.types.is_string_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of string type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct string type.\")\n",
    "        elif expected_type == 'date':\n",
    "            incorrect_format = df[column].apply(lambda x: check_date_format(x))\n",
    "            if incorrect_format.any():\n",
    "                print(f\"Rows with incorrect date format in column '{column}':\")\n",
    "                print(df[incorrect_format])\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct date type.\")\n",
    "\n",
    "# Helper function to check date format\n",
    "def check_date_format(date_string):\n",
    "    try:\n",
    "        pd.to_datetime(date_string, errors='raise')\n",
    "        return False  # Date format is correct\n",
    "    except ValueError:\n",
    "        return True  # Date format is incorrect\n",
    "\n",
    "# Mandatory columns (not nullable)\n",
    "mandatory_columns = [\n",
    "    'id', 'album_spotify_id', 'album_name', 'album_cover_art',\n",
    "    'album_release_date', 'release_date_precision', 'album_popularity',\n",
    "    'album_type', 'date_added_to_db', 'date_last_modified', 'musicbrainz_metadata_added'\n",
    "]\n",
    "\n",
    "# Columns with specific data types\n",
    "column_types = {\n",
    "    'id': 'numeric',\n",
    "    'album_spotify_id': 'string',\n",
    "    'album_name': 'string',\n",
    "    'album_cover_art': 'string',\n",
    "    'album_release_date': 'date',\n",
    "    'release_date_precision': 'string',\n",
    "    'album_popularity': 'numeric',\n",
    "    'album_type': 'string',\n",
    "    'spotify_album_upc': 'string',  # Nullable\n",
    "    'spotify_album_ean': 'string',  # Nullable\n",
    "    'spotify_album_isrc': 'string',  # Nullable\n",
    "    'date_added_to_db': 'date',\n",
    "    'date_last_modified': 'date',\n",
    "    'musicbrainz_metadata_added': 'boolean',  # Assuming true/false representation\n",
    "    'musicbrainz_id': 'string'  # Nullable\n",
    "}\n",
    "\n",
    "# Check for empty values in mandatory columns\n",
    "check_empty_values(df, mandatory_columns)\n",
    "\n",
    "# Check if data types are correct and for incorrect date formats\n",
    "check_data_types_and_dates(df, column_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\tracks_data_full.csv'  # Update this to the path of your CSV file\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')  # Ensure delimiter matches your CSV format\n",
    "\n",
    "# Function to check for empty values in mandatory columns\n",
    "def check_empty_values(df, columns):\n",
    "    for column in columns:\n",
    "        if df[column].isnull().any():\n",
    "            print(f\"Empty values found in column: {column}\")\n",
    "        else:\n",
    "            print(f\"No empty values in column: {column}\")\n",
    "\n",
    "# Function to verify data types\n",
    "def check_data_types(df, column_types):\n",
    "    for column, expected_type in column_types.items():\n",
    "        if expected_type == 'numeric':\n",
    "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of numeric type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct numeric type.\")\n",
    "        elif expected_type == 'string':\n",
    "            if not pd.api.types.is_string_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of string type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct string type.\")\n",
    "        elif expected_type == 'date':\n",
    "            try:\n",
    "                pd.to_datetime(df[column])\n",
    "                print(f\"Column {column} is of correct date type.\")\n",
    "            except ValueError:\n",
    "                print(f\"Column {column} contains incorrect date format.\")\n",
    "        elif expected_type == 'boolean':\n",
    "            # Checking if the column is boolean; Assuming boolean is represented as True/False or 1/0\n",
    "            if not pd.api.types.is_bool_dtype(df[column]) and not all(df[column].dropna().isin([0, 1, 'True', 'False', True, False])):\n",
    "                print(f\"Column {column} is not of boolean type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct boolean type.\")\n",
    "\n",
    "# Mandatory columns (not nullable)\n",
    "mandatory_columns = [\n",
    "    'id', 'song_spotify_id', 'song_title', 'song_duration', \n",
    "    'song_album_type', 'song_album_id', 'song_explicit', \n",
    "    'song_popularity', 'song_track_features_added', 'song_date_added_to_db', \n",
    "    'song_date_last_modified'\n",
    "]\n",
    "\n",
    "# Columns with specific data types\n",
    "column_types = {\n",
    "    'id': 'numeric',\n",
    "    'song_spotify_id': 'string',\n",
    "    'song_title': 'string',\n",
    "    'song_duration': 'numeric',\n",
    "    'song_album_type': 'string',\n",
    "    'song_album_id': 'string',\n",
    "    'song_explicit': 'boolean',\n",
    "    'song_popularity': 'numeric',\n",
    "    'song_preview_url': 'string',  # Nullable\n",
    "    'song_track_features_added': 'boolean',\n",
    "    # Assuming 'floatType' corresponds to 'numeric' in Python/Pandas\n",
    "    'song_acousticness': 'numeric',  # Nullable\n",
    "    'song_danceability': 'numeric',  # Nullable\n",
    "    'song_energy': 'numeric',  # Nullable\n",
    "    'song_instrumentalness': 'numeric',  # Nullable\n",
    "    'song_liveness': 'numeric',  # Nullable\n",
    "    'song_loudness': 'numeric',  # Nullable\n",
    "    'song_speechiness': 'numeric',  # Nullable\n",
    "    'song_tempo': 'numeric',  # Nullable\n",
    "    'song_valence': 'numeric',  # Nullable\n",
    "    'song_key': 'numeric',  # Nullable\n",
    "    'song_time_signature': 'numeric',  # Nullable\n",
    "    'song_date_added_to_db': 'date',\n",
    "    'song_date_last_modified': 'date',\n",
    "    'album_id': 'numeric'  # Nullable\n",
    "}\n",
    "\n",
    "# Check for empty values in mandatory columns\n",
    "check_empty_values(df, mandatory_columns)\n",
    "\n",
    "# Check if data types are correct\n",
    "check_data_types(df, column_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adjust these settings as needed\n",
    "pd.set_option('display.max_rows', None)  # This will allow all rows to be displayed\n",
    "pd.set_option('display.max_columns', None)  # This will allow all columns to be displayed\n",
    "pd.set_option('display.width', 1000)  # Adjust the width to accommodate the number of columns\n",
    "pd.set_option('display.max_colwidth', None)  # This ensures that the content of each column is fully displayed\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\tracks_data_full.csv'  # Update this to the path of your CSV file\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "\n",
    "# Function to print rows with empty values in a specific column\n",
    "def print_rows_with_empty_values_in_column(df, column_name):\n",
    "    empty_rows = df[df[column_name].isnull() | (df[column_name] == '')]\n",
    "    if not empty_rows.empty:\n",
    "        print(f\"Rows with empty values in column '{column_name}':\")\n",
    "        # print the song_spotfiy_id for each row with empty values\n",
    "        print(empty_rows['song_spotify_id'])\n",
    "    else:\n",
    "        print(f\"No empty values found in column '{column_name}'.\")\n",
    "\n",
    "# Print all rows where the song_title is empty\n",
    "print_rows_with_empty_values_in_column(df, 'song_title')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load the CSV files\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_data_full.csv', delimiter=';', dtype={'id': str, 'album_spotify_id': str})\n",
    "artists_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\artists_data_full.csv', delimiter=';', dtype={'id': str, 'artist_spotify_id': str})\n",
    "\n",
    "# Create dictionaries for ID mappings\n",
    "album_id_map = albums_df.set_index('album_spotify_id')['id'].to_dict()\n",
    "artist_id_map = artists_df.set_index('artist_spotify_id')['id'].to_dict()\n",
    "\n",
    "artist_album_mapping = []\n",
    "\n",
    "# Assuming the JSON structure is as shown in your example\n",
    "for root, dirs, files in os.walk('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\ALBUMS'):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for album in data['albums']:\n",
    "                    album_id = album_id_map.get(album['id'])\n",
    "                    if not album_id:\n",
    "                        print(f\"Album ID {album['id']} not found in album CSV.\")\n",
    "                        continue\n",
    "                    for artist in album['artists']:\n",
    "                        artist_id = artist_id_map.get(artist['id'])\n",
    "                        if not artist_id:\n",
    "                            print(f\"Artist ID {artist['id']} not found in artist CSV.\")\n",
    "                            continue\n",
    "                        artist_album_mapping.append({'artistID': artist_id, 'albumID': album_id})\n",
    "\n",
    "# Create DataFrame and remove duplicates\n",
    "artist_album_df = pd.DataFrame(artist_album_mapping).drop_duplicates()\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "artist_album_df.to_csv('artist_album_mappings_new.csv', index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load the CSV files\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_data_full.csv', delimiter=';', dtype={'id': str, 'album_spotify_id': str})\n",
    "artists_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\artists_data_full.csv', delimiter=';', dtype={'id': str, 'artist_spotify_id': str})\n",
    "\n",
    "# Create dictionaries for ID mappings\n",
    "album_id_map = albums_df.set_index('album_spotify_id')['id'].to_dict()\n",
    "artist_id_map = artists_df.set_index('artist_spotify_id')['id'].to_dict()\n",
    "\n",
    "artist_album_mapping = []\n",
    "missing_artists = {}  # Store missing artist IDs and the album IDs they're attributed to\n",
    "\n",
    "# Assuming the JSON structure is as shown in your example\n",
    "for root, dirs, files in os.walk('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\ALBUMS'):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for album in data['albums']:\n",
    "                    album_id = album_id_map.get(album['id'])\n",
    "                    if not album_id:\n",
    "                        print(f\"Album ID {album['id']} not found in album CSV.\")\n",
    "                        continue\n",
    "                    for artist in album['artists']:\n",
    "                        artist_id = artist_id_map.get(artist['id'])\n",
    "                        if not artist_id:\n",
    "                            # Record the missing artist ID along with the current album ID\n",
    "                            missing_artists.setdefault(artist['id'], []).append(album['id'])\n",
    "                            print(f\"Artist ID {artist['id']} not found in artist CSV, attributed to Album ID {album['id']}.\")\n",
    "                            continue\n",
    "                        artist_album_mapping.append({'artistID': artist_id, 'albumID': album_id})\n",
    "\n",
    "# Display missing artist IDs and the albums they're attributed to\n",
    "for artist_id, album_ids in missing_artists.items():\n",
    "    print(f\"Artist ID {artist_id} (not found) is attributed to Album IDs: {', '.join(album_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "S2TENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
