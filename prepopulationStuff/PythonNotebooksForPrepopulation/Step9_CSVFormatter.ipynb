{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# The directory containing your folders with JSON files\n",
    "root_directory = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\track_information'\n",
    "\n",
    "# The path for the output CSV file\n",
    "csv_file_path = 'tracks_data.csv'\n",
    "\n",
    "# Current date for 'Date Added To DB' and 'Date Last Modified' columns in the YYYY-MM-DD format\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Template for CSV rows with the updated header titles\n",
    "csv_columns = [\n",
    "    'id', 'song_spotify_id', 'song_title', 'song_duration', 'song_album_type', \n",
    "    'song_album_id', 'song_explicit', 'song_popularity', 'song_preview_url', \n",
    "    'song_track_features_added', 'song_acousticness', 'song_danceability', 'song_energy', \n",
    "    'song_instrumentalness', 'song_liveness', 'song_loudness', 'song_speechiness', \n",
    "    'song_tempo', 'song_valence', 'song_key', 'song_time_signature', 'song_date_added_to_db', \n",
    "    'song_date_last_modified'\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store rows for the CSV\n",
    "csv_rows = []\n",
    "\n",
    "def process_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for track in data.get('tracks', []):\n",
    "            # Skip processing if the track is None\n",
    "            if track is None:\n",
    "                continue\n",
    "\n",
    "            row = {\n",
    "                'id': -1,  # This will be updated later with the actual ID\n",
    "                'song_spotify_id': track['id'],\n",
    "                'song_title': track['name'],\n",
    "                'song_duration': track['duration_ms'],\n",
    "                'song_album_type': track['album']['album_type'].upper(),\n",
    "                'song_album_id': track['album']['id'],\n",
    "                'song_explicit': track['explicit'],\n",
    "                'song_popularity': track['popularity'],\n",
    "                'song_preview_url': track.get('preview_url', ''),\n",
    "                'song_track_features_added': False,\n",
    "                'song_acousticness': -1,\n",
    "                'song_danceability': -1,\n",
    "                'song_energy': -1,\n",
    "                'song_instrumentalness': -1,\n",
    "                'song_liveness': -1,\n",
    "                'song_loudness': -1,\n",
    "                'song_speechiness': -1,\n",
    "                'song_tempo': -1,\n",
    "                'song_valence': -1,\n",
    "                'song_key': -1,\n",
    "                'song_time_signature': -1,\n",
    "                'song_date_added_to_db': current_date,\n",
    "                'song_date_last_modified': current_date\n",
    "            }\n",
    "            csv_rows.append(row)\n",
    "\n",
    "# Iterate over each subfolder and JSON file in the directory\n",
    "for subdir, dirs, files in os.walk(root_directory):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.json'):\n",
    "            print(f\"Processing {filename}...\")\n",
    "            process_json_file(os.path.join(subdir, filename))\n",
    "\n",
    "# Write the CSV file\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=csv_columns, delimiter=';')\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Update each row with its ID before writing\n",
    "    for i, row in enumerate(csv_rows, start=1):\n",
    "        row['id'] = i\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"CSV file has been successfully created at {csv_file_path} with {len(csv_rows)} tracks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the root directory containing your JSON files\n",
    "root_directory = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\ALBUMS'\n",
    "\n",
    "# Define the output CSV file path\n",
    "csv_file_path = 'album_data.csv'\n",
    "\n",
    "# Current date for 'Date Added To DB' and 'Date Last Modified' columns\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# CSV column headers\n",
    "csv_columns = [\n",
    "    'id', 'album_spotify_id', 'album_name', 'album_cover_art', 'album_release_date',\n",
    "    'release_date_precision', 'album_popularity', 'album_type', 'spotify_album_upc',\n",
    "    'spotify_album_ean', 'spotify_album_isrc', 'date_added_to_db', 'date_last_modified',\n",
    "    'musicbrainz_metadata_added', 'musicbrainz_id'\n",
    "]\n",
    "\n",
    "# Initialize a list to hold album data\n",
    "albums_data = []\n",
    "\n",
    "# Function to process each JSON file\n",
    "def process_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for album in data.get('albums', []):\n",
    "            # Extract the required information, with checks for nullable fields\n",
    "            album_data = {\n",
    "                'id': -1,  # Placeholder, will be updated later\n",
    "                'album_spotify_id': album['id'],\n",
    "                'album_name': album['name'],\n",
    "                'album_cover_art': album['images'][0]['url'] if album.get('images') else '',\n",
    "                'album_release_date': album['release_date'],\n",
    "                'release_date_precision': album['release_date_precision'],\n",
    "                'album_popularity': album['popularity'],\n",
    "                'album_type': album['album_type'],\n",
    "                'spotify_album_upc': album['external_ids'].get('upc', '') if album.get('external_ids') else '',\n",
    "                'spotify_album_ean': album['external_ids'].get('ean', '') if album.get('external_ids') else '',\n",
    "                'spotify_album_isrc': album['external_ids'].get('isrc', '') if album.get('external_ids') else '',\n",
    "                'date_added_to_db': current_date,\n",
    "                'date_last_modified': current_date,\n",
    "                'musicbrainz_metadata_added': False,  # Placeholder\n",
    "                'musicbrainz_id': ''  # Placeholder\n",
    "            }\n",
    "            albums_data.append(album_data)\n",
    "\n",
    "# Process each JSON file in the directory and subdirectories\n",
    "for subdir, dirs, files in os.walk(root_directory):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.json'):\n",
    "            process_json_file(os.path.join(subdir, filename))\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=csv_columns, delimiter=';')\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Update each row with its actual ID before writing\n",
    "    for i, album_data in enumerate(albums_data, start=1):\n",
    "        album_data['id'] = i\n",
    "        writer.writerow(album_data)\n",
    "\n",
    "print(f\"CSV file has been successfully created at {csv_file_path} with {len(albums_data)} albums.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the dtypes for the IDs to be strings when reading the CSVs\n",
    "dtype_dict = {'id': str, 'song_album_id': str, 'album_spotify_id': str}\n",
    "tracks_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\tracks_data.csv', delimiter=';', dtype=dtype_dict)\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\album_data.csv', delimiter=';', dtype=dtype_dict)\n",
    "\n",
    "# Create a dictionary mapping from album_spotify_id to id from albums_df\n",
    "# Ensure the 'id' column in albums_df is converted to integer if it's not NaN\n",
    "album_id_map = albums_df.dropna(subset=['id']).set_index('album_spotify_id')['id'].astype(int).to_dict()\n",
    "\n",
    "# Map the song_album_id in tracks_df using the album_id_map to get the album id\n",
    "tracks_df['album_id'] = tracks_df['song_album_id'].map(album_id_map)\n",
    "\n",
    "# Convert the new album_id column to integers, NaNs will be converted to a float with a .0\n",
    "tracks_df['album_id'] = tracks_df['album_id'].fillna(-1).astype(int)\n",
    "\n",
    "# Replace -1 back to NaN if you want to keep NaN values\n",
    "tracks_df['album_id'].replace(-1, pd.NA, inplace=True)\n",
    "\n",
    "# Save the updated tracks DataFrame to a new CSV file\n",
    "tracks_df.to_csv('path_to_updated_tracks.csv', index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Read the albums and artists CSVs into DataFrames\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_data_full.csv', delimiter=';', dtype={'id': str, 'album_spotify_id': str})\n",
    "artists_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\artists_data_full.csv', delimiter=';', dtype={'id': str, 'artist_spotify_id': str})\n",
    "\n",
    "# Dictionary to map Spotify album ID to CSV album ID\n",
    "album_id_map = albums_df.set_index('album_spotify_id')['id'].to_dict()\n",
    "\n",
    "# Dictionary to map Spotify artist ID to CSV artist ID\n",
    "artist_id_map = artists_df.set_index('artist_spotify_id')['id'].to_dict()\n",
    "\n",
    "# Initialize a list to hold the artist-album mappings\n",
    "artist_album_mapping = []\n",
    "\n",
    "# Assuming 'path_to_json_folder' is the folder containing all the JSON subfolders\n",
    "for root, dirs, files in os.walk('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\ALBUMS'):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for album in data['albums']:\n",
    "                    album_id = album_id_map.get(album['id'])\n",
    "                    if album_id:\n",
    "                        for artist in album['artists']:\n",
    "                            artist_id = artist_id_map.get(artist['id'])\n",
    "                            if artist_id:\n",
    "                                artist_album_mapping.append({'artistID': artist_id, 'albumID': album_id})\n",
    "\n",
    "# Create a DataFrame from the artist-album mappings\n",
    "artist_album_df = pd.DataFrame(artist_album_mapping)\n",
    "\n",
    "# Remove duplicates if there are any\n",
    "artist_album_df = artist_album_df.drop_duplicates()\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "artist_album_df.to_csv('artist_album_mappings_new.csv', index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Read the albums and artists CSVs into DataFrames\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\album_data.csv', delimiter=';', dtype={'id': str, 'album_spotify_id': str})\n",
    "artists_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\artists_data_full.csv', delimiter=';', dtype={'id': str, 'artist_spotify_id': str})\n",
    "\n",
    "# Dictionary to map Spotify album ID to CSV album ID\n",
    "album_id_map = albums_df.set_index('album_spotify_id')['id'].to_dict()\n",
    "\n",
    "# Dictionary to map Spotify artist ID to CSV artist ID\n",
    "artist_id_map = artists_df.set_index('artist_spotify_id')['id'].to_dict()\n",
    "\n",
    "# Initialize a list to hold the artist-album mappings\n",
    "artist_album_mapping = []\n",
    "\n",
    "# List to hold artist Spotify IDs where CSV artist ID was not found\n",
    "missing_artist_ids = []\n",
    "\n",
    "# Assuming 'path_to_json_folder' is the folder containing all the JSON subfolders\n",
    "for root, dirs, files in os.walk('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\ALBUMS'):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for album in data['albums']:\n",
    "                    album_id = album_id_map.get(album['id'])\n",
    "                    if album_id:\n",
    "                        for artist in album['artists']:\n",
    "                            artist_spotify_id = artist['id']\n",
    "                            artist_id = artist_id_map.get(artist_spotify_id)\n",
    "                            if artist_id:\n",
    "                                artist_album_mapping.append({'artistID': artist_id, 'albumID': album_id})\n",
    "                            else:\n",
    "                                missing_artist_ids.append(artist_spotify_id)\n",
    "\n",
    "# Print the list of artist Spotify IDs where the CSV artist ID was not found\n",
    "print(\"List of artist Spotify IDs where the CSV artist ID was not found:\")\n",
    "for artist_id in missing_artist_ids:\n",
    "    print(artist_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "missing_artist_ids_df = pd.DataFrame(missing_artist_ids, columns=['missing_artist_spotify_id'])\n",
    "\n",
    "# Define the file path where you want to save the CSV\n",
    "file_path = 'missing_artist_ids.csv'  # You can specify your own path\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "missing_artist_ids_df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f'The missing artist IDs have been saved to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'input_file.csv' with the path to your input CSV file\n",
    "input_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\musicBrainzAllContributors.csv'\n",
    "# Replace 'output_file.csv' with the path where you want to save the output CSV file\n",
    "output_file_path = 'contributor_data_prelink.csv'\n",
    "\n",
    "# Step 1: Read the input CSV\n",
    "input_df = pd.read_csv(input_file_path, delimiter=',')\n",
    "\n",
    "# Step 2: Create the new DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    'ID': range(1, len(input_df) + 1),\n",
    "    'NAME': input_df['artist_credit_name'],\n",
    "    'ROLE': input_df['role'],\n",
    "    'INSTRUMENT': input_df['instrument'],\n",
    "    'MUSICBRAINZ_ID': input_df['artist_mbid'],\n",
    "    'MAINARTIST': input_df['artist_credit_name'],\n",
    "    'SONGTITLE': input_df['recording_name']\n",
    "})\n",
    "\n",
    "# Step 3: Write the resulting DataFrame to a new CSV file\n",
    "output_df.to_csv(output_file_path, sep=';', index=False)\n",
    "\n",
    "print(f\"Output CSV saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Load the CSV files\n",
    "musicbrainz_df = pd.read_csv('C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\prepopulationStuff\\\\PythonNotebooksForPrepopulation\\\\contributor_data_prelink.csv', sep=';')  # The file generated from the previous step\n",
    "spotify_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\tracks_data.csv', sep=';')  # The Spotify songs CSV\n",
    "\n",
    "# Create a list of combined song title and artist names from Spotify for matching\n",
    "spotify_combined_list = spotify_df.apply(lambda x: f\"{x['song_title']} {x['song_album_id']}\", axis=1).tolist()\n",
    "\n",
    "def find_best_match(mb_row, spotify_combined_list):\n",
    "    mb_combined = f\"{mb_row['SONGTITLE']} {mb_row['MAINARTIST']}\"\n",
    "    \n",
    "    # Using extractOne to find the best match from the list\n",
    "    best_match_info = process.extractOne(mb_combined, spotify_combined_list)\n",
    "    \n",
    "    # If there is a match found, extract it\n",
    "    if best_match_info:\n",
    "        best_match_text, best_score = best_match_info\n",
    "        # Find the index of the match in Spotify list to retrieve the full row from spotify_df\n",
    "        match_index = spotify_combined_list.index(best_match_text)\n",
    "        best_match_row = spotify_df.iloc[match_index]\n",
    "        return best_match_row, best_score\n",
    "    return None, 0\n",
    "\n",
    "# Prepare the output DataFrame\n",
    "matched_df = pd.DataFrame(columns=['CONTRIBUTOR_ID', 'SONG_TABLE_ID', 'Spotify_Song_Title', 'Spotify_Artist', 'MusicBrainz_Song_Title', 'MusicBrainz_Artist'])\n",
    "\n",
    "# Iterate over MusicBrainz entries to find matches\n",
    "for index, mb_row in musicbrainz_df.iterrows():\n",
    "    best_match_row, score = find_best_match(mb_row, spotify_combined_list)\n",
    "    if best_match_row is not None and score > 80:  # Adjust the threshold as needed\n",
    "        matched_df = matched_df.append({\n",
    "            'CONTRIBUTOR_ID': mb_row['ID'],\n",
    "            'SONG_TABLE_ID': best_match_row['id'],\n",
    "            'Spotify_Song_Title': best_match_row['song_title'],\n",
    "            'Spotify_Artist': best_match_row['song_album_id'],  # Note: Adjust if there's a more direct artist name column\n",
    "            'MusicBrainz_Song_Title': mb_row['SONGTITLE'],\n",
    "            'MusicBrainz_Artist': mb_row['MAINARTIST']\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# Output the matched DataFrame to CSV\n",
    "matched_df.to_csv('linked_songs.csv', sep=';', index=False)\n",
    "\n",
    "print(\"Linked songs CSV generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\contributors_non-normalized.csv')\n",
    "\n",
    "# Select only the 'artist_credit_name' and 'artist_credit_id' columns\n",
    "df_selected = df[['artist_credit_name', 'artist_credit_id']]\n",
    "\n",
    "# Drop duplicates based on 'artist_credit_id' to ensure each ID is unique\n",
    "df_unique = df_selected.drop_duplicates(subset=['artist_credit_id'])\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "df_unique.to_csv('unique_artist_credits.csv', index=False)\n",
    "\n",
    "print('Unique artist credits CSV file has been saved as unique_artist_credits.csv.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def parse_manual(array_str):\n",
    "    # Manually parse the string to extract elements between curly braces\n",
    "    # Remove leading and trailing braces and split by comma\n",
    "    items = array_str.strip('{}').split(',')\n",
    "    # Strip quotes and extra spaces from each item\n",
    "    items = [item.strip('\"').strip() for item in items]\n",
    "    return items\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\contributors_non-normalized.csv')\n",
    "\n",
    "# Prepare a list to collect all artist name-MBID pairs\n",
    "artist_pairs = []\n",
    "i=0\n",
    "\n",
    "# Iterate over the DataFrame rows\n",
    "for _, row in df.iterrows():\n",
    "    # Parse the 'artist_mbids' and 'individual_artist_names' fields manually\n",
    "    artist_mbids = parse_manual(row['artist_mbids'])\n",
    "    individual_artist_names = parse_manual(row['individual_artist_names'])\n",
    "    \n",
    "    # Ensure we have equal lengths of MBIDs and names before proceeding\n",
    "    if len(artist_mbids) == len(individual_artist_names):\n",
    "        # Pair each individual artist name with its corresponding MBID\n",
    "        for artist_name, artist_mbid in zip(individual_artist_names, artist_mbids):\n",
    "            artist_pairs.append((artist_name, artist_mbid))\n",
    "    else:\n",
    "        print(\"Warning: Mismatched MBIDs and artist names for a row, skipping.\")\n",
    "        print(f\"Row index: {i}\")\n",
    "        i = i+1\n",
    "\n",
    "# Convert the list of pairs into a DataFrame, ensuring uniqueness\n",
    "df_pairs = pd.DataFrame(list(set(artist_pairs)), columns=['individual_artist_name', 'artist_mbid'])\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "df_pairs.to_csv('artist_name_mbid_pairs.csv', index=False)\n",
    "\n",
    "print('Artist name-MBID pairs CSV file has been saved as artist_name_mbid_pairs.csv.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the MBID CSV\n",
    "mbid_df = pd.read_csv('C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\prepopulationStuff\\\\PythonNotebooksForPrepopulation\\\\artist_name_mbid_pairs.csv')\n",
    "\n",
    "# Load the Spotify artist CSV, remember to use the ';' delimiter\n",
    "spotify_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\artists_data.csv', delimiter=';')\n",
    "\n",
    "# Perform a direct merge based on artist names\n",
    "# Note: This assumes 'artist_name' in spotify_df exactly matches 'individual_artist_name' in mbid_df\n",
    "merged_df = pd.merge(spotify_df, mbid_df, how='left', left_on='artist_name', right_on='individual_artist_name')\n",
    "\n",
    "# Drop the 'individual_artist_name' column as it's redundant after merge\n",
    "merged_df.drop(columns=['individual_artist_name'], inplace=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('spotify_artist_with_mbid_direct_match.csv', index=False)\n",
    "\n",
    "print('Spotify artist data with direct match MBIDs has been saved as spotify_artist_with_mbid_direct_match.csv.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adjust the path to your Spotify artists CSV file\n",
    "spotify_csv_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\artists_data.csv'\n",
    "spotify_df = pd.read_csv(spotify_csv_path, delimiter=';')\n",
    "\n",
    "# Print column names to verify\n",
    "print(spotify_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\team_project\\\\team37\\\\downloaded_files\\\\artists_data_full.csv'\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "\n",
    "# Function to check for empty values in columns that shouldn't have them\n",
    "def check_empty_values(df, columns):\n",
    "    for column in columns:\n",
    "        if df[column].isnull().any():\n",
    "            print(f\"Empty values found in column: {column}\")\n",
    "        else:\n",
    "            print(f\"No empty values in column: {column}\")\n",
    "\n",
    "# Function to check data types\n",
    "def check_data_types(df):\n",
    "    errors = []\n",
    "    for column, expected_type in expected_column_types.items():\n",
    "        if expected_type == 'numeric':\n",
    "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "                errors.append(f\"Column {column} is not of type {expected_type}\")\n",
    "        elif expected_type == 'string':\n",
    "            if not pd.api.types.is_string_dtype(df[column]):\n",
    "                errors.append(f\"Column {column} is not of type {expected_type}\")\n",
    "        elif expected_type == 'date':\n",
    "            try:\n",
    "                pd.to_datetime(df[column])\n",
    "            except ValueError:\n",
    "                errors.append(f\"Column {column} contains non-date values\")\n",
    "    \n",
    "    if errors:\n",
    "        for error in errors:\n",
    "            print(error)\n",
    "    else:\n",
    "        print(\"All columns match their expected data types.\")\n",
    "\n",
    "# Columns expected not to have empty values based on your schema (all of them in this case)\n",
    "mandatory_columns = [\n",
    "    'id', 'artist_spotify_id', 'artist_name', 'artist_popularity',\n",
    "    'artist_image_small', 'artist_image_medium', 'artist_image_large',\n",
    "    'artist_followers', 'date_added_to_db', 'date_last_modified'\n",
    "]\n",
    "\n",
    "# Expected data types\n",
    "expected_column_types = {\n",
    "    'id': 'numeric',\n",
    "    'artist_spotify_id': 'string',\n",
    "    'artist_name': 'string',\n",
    "    'artist_popularity': 'numeric',\n",
    "    'artist_image_small': 'string',\n",
    "    'artist_image_medium': 'string',\n",
    "    'artist_image_large': 'string',\n",
    "    'artist_followers': 'numeric',\n",
    "    'date_added_to_db': 'date',\n",
    "    'date_last_modified': 'date'\n",
    "}\n",
    "\n",
    "# Perform checks\n",
    "check_empty_values(df, mandatory_columns)\n",
    "check_data_types(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_data_full.csv', sep=';')  # Adjust the path and separator as needed\n",
    "\n",
    "# Convert the dates in the last two columns\n",
    "# Assuming the last two columns contain your dates\n",
    "df.iloc[:, -4] = pd.to_datetime(df.iloc[:, -4], dayfirst=True).dt.strftime('%Y-%m-%d')\n",
    "df.iloc[:, -3] = pd.to_datetime(df.iloc[:, -3], dayfirst=True).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Save the modified DataFrame back to a new CSV file\n",
    "df.to_csv('path_to_modified_csv.csv', index=False, sep=';')  # Adjust the path and separator as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_data_full.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "\n",
    "# Function to check for empty values in mandatory columns\n",
    "def check_empty_values(df, columns):\n",
    "    for column in columns:\n",
    "        if df[column].isnull().any():\n",
    "            print(f\"Empty values found in column: {column}\")\n",
    "        else:\n",
    "            print(f\"No empty values in column: {column}\")\n",
    "\n",
    "# Function to verify data types\n",
    "def check_data_types(df, column_types):\n",
    "    for column, expected_type in column_types.items():\n",
    "        if expected_type == 'numeric':\n",
    "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of numeric type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct numeric type.\")\n",
    "        elif expected_type == 'string':\n",
    "            if not pd.api.types.is_string_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of string type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct string type.\")\n",
    "        elif expected_type == 'date':\n",
    "            try:\n",
    "                pd.to_datetime(df[column])\n",
    "                print(f\"Column {column} is of correct date type.\")\n",
    "            except ValueError:\n",
    "                print(f\"Column {column} contains incorrect date format.\")\n",
    "\n",
    "# Mandatory columns (not nullable)\n",
    "mandatory_columns = [\n",
    "    'id', 'album_spotify_id', 'album_name', 'album_cover_art',\n",
    "    'album_release_date', 'release_date_precision', 'album_popularity',\n",
    "    'album_type', 'date_added_to_db', 'date_last_modified', 'musicbrainz_metadata_added'\n",
    "]\n",
    "\n",
    "# Columns with specific data types\n",
    "column_types = {\n",
    "    'id': 'numeric',\n",
    "    'album_spotify_id': 'string',\n",
    "    'album_name': 'string',\n",
    "    'album_cover_art': 'string',\n",
    "    'album_release_date': 'date',\n",
    "    'release_date_precision': 'string',\n",
    "    'album_popularity': 'numeric',\n",
    "    'album_type': 'string',\n",
    "    'spotify_album_upc': 'string',  # Nullable\n",
    "    'spotify_album_ean': 'string',  # Nullable\n",
    "    'spotify_album_isrc': 'string',  # Nullable\n",
    "    'date_added_to_db': 'date',\n",
    "    'date_last_modified': 'date',\n",
    "    'musicbrainz_metadata_added': 'boolean',  # Assuming true/false representation\n",
    "    'musicbrainz_id': 'string'  # Nullable\n",
    "}\n",
    "\n",
    "# Check for empty values in mandatory columns\n",
    "check_empty_values(df, mandatory_columns)\n",
    "\n",
    "# Check if data types are correct\n",
    "check_data_types(df, column_types)\n",
    "31045;59\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_data_full.csv'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "\n",
    "# Function to check for empty values in mandatory columns\n",
    "def check_empty_values(df, columns):\n",
    "    for column in columns:\n",
    "        if df[column].isnull().any():\n",
    "            print(f\"Empty values found in column: {column}\")\n",
    "        else:\n",
    "            print(f\"No empty values in column: {column}\")\n",
    "\n",
    "# Function to verify data types and print incorrect date formats\n",
    "def check_data_types_and_dates(df, column_types):\n",
    "    for column, expected_type in column_types.items():\n",
    "        if expected_type == 'numeric':\n",
    "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of numeric type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct numeric type.\")\n",
    "        elif expected_type == 'string':\n",
    "            if not pd.api.types.is_string_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of string type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct string type.\")\n",
    "        elif expected_type == 'date':\n",
    "            incorrect_format = df[column].apply(lambda x: check_date_format(x))\n",
    "            if incorrect_format.any():\n",
    "                print(f\"Rows with incorrect date format in column '{column}':\")\n",
    "                print(df[incorrect_format])\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct date type.\")\n",
    "\n",
    "# Helper function to check date format\n",
    "def check_date_format(date_string):\n",
    "    try:\n",
    "        pd.to_datetime(date_string, errors='raise')\n",
    "        return False  # Date format is correct\n",
    "    except ValueError:\n",
    "        return True  # Date format is incorrect\n",
    "\n",
    "# Mandatory columns (not nullable)\n",
    "mandatory_columns = [\n",
    "    'id', 'album_spotify_id', 'album_name', 'album_cover_art',\n",
    "    'album_release_date', 'release_date_precision', 'album_popularity',\n",
    "    'album_type', 'date_added_to_db', 'date_last_modified', 'musicbrainz_metadata_added'\n",
    "]\n",
    "\n",
    "# Columns with specific data types\n",
    "column_types = {\n",
    "    'id': 'numeric',\n",
    "    'album_spotify_id': 'string',\n",
    "    'album_name': 'string',\n",
    "    'album_cover_art': 'string',\n",
    "    'album_release_date': 'date',\n",
    "    'release_date_precision': 'string',\n",
    "    'album_popularity': 'numeric',\n",
    "    'album_type': 'string',\n",
    "    'spotify_album_upc': 'string',  # Nullable\n",
    "    'spotify_album_ean': 'string',  # Nullable\n",
    "    'spotify_album_isrc': 'string',  # Nullable\n",
    "    'date_added_to_db': 'date',\n",
    "    'date_last_modified': 'date',\n",
    "    'musicbrainz_metadata_added': 'boolean',  # Assuming true/false representation\n",
    "    'musicbrainz_id': 'string'  # Nullable\n",
    "}\n",
    "\n",
    "# Check for empty values in mandatory columns\n",
    "check_empty_values(df, mandatory_columns)\n",
    "\n",
    "# Check if data types are correct and for incorrect date formats\n",
    "check_data_types_and_dates(df, column_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\tracks_data_full.csv'  # Update this to the path of your CSV file\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')  # Ensure delimiter matches your CSV format\n",
    "\n",
    "# Function to check for empty values in mandatory columns\n",
    "def check_empty_values(df, columns):\n",
    "    for column in columns:\n",
    "        if df[column].isnull().any():\n",
    "            print(f\"Empty values found in column: {column}\")\n",
    "        else:\n",
    "            print(f\"No empty values in column: {column}\")\n",
    "\n",
    "# Function to verify data types\n",
    "def check_data_types(df, column_types):\n",
    "    for column, expected_type in column_types.items():\n",
    "        if expected_type == 'numeric':\n",
    "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of numeric type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct numeric type.\")\n",
    "        elif expected_type == 'string':\n",
    "            if not pd.api.types.is_string_dtype(df[column]):\n",
    "                print(f\"Column {column} is not of string type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct string type.\")\n",
    "        elif expected_type == 'date':\n",
    "            try:\n",
    "                pd.to_datetime(df[column])\n",
    "                print(f\"Column {column} is of correct date type.\")\n",
    "            except ValueError:\n",
    "                print(f\"Column {column} contains incorrect date format.\")\n",
    "        elif expected_type == 'boolean':\n",
    "            # Checking if the column is boolean; Assuming boolean is represented as True/False or 1/0\n",
    "            if not pd.api.types.is_bool_dtype(df[column]) and not all(df[column].dropna().isin([0, 1, 'True', 'False', True, False])):\n",
    "                print(f\"Column {column} is not of boolean type.\")\n",
    "            else:\n",
    "                print(f\"Column {column} is of correct boolean type.\")\n",
    "\n",
    "# Mandatory columns (not nullable)\n",
    "mandatory_columns = [\n",
    "    'id', 'song_spotify_id', 'song_title', 'song_duration', \n",
    "    'song_album_type', 'song_album_id', 'song_explicit', \n",
    "    'song_popularity', 'song_track_features_added', 'song_date_added_to_db', \n",
    "    'song_date_last_modified'\n",
    "]\n",
    "\n",
    "# Columns with specific data types\n",
    "column_types = {\n",
    "    'id': 'numeric',\n",
    "    'song_spotify_id': 'string',\n",
    "    'song_title': 'string',\n",
    "    'song_duration': 'numeric',\n",
    "    'song_album_type': 'string',\n",
    "    'song_album_id': 'string',\n",
    "    'song_explicit': 'boolean',\n",
    "    'song_popularity': 'numeric',\n",
    "    'song_preview_url': 'string',  # Nullable\n",
    "    'song_track_features_added': 'boolean',\n",
    "    # Assuming 'floatType' corresponds to 'numeric' in Python/Pandas\n",
    "    'song_acousticness': 'numeric',  # Nullable\n",
    "    'song_danceability': 'numeric',  # Nullable\n",
    "    'song_energy': 'numeric',  # Nullable\n",
    "    'song_instrumentalness': 'numeric',  # Nullable\n",
    "    'song_liveness': 'numeric',  # Nullable\n",
    "    'song_loudness': 'numeric',  # Nullable\n",
    "    'song_speechiness': 'numeric',  # Nullable\n",
    "    'song_tempo': 'numeric',  # Nullable\n",
    "    'song_valence': 'numeric',  # Nullable\n",
    "    'song_key': 'numeric',  # Nullable\n",
    "    'song_time_signature': 'numeric',  # Nullable\n",
    "    'song_date_added_to_db': 'date',\n",
    "    'song_date_last_modified': 'date',\n",
    "    'album_id': 'numeric'  # Nullable\n",
    "}\n",
    "\n",
    "# Check for empty values in mandatory columns\n",
    "check_empty_values(df, mandatory_columns)\n",
    "\n",
    "# Check if data types are correct\n",
    "check_data_types(df, column_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adjust these settings as needed\n",
    "pd.set_option('display.max_rows', None)  # This will allow all rows to be displayed\n",
    "pd.set_option('display.max_columns', None)  # This will allow all columns to be displayed\n",
    "pd.set_option('display.width', 1000)  # Adjust the width to accommodate the number of columns\n",
    "pd.set_option('display.max_colwidth', None)  # This ensures that the content of each column is fully displayed\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\tracks_data_full.csv'  # Update this to the path of your CSV file\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path, delimiter=';')\n",
    "\n",
    "# Function to print rows with empty values in a specific column\n",
    "def print_rows_with_empty_values_in_column(df, column_name):\n",
    "    empty_rows = df[df[column_name].isnull() | (df[column_name] == '')]\n",
    "    if not empty_rows.empty:\n",
    "        print(f\"Rows with empty values in column '{column_name}':\")\n",
    "        # print the song_spotfiy_id for each row with empty values\n",
    "        print(empty_rows['song_spotify_id'])\n",
    "    else:\n",
    "        print(f\"No empty values found in column '{column_name}'.\")\n",
    "\n",
    "# Print all rows where the song_title is empty\n",
    "print_rows_with_empty_values_in_column(df, 'song_title')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load the CSV files\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_data_full.csv', delimiter=';', dtype={'id': str, 'album_spotify_id': str})\n",
    "artists_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\artists_data_full.csv', delimiter=';', dtype={'id': str, 'artist_spotify_id': str})\n",
    "\n",
    "# Create dictionaries for ID mappings\n",
    "album_id_map = albums_df.set_index('album_spotify_id')['id'].to_dict()\n",
    "artist_id_map = artists_df.set_index('artist_spotify_id')['id'].to_dict()\n",
    "\n",
    "artist_album_mapping = []\n",
    "\n",
    "# Assuming the JSON structure is as shown in your example\n",
    "for root, dirs, files in os.walk('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\ALBUMS'):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for album in data['albums']:\n",
    "                    album_id = album_id_map.get(album['id'])\n",
    "                    if not album_id:\n",
    "                        print(f\"Album ID {album['id']} not found in album CSV.\")\n",
    "                        continue\n",
    "                    for artist in album['artists']:\n",
    "                        artist_id = artist_id_map.get(artist['id'])\n",
    "                        if not artist_id:\n",
    "                            print(f\"Artist ID {artist['id']} not found in artist CSV.\")\n",
    "                            continue\n",
    "                        artist_album_mapping.append({'artistID': artist_id, 'albumID': album_id})\n",
    "\n",
    "# Create DataFrame and remove duplicates\n",
    "artist_album_df = pd.DataFrame(artist_album_mapping).drop_duplicates()\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "artist_album_df.to_csv('artist_album_mappings_new.csv', index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load the CSV files\n",
    "albums_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\album_table.csv', delimiter=';', dtype={'id': str, 'album_spotify_id': str})\n",
    "artists_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\artists_data_full.csv', delimiter=';', dtype={'id': str, 'artist_spotify_id': str})\n",
    "\n",
    "# Create dictionaries for ID mappings\n",
    "album_id_map = albums_df.set_index('album_spotify_id')['id'].to_dict()\n",
    "artist_id_map = artists_df.set_index('artist_spotify_id')['id'].to_dict()\n",
    "\n",
    "artist_album_mapping = []\n",
    "missing_artists = {}  # Store missing artist IDs and the album IDs they're attributed to\n",
    "\n",
    "# Assuming the JSON structure is as shown in your example\n",
    "for root, dirs, files in os.walk('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\ALBUMS'):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for album in data['albums']:\n",
    "                    album_id = album_id_map.get(album['id'])\n",
    "                    if not album_id:\n",
    "                        print(f\"Album ID {album['id']} not found in album CSV.\")\n",
    "                        continue\n",
    "                    for artist in album['artists']:\n",
    "                        artist_id = artist_id_map.get(artist['id'])\n",
    "                        if not artist_id:\n",
    "                            # Record the missing artist ID along with the current album ID\n",
    "                            missing_artists.setdefault(artist['id'], []).append(album['id'])\n",
    "                            print(f\"Artist ID {artist['id']} not found in artist CSV, attributed to Album ID {album['id']}.\")\n",
    "                            continue\n",
    "                        artist_album_mapping.append({'artistID': artist_id, 'albumID': album_id})\n",
    "\n",
    "# Display missing artist IDs and the albums they're attributed to\n",
    "for artist_id, album_ids in missing_artists.items():\n",
    "    print(f\"Artist ID {artist_id} (not found) is attributed to Album IDs: {', '.join(album_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\WORKING\\\\recordings_artist_list.csv', delimiter=',', dtype={'id': str, 'artist_spotify_id': str})\n",
    "df2 = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\artists_table.csv', delimiter=';', dtype={'id': str, 'artist_spotify_id': str})\n",
    "\n",
    "\n",
    "# Convert the musicbrainz_id column into a set for fast lookup\n",
    "musicbrainz_ids = set(df2['musicbrainz_id'])\n",
    "\n",
    "# Define a function to check if any artist_mbids match the musicbrainz_ids\n",
    "def matches_musicbrainz_ids(artist_mbids_str):\n",
    "    # Parse the string of artist_mbids into a list\n",
    "    artist_mbids = artist_mbids_str.strip('{}').split(',')\n",
    "    # Check if any of the artist_mbids is in the set of musicbrainz_ids\n",
    "    return any(mbid.strip() in musicbrainz_ids for mbid in artist_mbids)\n",
    "\n",
    "# Apply the function to filter df1\n",
    "df1['artist_mbids_match'] = df1['artist_mbids'].apply(matches_musicbrainz_ids)\n",
    "filtered_df1 = df1[df1['artist_mbids_match']]\n",
    "\n",
    "# Drop the helper column after filtering\n",
    "filtered_df1 = filtered_df1.drop(columns=['artist_mbids_match'])\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "output_csv_path = 'recordings_artist_list_onlyMatchedArtists.csv'\n",
    "filtered_df1.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m csv_path_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mMusic\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mPROJECTS\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSpotify Project\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSCRAPED_DATA\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mWORKING\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mrecordings_artist_list_onlyMatchedArtists.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m csv_path_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mMusic\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mPROJECTS\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSpotify Project\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSCRAPED_DATA\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mWORKING\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcontributors_normalized.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_path_2)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Merge the DataFrames on 'recording_mbid'\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Ensure that the key for merging ('recording_mbid') exists in both DataFrames exactly as named.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Music\\miniconda3\\envs\\S2TENV\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Music\\miniconda3\\envs\\S2TENV\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Music\\miniconda3\\envs\\S2TENV\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Music\\miniconda3\\envs\\S2TENV\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1250\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1248\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1250\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Music\\miniconda3\\envs\\S2TENV\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\Users\\Music\\miniconda3\\envs\\S2TENV\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Music\\miniconda3\\envs\\S2TENV\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Music\\miniconda3\\envs\\S2TENV\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Music\\miniconda3\\envs\\S2TENV\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1960\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV files into DataFrames\n",
    "csv_path_1 = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\WORKING\\\\recordings_artist_list_onlyMatchedArtists.csv'\n",
    "csv_path_2 = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\WORKING\\\\contributors_normalized.csv'\n",
    "\n",
    "df1 = pd.read_csv(csv_path_1)\n",
    "df2 = pd.read_csv(csv_path_2)\n",
    "\n",
    "# Merge the DataFrames on 'recording_mbid'\n",
    "# Ensure that the key for merging ('recording_mbid') exists in both DataFrames exactly as named.\n",
    "merged_df = pd.merge(df1, df2, on='recording_mbid', how='inner')\n",
    "\n",
    "# Select only the required columns\n",
    "final_df = merged_df[['recording_mbid', 'recording_name_x', 'artist_names', 'artist_mbids', 'artist_mbid', 'individual_artist_name', 'role', 'instrument']]\n",
    "\n",
    "# Optionally, rename 'recording_name_x' to 'recording_name' to correct the column name after merging\n",
    "final_df.rename(columns={'recording_name_x': 'recording_name'}, inplace=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "output_csv_path = 'contributors_normalized_onlyMatchedArtists.csv'\n",
    "final_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Combined data saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "contributors_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\WORKING\\\\contributors_normalized_onlyMatchedArtists.csv')\n",
    "artists_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\artists_table.csv', sep=';')\n",
    "artist_album_mapping_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\artist_album_mapping.csv', sep=';')\n",
    "track_data_df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\tracks_data_full.csv', sep=';')\n",
    "\n",
    "# Step 1: Merge artists with their albums\n",
    "artists_with_albums = pd.merge(artists_df, artist_album_mapping_df, left_on='id', right_on='artistID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Merge the above with track data on albumID\n",
    "artist_tracks = pd.merge(artists_with_albums, track_data_df, left_on='albumID', right_on='album_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributors_dict = contributors_df.groupby('artist_mbid')['recording_mbid'].apply(set).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_recording_mbid(row):\n",
    "    artist_mbids = contributors_dict.get(row['musicbrainz_id'], set())\n",
    "    # Filter contributors where recording_name matches song_title and artist_mbid is in the dictionary entry\n",
    "    for mbid in artist_mbids:\n",
    "        # Filter by artist_mbid first to create a reduced subset\n",
    "        possible_matches = contributors_df[(contributors_df['artist_mbid'] == mbid)]\n",
    "        # Then check if recording_name matches song_title\n",
    "        match = possible_matches[possible_matches['recording_name'] == row['song_title']]\n",
    "        if not match.empty:\n",
    "            return match['recording_mbid'].iloc[0]\n",
    "    return None\n",
    "\n",
    "# Apply the function across the artist_tracks DataFrame\n",
    "artist_tracks['mapped_recording_mbid'] = artist_tracks.apply(map_recording_mbid, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tracks_with_mbid = artist_tracks.dropna(subset=['mapped_recording_mbid'])\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "final_tracks_with_mbid.to_csv('final_tracks_with_mapped_mbid.csv', sep=';', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where no recording_mbid was mapped (if necessary)\n",
    "# final_tracks_with_mbid = artist_tracks.dropna(subset=['mapped_recording_mbid'])\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "final_tracks_with_mbid.to_csv('final_tracks_with_mapped_mbid.csv', sep=';', index=False)\n",
    "\n",
    "print(\"Mapping complete and saved to final_tracks_with_mapped_mbid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\tracks_data_full.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered file saved to tracks_data_full_new.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "# Remove rows where 'album_id' is empty\n",
    "df_filtered = df[df['album_id'].notna()]\n",
    "\n",
    "# Save the filtered DataFrame back to a new CSV file\n",
    "output_file_path = 'tracks_data_full_new.csv'  # Replace with your desired output file path\n",
    "df_filtered.to_csv(output_file_path, index=False, sep=';')\n",
    "\n",
    "print(f\"Filtered file saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have already read the CSV into a DataFrame named df\n",
    "df = pd.read_csv('C:\\\\Users\\\\Music\\\\Desktop\\\\PROJECTS\\\\Spotify Project\\\\SCRAPED_DATA\\\\FINISHED\\\\NEWEST\\\\tracks_data_full_new.csv', delimiter=';')\n",
    "\n",
    "# Function to truncate strings longer than 254 characters\n",
    "def truncate_string(s):\n",
    "    return s[:254] if isinstance(s, str) and len(s) > 254 else s\n",
    "\n",
    "# Apply the truncation function to the 'song_title' column\n",
    "df['song_title'] = df['song_title'].apply(truncate_string)\n",
    "\n",
    "# Save the DataFrame back to CSV\n",
    "df.to_csv('tracks_data_full_new_new.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "S2TENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
